{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Clearstone SDK","text":"<p>Production-Grade Governance and Observability for AI Agent Systems.</p> <p>Clearstone is a comprehensive Python SDK that provides safety, governance, and observability for multi-agent AI workflows. It combines declarative Policy-as-Code with OpenTelemetry-aligned distributed tracing to help you build reliable, debuggable, and compliant AI systems.</p>"},{"location":"#the-problem","title":"The Problem","text":"<p>Autonomous AI agents are powerful but operate in a high-stakes environment. Without robust guardrails and observability, they can be:</p> <ul> <li>Unsafe: Accidentally executing destructive actions (e.g., deleting files).</li> <li>Costly: Over-using expensive tools or LLM tokens.</li> <li>Non-compliant: Mishandling sensitive data (PII).</li> <li>Unpredictable: Difficult to debug when they fail.</li> <li>Opaque: No visibility into what they're actually doing at runtime.</li> </ul> <p>Clearstone provides the tools to manage these risks with declarative Policy-as-Code governance and production-ready distributed tracing.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#policy-governance","title":"Policy Governance","text":"<ul> <li>\u2705 Declarative Policy-as-Code: Write policies as simple Python functions using the <code>@Policy</code> decorator. No YAML or complex DSLs.</li> <li>\u2705 Seamless LangChain Integration: Drop the <code>PolicyCallbackHandler</code> into any LangChain agent to enforce policies at runtime.</li> <li>\u2705 Rich Pre-Built Policy Library: Get started in minutes with 17+ production-ready policies for cost control, RBAC, PII redaction, security alerts, and more.</li> <li>\u2705 Local LLM Protection: Built-in policies for system load monitoring and model server health checks\u2014specifically designed for local-first AI workflows.</li> <li>\u2705 Human-in-the-Loop Controls: Pause agent execution for manual approval with the <code>PAUSE</code> action and <code>InterventionClient</code> for high-stakes decisions.</li> <li>\u2705 Pre-Deploy Validation: Catch buggy, slow, or non-deterministic policies before they reach production with the <code>PolicyValidator</code>.</li> <li>\u2705 Line-by-Line Debugging: Understand exactly why a policy made a decision with the <code>PolicyDebugger</code>'s execution trace.</li> <li>\u2705 Performance Metrics: Track policy execution times, identify bottlenecks, and analyze decision patterns with <code>PolicyMetrics</code>.</li> <li>\u2705 Composable Logic: Build complex rules from simple, reusable policies with <code>compose_and</code> and <code>compose_or</code> helpers.</li> <li>\u2705 Exportable Audit Trails: Generate JSON or CSV audit logs for every policy decision, perfect for compliance and analysis.</li> <li>\u2705 Developer CLI: Accelerate development by scaffolding new, well-structured policy files with the <code>clearstone new-policy</code> command.</li> </ul>"},{"location":"#observability-tracing","title":"Observability &amp; Tracing","text":"<ul> <li>\u2705 Production-Ready Tracing: OpenTelemetry-aligned distributed tracing for complete agent execution visibility.</li> <li>\u2705 Automatic Hierarchy Tracking: Nested spans automatically establish parent-child relationships without manual configuration.</li> <li>\u2705 High-Fidelity Capture: Nanosecond-precision timing, input/output snapshots, and full error stack traces.</li> <li>\u2705 Thread-Safe Persistence: SQLite storage with Write-Ahead Logging (WAL) for concurrent-safe trace storage.</li> <li>\u2705 Asynchronous Batching: Non-blocking span capture with automatic batch writes for zero performance impact.</li> <li>\u2705 Hybrid Serialization: Smart JSON-first serialization with automatic pickle fallback for complex objects.</li> <li>\u2705 Single-Line Setup: Initialize the entire tracing system with one <code>TracerProvider</code> instantiation.</li> </ul>"},{"location":"#ai-native-testing-backtesting","title":"AI-Native Testing &amp; Backtesting","text":"<ul> <li>\u2705 Behavioral Assertions: Declarative test functions for validating agent behavior (tool usage, execution order, costs, errors).</li> <li>\u2705 Historical Backtesting: Test new policies against real production traces to predict impact before deployment.</li> <li>\u2705 Policy Test Harness: Simulate policy enforcement on historical data with detailed impact reports and metrics.</li> <li>\u2705 pytest Integration: Seamlessly integrate behavioral tests into existing test workflows and CI/CD pipelines.</li> <li>\u2705 Trace-Level Validation: Assert on complete execution flows, not just individual operations or outputs.</li> <li>\u2705 Comprehensive Reporting: Track block rates, decision distributions, and identify problematic traces.</li> </ul>"},{"location":"#time-travel-debugging","title":"Time-Travel Debugging","text":"<ul> <li>\u2705 Checkpoint System: Capture complete agent state at any point in execution history.</li> <li>\u2705 Agent Rehydration: Dynamically restore agents from checkpoints with full state preservation.</li> <li>\u2705 Deterministic Replay: Mock non-deterministic functions (time, random) for reproducible debugging sessions.</li> <li>\u2705 Interactive Debugging: Drop into pdb at any historical execution point with full context.</li> <li>\u2705 Pre-flight Mock Analysis: See exactly which functions will be mocked and how many responses were recorded before debugging.</li> <li>\u2705 Intelligent Error Handling: Clear error messages when mock data is insufficient, with actionable guidance.</li> <li>\u2705 Hybrid Serialization: JSON metadata with pickle state for human-readable yet high-fidelity checkpoints.</li> <li>\u2705 Upstream Span Tracking: Automatically capture parent span hierarchy for complete execution context.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>The SDK requires Python 3.10+.</p> <pre><code>pip install clearstone-sdk\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<p>See how easy it is to protect an agent from performing unauthorized actions.</p> <pre><code>from clearstone import Policy, ALLOW, BLOCK, PolicyEngine, create_context, context_scope\nfrom clearstone.integrations.langchain import PolicyCallbackHandler\n\n@Policy(name=\"block_admin_tools_for_guests\", priority=100)\ndef block_admin_tools_policy(context):\n    role = context.metadata.get(\"role\")\n    tool_name = context.metadata.get(\"tool_name\")\n\n    if role == \"guest\" and tool_name == \"admin_panel\":\n        return BLOCK(f\"Role '{role}' is not authorized to access '{tool_name}'.\")\n\n    return ALLOW\n\nengine = PolicyEngine()\nhandler = PolicyCallbackHandler(engine)\n\ncontext = create_context(\n    user_id=\"user_guest\",\n    agent_id=\"admin_agent_v1\",\n    metadata={\"role\": \"guest\"}\n)\n\nwith context_scope(context):\n    handler.on_tool_start(serialized={\"name\": \"admin_panel\"}, input_str=\"\")\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started: 5-minute quickstart tutorial</li> <li>Core Concepts: Understand the foundational concepts</li> <li>Pre-Built Policies: Explore the policy library</li> <li>User Guide: Deep dive into governance features</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"#anonymous-usage-telemetry","title":"Anonymous Usage Telemetry","text":"<p>To help improve Clearstone, the SDK collects anonymous usage statistics by default. This telemetry is:</p> <ul> <li>Anonymous: Only component initialization events are tracked (e.g., \"PolicyEngine initialized\")</li> <li>Non-Identifying: No user data, policy logic, or trace content is ever collected</li> <li>Transparent: All telemetry code is open source and auditable</li> <li>Opt-Out: Easy to disable at any time</li> </ul>"},{"location":"#how-to-opt-out","title":"How to Opt Out","text":"<p>Option 1: Environment Variable (Recommended) <pre><code>export CLEARSTONE_TELEMETRY_DISABLED=1\n</code></pre></p> <p>Option 2: Config File</p> <p>Edit or create <code>~/.clearstone/config.json</code>: <pre><code>{\n  \"telemetry\": {\n    \"disabled\": true\n  }\n}\n</code></pre></p> <p>Learn more in the Telemetry documentation.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please see our Contributing Guide for details on how to submit pull requests, set up a development environment, and run tests.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<p>Join our community to ask questions, share your projects, and get help from the team and other users.</p> <ul> <li>Discord: Join the Clearstone Community</li> <li>Twitter: Follow @clearstonedev for the latest news and updates.</li> <li>GitHub Issues: Report a bug or suggest a feature.</li> <li>Email: For other inquiries, you can reach out to pablo@clearstone.dev.</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to Clearstone! This 5-minute quickstart will guide you through the basics of protecting your AI agents with policy governance and distributed tracing.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Clearstone requires Python 3.10 or higher.</p> <pre><code>pip install clearstone-sdk\n</code></pre>"},{"location":"getting-started/#your-first-policy-protected-agent","title":"Your First Policy-Protected Agent","text":"<p>Let's build a simple agent that's protected by a policy. This example will show you how policies can prevent unauthorized actions before they happen.</p>"},{"location":"getting-started/#step-1-define-your-policies","title":"Step 1: Define Your Policies","text":"<p>Create a new file called <code>policies.py</code>:</p> <pre><code>from clearstone import Policy, ALLOW, BLOCK\n\n@Policy(name=\"block_dangerous_operations\", priority=100)\ndef block_dangerous_operations(context):\n    \"\"\"Prevent the agent from executing dangerous file operations.\"\"\"\n    tool_name = context.metadata.get(\"tool_name\", \"\")\n\n    dangerous_operations = [\"delete_file\", \"format_drive\", \"drop_table\"]\n\n    if any(op in tool_name.lower() for op in dangerous_operations):\n        return BLOCK(f\"Dangerous operation '{tool_name}' is not allowed.\")\n\n    return ALLOW\n</code></pre>"},{"location":"getting-started/#step-2-set-up-the-policy-engine","title":"Step 2: Set Up the Policy Engine","text":"<p>Create your main application file <code>main.py</code>:</p> <pre><code>from clearstone import (\n    PolicyEngine,\n    create_context,\n    context_scope,\n    PolicyViolationError\n)\nfrom clearstone.integrations.langchain import PolicyCallbackHandler\n\nimport policies\n\nengine = PolicyEngine()\nhandler = PolicyCallbackHandler(engine)\n\ndef run_agent_task(tool_name: str):\n    \"\"\"Simulate running an agent task with a specific tool.\"\"\"\n    print(f\"\\n--- Attempting to use tool: '{tool_name}' ---\")\n\n    context = create_context(\n        user_id=\"user_123\",\n        agent_id=\"file_management_agent\",\n        metadata={\"tool_name\": tool_name}\n    )\n\n    try:\n        with context_scope(context):\n            handler.on_tool_start(\n                serialized={\"name\": tool_name},\n                input_str=\"process_task\"\n            )\n\n        print(\"\u2705 SUCCESS: Operation approved by policies.\")\n\n    except PolicyViolationError as e:\n        print(f\"\u274c BLOCKED: {e.decision.reason}\")\n\nrun_agent_task(\"read_file\")\nrun_agent_task(\"delete_file\")\n</code></pre>"},{"location":"getting-started/#step-3-run-it","title":"Step 3: Run It","text":"<pre><code>python main.py\n</code></pre> <p>Output: <pre><code>--- Attempting to use tool: 'read_file' ---\n\u2705 SUCCESS: Operation approved by policies.\n\n--- Attempting to use tool: 'delete_file' ---\n\u274c BLOCKED: Dangerous operation 'delete_file' is not allowed.\n</code></pre></p> <p>That's it! Your agent is now protected by a policy that prevents dangerous operations.</p>"},{"location":"getting-started/#adding-distributed-tracing","title":"Adding Distributed Tracing","text":"<p>Now let's add observability to see exactly what your agent is doing.</p>"},{"location":"getting-started/#step-4-instrument-with-tracing","title":"Step 4: Instrument with Tracing","text":"<p>Update your <code>main.py</code> to include tracing:</p> <pre><code>from clearstone.observability import TracerProvider, SpanKind\n\nprovider = TracerProvider(db_path=\"agent_traces.db\")\ntracer = provider.get_tracer(\"file_management_agent\", version=\"1.0\")\n\ndef run_agent_task_with_tracing(tool_name: str):\n    \"\"\"Run an agent task with both policies and tracing.\"\"\"\n\n    with tracer.span(\"agent_task\", kind=SpanKind.INTERNAL) as root_span:\n        print(f\"\\n--- Task: {tool_name} (Trace ID: {root_span.trace_id}) ---\")\n\n        context = create_context(\n            user_id=\"user_123\",\n            agent_id=\"file_management_agent\",\n            metadata={\"tool_name\": tool_name}\n        )\n\n        try:\n            with context_scope(context):\n                with tracer.span(\"policy_check\", attributes={\"tool\": tool_name}):\n                    handler.on_tool_start(\n                        serialized={\"name\": tool_name},\n                        input_str=\"process_task\"\n                    )\n\n                with tracer.span(\"tool_execution\", attributes={\"tool\": tool_name}):\n                    print(f\"Executing {tool_name}...\")\n\n            print(\"\u2705 SUCCESS: Task completed.\")\n\n        except PolicyViolationError as e:\n            root_span.set_status(\"ERROR\")\n            print(f\"\u274c BLOCKED: {e.decision.reason}\")\n\nrun_agent_task_with_tracing(\"read_file\")\nrun_agent_task_with_tracing(\"delete_file\")\n\nprovider.shutdown()\n</code></pre>"},{"location":"getting-started/#step-5-query-your-traces","title":"Step 5: Query Your Traces","text":"<p>After running your agent, you can query the traces from the SQLite database:</p> <pre><code>from clearstone.observability import TracerProvider\n\nprovider = TracerProvider(db_path=\"agent_traces.db\")\ntraces = provider.trace_store.list_traces(limit=10)\n\nfor trace in traces:\n    print(f\"Trace ID: {trace.trace_id}\")\n    print(f\"Root Span: {trace.root_span.name}\")\n    print(f\"Duration: {trace.root_span.duration_ms:.2f}ms\")\n    print(f\"Status: {trace.root_span.status}\")\n    print(\"---\")\n</code></pre>"},{"location":"getting-started/#testing-your-policies","title":"Testing Your Policies","text":"<p>Use the testing framework to validate your agent's behavior:</p> <pre><code>from clearstone.testing import PolicyTestHarness, assert_tool_was_called, assert_no_errors_in_trace\n\nharness = PolicyTestHarness(\"agent_traces.db\")\ntraces = harness.load_traces()\n\nread_file_check = assert_tool_was_called(\"read_file\", times=1)\nno_errors_check = assert_no_errors_in_trace()\n\nresult = harness.simulate_policy(read_file_check, traces)\nsummary = result.summary()\n\nif summary[\"runs_blocked\"] == 0:\n    print(\"\u2705 Test passed: Agent used read_file correctly\")\nelse:\n    print(f\"\u274c Test failed: {summary['runs_blocked']} traces blocked\")\n</code></pre>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<p>You've now created a policy-protected agent with full observability and testing! Here are some next steps:</p> <ul> <li>Core Concepts: Understand the three pillars of Clearstone</li> <li>Pre-Built Policies: Explore 17+ production-ready policies</li> <li>Governance Guide: Learn about all policy decision types (BLOCK, ALERT, PAUSE, REDACT)</li> <li>Observability Guide: Deep dive into distributed tracing</li> <li>Testing Guide: Master behavioral assertions and backtesting</li> <li>Time-Travel Debugging: Debug agents by traveling back in time</li> </ul>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: You're reading it! Browse the sections in the navigation.</li> <li>GitHub Issues: Report bugs or request features at github.com/Sancauid/clearstone-sdk</li> <li>Examples: Check out the <code>examples/</code> directory in the repository for more complete demonstrations.</li> </ul>"},{"location":"policies/","title":"Pre-Built Policy Library","text":"<p>Clearstone provides 17+ production-ready policies for common governance scenarios. Import them from <code>clearstone.policies.common</code> and use them immediately.</p>"},{"location":"policies/#cost-control-policies","title":"Cost Control Policies","text":""},{"location":"policies/#token_limit_policy","title":"token_limit_policy","text":"<p>Block execution if token usage exceeds a threshold.</p> <p>Priority: 100</p> <p>Metadata Required: - <code>token_limit</code>: Maximum tokens allowed (int) - <code>tokens_used</code>: Current tokens consumed (int, defaults to 0)</p> <p>Example: <pre><code>from clearstone import create_context, PolicyEngine\nfrom clearstone.policies.common import token_limit_policy\n\nengine = PolicyEngine()\n\ncontext = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\"token_limit\": 5000, \"tokens_used\": 6000}\n)\n\ndecision = engine.evaluate(context)\n</code></pre></p>"},{"location":"policies/#session_cost_limit_policy","title":"session_cost_limit_policy","text":"<p>Alert if session cost exceeds a threshold.</p> <p>Priority: 100</p> <p>Metadata Required: - <code>session_cost_limit</code>: Maximum session cost in dollars (float) - <code>session_cost</code>: Current session cost in dollars (float, defaults to 0)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\"session_cost_limit\": 50.0, \"session_cost\": 55.0}\n)\n</code></pre></p>"},{"location":"policies/#daily_cost_limit_policy","title":"daily_cost_limit_policy","text":"<p>Block execution if daily cost exceeds a threshold.</p> <p>Priority: 100</p> <p>Metadata Required: - <code>daily_cost_limit</code>: Maximum daily cost in dollars (float) - <code>daily_cost</code>: Current daily cost in dollars (float, defaults to 0)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\"daily_cost_limit\": 1000.0, \"daily_cost\": 1250.0}\n)\n</code></pre></p>"},{"location":"policies/#require_approval_for_high_cost_policy","title":"require_approval_for_high_cost_policy","text":"<p>Pause for human approval if operation cost exceeds threshold.</p> <p>Priority: 90</p> <p>Metadata Required: - <code>operation_cost</code>: Estimated cost of operation (float) - <code>high_cost_threshold</code>: Cost threshold requiring approval (float, defaults to 10.0)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\"operation_cost\": 25.0, \"high_cost_threshold\": 10.0}\n)\n</code></pre></p>"},{"location":"policies/#access-control-policies","title":"Access Control Policies","text":""},{"location":"policies/#rbac_tool_access_policy","title":"rbac_tool_access_policy","text":"<p>Block tool access based on user role.</p> <p>Priority: 90</p> <p>Metadata Required: - <code>user_role</code>: User's role (str, defaults to \"guest\") - <code>tool_name</code>: Name of the tool being called (str) - <code>restricted_tools</code>: Dict mapping roles to forbidden tools (dict)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\n        \"user_role\": \"guest\",\n        \"tool_name\": \"delete_database\",\n        \"restricted_tools\": {\n            \"guest\": [\"delete_database\", \"admin_panel\"],\n            \"user\": [\"admin_panel\"]\n        }\n    }\n)\n</code></pre></p>"},{"location":"policies/#admin_only_action_policy","title":"admin_only_action_policy","text":"<p>Block execution unless user is admin.</p> <p>Priority: 95</p> <p>Metadata Required: - <code>user_role</code>: User's role (str, defaults to \"guest\") - <code>tool_name</code>: Name of the tool being called (str) - <code>require_admin_for</code>: List of tool names requiring admin role (list)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\n        \"user_role\": \"user\",\n        \"tool_name\": \"delete_all_users\",\n        \"require_admin_for\": [\"delete_all_users\", \"export_database\"]\n    }\n)\n</code></pre></p>"},{"location":"policies/#data-protection-policies","title":"Data Protection Policies","text":""},{"location":"policies/#redact_pii_policy","title":"redact_pii_policy","text":"<p>Redact PII fields from outputs automatically.</p> <p>Priority: 85</p> <p>Metadata Required: - <code>tool_name</code>: Name of tool being called (str) - <code>pii_fields</code>: Dict mapping tool names to fields to redact (dict)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\n        \"tool_name\": \"fetch_user_data\",\n        \"pii_fields\": {\n            \"fetch_user_data\": [\"ssn\", \"credit_card\", \"email\"],\n            \"get_medical_records\": [\"diagnosis\", \"prescription\"]\n        }\n    }\n)\n</code></pre></p>"},{"location":"policies/#block_pii_tools_policy","title":"block_pii_tools_policy","text":"<p>Block access to PII-sensitive tools for non-privileged users.</p> <p>Priority: 90</p> <p>Metadata Required: - <code>user_role</code>: User's role (str, defaults to \"guest\") - <code>tool_name</code>: Tool being called (str) - <code>pii_tools</code>: List of PII-sensitive tool names (list)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\n        \"user_role\": \"guest\",\n        \"tool_name\": \"fetch_ssn\",\n        \"pii_tools\": [\"fetch_ssn\", \"get_credit_card\", \"view_medical_records\"]\n    }\n)\n</code></pre></p>"},{"location":"policies/#safety-policies","title":"Safety Policies","text":""},{"location":"policies/#block_dangerous_tools_policy","title":"block_dangerous_tools_policy","text":"<p>Block inherently dangerous tools (delete, drop, truncate, etc).</p> <p>Priority: 100</p> <p>Metadata Required: - <code>tool_name</code>: Name of tool being called (str)</p> <p>Dangerous Operations Blocked: - delete_database - drop_table - truncate - format_drive - shutdown - restart - hard_delete - purge - destroy</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\"tool_name\": \"drop_table\"}\n)\n</code></pre></p>"},{"location":"policies/#pause_before_write_policy","title":"pause_before_write_policy","text":"<p>Pause (for manual review) before any write/delete operation.</p> <p>Priority: 80</p> <p>Metadata Required: - <code>tool_name</code>: Name of tool (str) - <code>require_pause_for</code>: List of write operations requiring pause (list)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\n        \"tool_name\": \"delete_user\",\n        \"require_pause_for\": [\"create\", \"update\", \"delete\", \"modify\"]\n    }\n)\n</code></pre></p>"},{"location":"policies/#security-policies","title":"Security Policies","text":""},{"location":"policies/#alert_on_privileged_access_policy","title":"alert_on_privileged_access_policy","text":"<p>Alert security team when privileged tools are accessed.</p> <p>Priority: 75</p> <p>Metadata Required: - <code>tool_name</code>: Name of tool (str) - <code>privileged_tools</code>: List of privileged tools to alert on (list) - <code>user_id</code>: User performing the action (str, from context)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\n        \"tool_name\": \"export_all_data\",\n        \"privileged_tools\": [\"export_all_data\", \"admin_console\", \"grant_permissions\"]\n    }\n)\n</code></pre></p>"},{"location":"policies/#alert_on_failed_auth_policy","title":"alert_on_failed_auth_policy","text":"<p>Alert on suspicious authentication failures.</p> <p>Priority: 100</p> <p>Metadata Required: - <code>auth_failed</code>: Boolean indicating auth failure (bool) - <code>user_id</code>: User ID (str, from context) - <code>attempt_count</code>: Number of failed attempts (int, defaults to 1)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\"auth_failed\": True, \"attempt_count\": 5}\n)\n</code></pre></p>"},{"location":"policies/#rate-limiting-policies","title":"Rate Limiting Policies","text":""},{"location":"policies/#rate_limit_policy","title":"rate_limit_policy","text":"<p>Block execution if rate limit is exceeded.</p> <p>Priority: 95</p> <p>Metadata Required: - <code>rate_limit</code>: Maximum requests allowed in time window (int) - <code>rate_count</code>: Current request count (int)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\"rate_limit\": 100, \"rate_count\": 105}\n)\n</code></pre></p>"},{"location":"policies/#business_hours_only_policy","title":"business_hours_only_policy","text":"<p>Block execution outside of business hours.</p> <p>Priority: 70</p> <p>Metadata Required: - <code>current_hour</code>: Current hour 0-23 (int, optional - defaults to current time) - <code>business_hours</code>: Tuple of (start_hour, end_hour) (tuple, defaults to (9, 17))</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\"current_hour\": 22, \"business_hours\": (9, 17)}\n)\n</code></pre></p>"},{"location":"policies/#block_external_apis_policy","title":"block_external_apis_policy","text":"<p>Block calls to external APIs that are not whitelisted.</p> <p>Priority: 85</p> <p>Metadata Required: - <code>tool_name</code>: Name of tool being called (str) - <code>external_api_tools</code>: List of tools that call external APIs (list) - <code>whitelisted_apis</code>: List of allowed external API tools (list, defaults to [])</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\n        \"tool_name\": \"call_third_party_api\",\n        \"external_api_tools\": [\"call_third_party_api\", \"fetch_weather\"],\n        \"whitelisted_apis\": [\"fetch_weather\"]\n    }\n)\n</code></pre></p>"},{"location":"policies/#local-system-policies","title":"Local System Policies","text":"<p>These policies are specifically designed for users running local LLMs.</p>"},{"location":"policies/#system_load_policy","title":"system_load_policy","text":"<p>Blocks new, intensive actions if the local system's CPU or memory is overloaded.</p> <p>Priority: 200 (Highest - Critical for system stability)</p> <p>Metadata Required: - <code>cpu_threshold_percent</code>: CPU usage percent to trigger block (optional, default 90) - <code>memory_threshold_percent</code>: Memory usage percent to trigger block (optional, default 95)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\n        \"cpu_threshold_percent\": 85.0,\n        \"memory_threshold_percent\": 90.0\n    }\n)\n</code></pre></p> <p>Why This Matters: Prevents system freezes when running resource-intensive local LLMs (Ollama, LM Studio, etc).</p>"},{"location":"policies/#model_health_check_policy","title":"model_health_check_policy","text":"<p>Performs a quick health check on a local model server endpoint before allowing an LLM call.</p> <p>Priority: 190</p> <p>Metadata Required: - <code>local_model_health_url</code>: Health check endpoint URL (optional, default: \"http://localhost:11434/api/tags\") - <code>health_check_timeout</code>: Timeout in seconds (optional, default: 0.5)</p> <p>Example: <pre><code>context = create_context(\n    \"user_123\", \"agent_1\",\n    metadata={\n        \"local_model_health_url\": \"http://localhost:11434/api/tags\",\n        \"health_check_timeout\": 1.0\n    }\n)\n</code></pre></p> <p>Why This Matters: Provides instant feedback when your local model server is down, avoiding mysterious 60-second timeout errors.</p>"},{"location":"policies/#policy-sets","title":"Policy Sets","text":"<p>Pre-configured policy combinations for common use cases.</p>"},{"location":"policies/#create_safe_mode_policies","title":"create_safe_mode_policies()","text":"<p>Conservative execution mode for production environments.</p> <p>Includes: - block_dangerous_tools_policy - pause_before_write_policy - token_limit_policy - alert_on_privileged_access_policy</p> <p>Example: <pre><code>from clearstone import PolicyEngine\nfrom clearstone.policies.common import create_safe_mode_policies\n\nengine = PolicyEngine()\n\nfor policy in create_safe_mode_policies():\n    engine.register_policy(policy)\n</code></pre></p>"},{"location":"policies/#create_audit_mode_policies","title":"create_audit_mode_policies()","text":"<p>Full audit logging for compliance.</p> <p>Includes: - alert_on_privileged_access_policy - alert_on_failed_auth_policy - rbac_tool_access_policy - rate_limit_policy</p> <p>Example: <pre><code>from clearstone.policies.common import create_audit_mode_policies\n\nfor policy in create_audit_mode_policies():\n    engine.register_policy(policy)\n</code></pre></p>"},{"location":"policies/#create_cost_control_policies","title":"create_cost_control_policies()","text":"<p>Strict cost control for budget management.</p> <p>Includes: - token_limit_policy - session_cost_limit_policy - daily_cost_limit_policy - require_approval_for_high_cost_policy</p> <p>Example: <pre><code>from clearstone.policies.common import create_cost_control_policies\n\nfor policy in create_cost_control_policies():\n    engine.register_policy(policy)\n</code></pre></p>"},{"location":"policies/#create_security_policies","title":"create_security_policies()","text":"<p>Comprehensive security enforcement.</p> <p>Includes: - rbac_tool_access_policy - admin_only_action_policy - block_pii_tools_policy - alert_on_failed_auth_policy - alert_on_privileged_access_policy - block_dangerous_tools_policy</p> <p>Example: <pre><code>from clearstone.policies.common import create_security_policies\n\nfor policy in create_security_policies():\n    engine.register_policy(policy)\n</code></pre></p>"},{"location":"policies/#create_data_protection_policies","title":"create_data_protection_policies()","text":"<p>Sensitive data protection.</p> <p>Includes: - redact_pii_policy - block_pii_tools_policy - admin_only_action_policy</p> <p>Example: <pre><code>from clearstone.policies.common import create_data_protection_policies\n\nfor policy in create_data_protection_policies():\n    engine.register_policy(policy)\n</code></pre></p>"},{"location":"policies/#using-pre-built-policies","title":"Using Pre-Built Policies","text":""},{"location":"policies/#option-1-import-and-register","title":"Option 1: Import and Register","text":"<pre><code>from clearstone import PolicyEngine\nfrom clearstone.policies.common import token_limit_policy, cost_limit_policy\n\nengine = PolicyEngine()\n</code></pre> <p>Policies are automatically registered when imported.</p>"},{"location":"policies/#option-2-use-policy-sets","title":"Option 2: Use Policy Sets","text":"<pre><code>from clearstone import PolicyEngine\nfrom clearstone.policies.common import create_security_policies\n\nengine = PolicyEngine()\n\nfor policy in create_security_policies():\n    pass\n</code></pre>"},{"location":"policies/#option-3-compose-policies","title":"Option 3: Compose Policies","text":"<pre><code>from clearstone import compose_and\nfrom clearstone.policies.common import token_limit_policy, cost_limit_policy\n\ncombined = compose_and(token_limit_policy, cost_limit_policy)\n</code></pre>"},{"location":"policies/#customizing-pre-built-policies","title":"Customizing Pre-Built Policies","text":"<p>You can extend or modify pre-built policies:</p> <pre><code>from clearstone import Policy, BLOCK, ALLOW\nfrom clearstone.policies.common import token_limit_policy\n\n@Policy(name=\"custom_token_limit\", priority=100)\ndef custom_token_limit(context):\n    result = token_limit_policy(context)\n\n    if result.action == \"BLOCK\":\n        return BLOCK(f\"[CUSTOM] {result.reason}\")\n\n    return result\n</code></pre>"},{"location":"policies/#next-steps","title":"Next Steps","text":"<ul> <li>Governance Guide: Learn to write your own policies</li> <li>Getting Started: See policies in action</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"about/contributing/","title":"Contributing to the Clearstone SDK","text":"<p>We're thrilled you're interested in contributing to the Clearstone SDK! Your contributions help us build a more robust and reliable toolkit for the entire AI agent ecosystem. This document provides guidelines to ensure a smooth and effective contribution process.</p>"},{"location":"about/contributing/#how-can-i-contribute","title":"How Can I Contribute?","text":"<p>There are many ways to contribute, and all are valuable. Here are a few ideas:</p> <ul> <li>\ud83d\udc1b Reporting Bugs: If you find a bug, please open an issue and provide as much detail as possible, including steps to reproduce it.</li> <li>\u2728 Suggesting Enhancements: Have an idea for a new feature, a pre-built policy, or an improvement to an existing one? We'd love to hear it. Open an issue with the \"enhancement\" label.</li> <li>\ud83d\udcdd Improving Documentation: If you find parts of the documentation that are unclear, confusing, or incomplete, a pull request with improvements is highly appreciated.</li> <li>\ud83d\udcbb Contributing Code: If you're ready to write some code, you can pick up an existing issue or propose a new feature of your own.</li> </ul>"},{"location":"about/contributing/#development-setup","title":"Development Setup","text":"<p>To get started with the codebase, you'll need to set up a local development environment.</p>"},{"location":"about/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Git</li> <li>Python 3.10+</li> <li>A fork of the repository.</li> </ul>"},{"location":"about/contributing/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li> <p>Fork &amp; Clone the Repository</p> <p>First, fork the repository to your own GitHub account. Then, clone your fork locally: <pre><code>git clone https://github.com/YOUR_USERNAME/clearstone-sdk.git\ncd clearstone-sdk\n</code></pre></p> </li> <li> <p>Create a Virtual Environment</p> <p>It is highly recommended to work inside a Python virtual environment to isolate project dependencies. <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n</code></pre></p> </li> <li> <p>Install Dependencies</p> <p>Install the package in \"editable\" mode along with all development dependencies (like <code>pytest</code>, <code>black</code>, and <code>ruff</code>). This allows you to edit the code and have the changes immediately reflected. <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> <li> <p>Run the Test Suite</p> <p>Before making any changes, ensure that the existing test suite passes. This is a crucial first step. <pre><code>pytest\n</code></pre></p> </li> </ol>"},{"location":"about/contributing/#pull-request-process","title":"Pull Request Process","text":"<p>We follow a standard, automated pull request workflow.</p> <ol> <li> <p>Create a Feature Branch</p> <p>Create a new branch from <code>main</code> for your changes. Please use a descriptive name. <pre><code>git checkout -b feature/my-new-policy-validator\n</code></pre></p> </li> <li> <p>Make Your Changes</p> <p>Write your code! Follow the existing code style and structure. Remember to add docstrings and type hints.</p> </li> <li> <p>Add or Update Tests</p> <p>Any new feature or bug fix must be accompanied by tests. *   Bug fixes should include a test that fails without the fix and passes with it. *   New features must have corresponding unit or integration tests.</p> </li> <li> <p>Format and Lint Your Code (CRITICAL STEP)</p> <p>Before committing, you must run our automated formatting and linting tools. Our CI pipeline will fail your pull request if this step is skipped. <pre><code># Automatically format all code to the project's style\nblack .\n\n# Automatically fix any fixable linting errors\nruff check . --fix\n\n# Check for any remaining, non-fixable errors\nruff check .\n</code></pre></p> </li> <li> <p>Ensure All Local Checks Pass</p> <p>Run the full test suite one last time to ensure you haven't introduced any regressions. <pre><code>pytest\n</code></pre></p> </li> <li> <p>Submit the Pull Request</p> <p>Push your branch to your fork and open a pull request against the <code>main</code> branch of the original repository. *   Our GitHub Actions CI will automatically run all tests and style checks. All checks must pass before the PR can be merged. *   Use a clear and descriptive title (e.g., \"feat: Add PolicyMetrics collector for observability\"). *   Provide a detailed description of the changes. *   If your PR addresses an existing issue, link it using <code>Fixes #123</code>.</p> </li> </ol>"},{"location":"about/contributing/#coding-style-guide","title":"Coding Style Guide","text":"<ul> <li>Our code style is enforced automatically by Black (for formatting) and Ruff (for linting).</li> <li>Please run <code>black .</code> and <code>ruff check . --fix</code> before committing your changes.</li> <li>Docstrings should follow the Google Python Style Guide.</li> </ul>"},{"location":"about/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project and everyone participating in it is governed by our Code of Conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior to the project maintainers.</p>"},{"location":"about/contributing/#license-agreement","title":"License Agreement","text":"<p>By contributing to the Clearstone SDK, you agree that your contributions will be licensed under its MIT License.</p>"},{"location":"about/license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025 Clearstone SDK</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"about/telemetry/","title":"Telemetry","text":""},{"location":"about/telemetry/#our-privacy-first-philosophy","title":"Our Privacy-First Philosophy","text":"<p>Clearstone takes user privacy seriously. We believe in transparency, minimal data collection, and making opt-out simple. This page explains exactly what anonymous telemetry we collect and how to disable it.</p>"},{"location":"about/telemetry/#what-we-collect","title":"What We Collect","text":"<p>Clearstone collects anonymous usage statistics to help us understand which features are being used and prioritize improvements. The telemetry is:</p> <ul> <li>Anonymous: No personally identifiable information</li> <li>Non-Sensitive: Only component initialization events</li> <li>Transparent: All telemetry code is open source and auditable</li> <li>Opt-Out: Easy to disable at any time</li> </ul>"},{"location":"about/telemetry/#data-collected","title":"Data Collected","text":"<ol> <li>Component Initialization Events</li> <li>When <code>PolicyEngine</code> is initialized</li> <li>When <code>TracerProvider</code> is initialized</li> <li>When <code>CheckpointManager</code> is initialized</li> <li> <p>When <code>PolicyTestHarness</code> is initialized</p> </li> <li> <p>SDK Metadata</p> </li> <li>SDK version (e.g., \"0.1.0\")</li> <li> <p>Python version (e.g., \"3.11.5\")</p> </li> <li> <p>Anonymous Identifiers</p> </li> <li>Session ID: Generated per-process, not persisted</li> <li>User ID: Anonymous UUID, stored in <code>~/.clearstone/config.json</code></li> </ol>"},{"location":"about/telemetry/#example-telemetry-event","title":"Example Telemetry Event","text":"<pre><code>{\n  \"event\": \"component_initialized\",\n  \"component\": \"PolicyEngine\",\n  \"sdk_version\": \"0.1.0\",\n  \"python_version\": \"3.11.5\",\n  \"session_id\": \"abc123\",\n  \"user_id\": \"anonymous-uuid-xyz\",\n  \"timestamp\": \"2025-01-15T10:30:00Z\"\n}\n</code></pre>"},{"location":"about/telemetry/#what-we-dont-collect","title":"What We DON'T Collect","text":"<p>We explicitly do not collect:</p> <ul> <li>\u274c Your policy logic or decisions</li> <li>\u274c Trace data or agent outputs</li> <li>\u274c User identifiers or credentials</li> <li>\u274c Any personally identifiable information (PII)</li> <li>\u274c File paths or environment variables</li> <li>\u274c API keys or secrets</li> <li>\u274c IP addresses (anonymized by default)</li> <li>\u274c Specific metadata values from your PolicyContext</li> <li>\u274c Tool names, agent names, or any application-specific data</li> </ul>"},{"location":"about/telemetry/#how-we-use-the-data","title":"How We Use the Data","text":"<p>The anonymous telemetry helps us:</p> <ol> <li>Prioritize Features: Understand which components are most used</li> <li>Improve Reliability: Detect if specific Python versions have issues</li> <li>Plan Deprecations: Safely remove unused features</li> <li>Understand Adoption: See overall SDK adoption trends</li> </ol>"},{"location":"about/telemetry/#how-to-opt-out","title":"How to Opt Out","text":""},{"location":"about/telemetry/#option-1-environment-variable-recommended","title":"Option 1: Environment Variable (Recommended)","text":"<p>Set the <code>CLEARSTONE_TELEMETRY_DISABLED</code> environment variable:</p> <pre><code>export CLEARSTONE_TELEMETRY_DISABLED=1\n</code></pre> <p>Add this to your shell profile (<code>.bashrc</code>, <code>.zshrc</code>, etc.) to make it permanent:</p> <pre><code>echo 'export CLEARSTONE_TELEMETRY_DISABLED=1' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n</code></pre>"},{"location":"about/telemetry/#option-2-configuration-file","title":"Option 2: Configuration File","text":"<p>Create or edit <code>~/.clearstone/config.json</code>:</p> <pre><code>{\n  \"telemetry\": {\n    \"disabled\": true\n  }\n}\n</code></pre> <p>Create the config file:</p> <pre><code>mkdir -p ~/.clearstone\ncat &gt; ~/.clearstone/config.json &lt;&lt; EOF\n{\n  \"telemetry\": {\n    \"disabled\": true\n  }\n}\nEOF\n</code></pre>"},{"location":"about/telemetry/#option-3-programmatic","title":"Option 3: Programmatic","text":"<p>Disable telemetry in your application code:</p> <pre><code>from clearstone.utils.telemetry import disable_telemetry\n\ndisable_telemetry()\n</code></pre>"},{"location":"about/telemetry/#verifying-telemetry-is-disabled","title":"Verifying Telemetry is Disabled","text":"<p>Check if telemetry is disabled:</p> <pre><code>from clearstone.utils.telemetry import is_telemetry_enabled\n\nif is_telemetry_enabled():\n    print(\"Telemetry is ENABLED\")\nelse:\n    print(\"Telemetry is DISABLED\")\n</code></pre>"},{"location":"about/telemetry/#data-retention","title":"Data Retention","text":"<ul> <li>Session IDs: Not persisted, exist only for the process lifetime</li> <li>User IDs: Stored locally in <code>~/.clearstone/config.json</code> only</li> <li>Telemetry Events: Sent to our servers and retained for 90 days</li> </ul>"},{"location":"about/telemetry/#auditing-the-code","title":"Auditing the Code","text":"<p>All telemetry code is open source and auditable:</p> <ul> <li>Telemetry Module: <code>clearstone/utils/telemetry.py</code></li> <li>Configuration: <code>~/.clearstone/config.json</code></li> </ul> <p>You can review the exact implementation to verify what data is collected:</p> <pre><code># View the telemetry implementation\ncat clearstone/utils/telemetry.py\n</code></pre>"},{"location":"about/telemetry/#transparency-report","title":"Transparency Report","text":"<p>We publish an annual transparency report showing: - Total number of active installations - Most-used components - SDK version distribution - Python version distribution</p> <p>No user-specific data is ever included in these reports.</p>"},{"location":"about/telemetry/#data-security","title":"Data Security","text":"<ul> <li>All telemetry is sent over HTTPS</li> <li>No authentication tokens or credentials are ever transmitted</li> <li>Anonymous IDs cannot be reverse-engineered to identify users</li> </ul>"},{"location":"about/telemetry/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"about/telemetry/#does-telemetry-slow-down-my-agent","title":"Does telemetry slow down my agent?","text":"<p>No. Telemetry events are: - Sent asynchronously (non-blocking) - Only triggered on component initialization (not during execution) - Batched and rate-limited</p>"},{"location":"about/telemetry/#can-you-identify-me-from-the-anonymous-user-id","title":"Can you identify me from the anonymous user ID?","text":"<p>No. The user ID is a randomly generated UUID with no connection to your identity, email, or any other identifying information.</p>"},{"location":"about/telemetry/#what-if-i-forget-to-opt-out-and-then-decide-to","title":"What if I forget to opt out and then decide to?","text":"<p>Just set the environment variable or config file. The SDK checks for opt-out on every process start.</p>"},{"location":"about/telemetry/#does-opting-out-affect-functionality","title":"Does opting out affect functionality?","text":"<p>No. All features work identically whether telemetry is enabled or disabled.</p>"},{"location":"about/telemetry/#do-you-sell-telemetry-data","title":"Do you sell telemetry data?","text":"<p>Absolutely not. We never sell, share, or monetize telemetry data. It's used solely for improving the SDK.</p>"},{"location":"about/telemetry/#can-i-opt-back-in","title":"Can I opt back in?","text":"<p>Yes. Simply remove the environment variable or set <code>\"disabled\": false</code> in the config file.</p>"},{"location":"about/telemetry/#contact","title":"Contact","text":"<p>If you have questions or concerns about telemetry:</p> <ul> <li>GitHub Issues: Report Privacy Concerns</li> </ul>"},{"location":"about/telemetry/#changes-to-this-policy","title":"Changes to This Policy","text":"<p>We will notify users of any changes to this telemetry policy via: - SDK release notes - GitHub announcements - Documentation updates</p> <p>Last Updated: October 2025</p>"},{"location":"api/debugging/","title":"Debugging API Reference","text":"<p>This page documents the complete API for Clearstone's time-travel debugging system.</p>"},{"location":"api/debugging/#checkpointmanager","title":"CheckpointManager","text":"<p>Creates, saves, and loads agent state checkpoints.</p> <pre><code>from clearstone.debugging import CheckpointManager\n\nmanager = CheckpointManager(checkpoint_dir=\".checkpoints\")\n</code></pre>"},{"location":"api/debugging/#clearstone.debugging.checkpoint.CheckpointManager","title":"<code>clearstone.debugging.checkpoint.CheckpointManager</code>","text":"<p>Manages the creation, storage, and retrieval of checkpoints.</p>"},{"location":"api/debugging/#clearstone.debugging.checkpoint.CheckpointManager.create_checkpoint","title":"<code>create_checkpoint(agent, trace, span_id)</code>","text":"<p>Creates a checkpoint for a given agent at a specific span within a trace.</p>"},{"location":"api/debugging/#clearstone.debugging.checkpoint.CheckpointManager.load_checkpoint","title":"<code>load_checkpoint(path)</code>","text":"<p>Loads and deserializes a checkpoint from a file.</p>"},{"location":"api/debugging/#replayengine","title":"ReplayEngine","text":"<p>Restores agent state from checkpoints and enables interactive debugging.</p> <pre><code>from clearstone.debugging import ReplayEngine\n\ncheckpoint = manager.load_checkpoint(\"checkpoint.ckpt\")\n# Pass trace_store to access all spans in the trace\nengine = ReplayEngine(checkpoint, trace_store=provider.trace_store)\n</code></pre>"},{"location":"api/debugging/#clearstone.debugging.replay.ReplayEngine","title":"<code>clearstone.debugging.replay.ReplayEngine</code>","text":"<p>Loads a checkpoint and rehydrates an agent to allow for interactive, time-travel debugging.</p>"},{"location":"api/debugging/#clearstone.debugging.replay.ReplayEngine.start_debugging_session","title":"<code>start_debugging_session(function_to_replay, mock_config, *args, **kwargs)</code>","text":"<p>Starts an interactive debugging session using pdb.</p> <p>Parameters:</p> Name Type Description Default <code>function_to_replay</code> <code>str</code> <p>The name of the method on the agent to call.</p> required <code>mock_config</code> <code>Dict[str, str]</code> <p>A mapping from a span_type (e.g., \"llm\") to the          import path of the function to mock (e.g., \"my_app.utils.llm.invoke\").          This tells the replay engine how to map trace data to your code.</p> required <code>*args</code> <p>Positional arguments to pass to the replay method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the replay method.</p> <code>{}</code> Example <p>mock_config = {     \"llm\": \"finops_autopilot.tools.llm.invoke\",     \"tool\": \"finops_autopilot.tools.cloud_api.run_tool\" } engine.start_debugging_session(\"run_analysis_step\", mock_config=mock_config)</p>"},{"location":"api/debugging/#checkpoint-models","title":"Checkpoint Models","text":""},{"location":"api/debugging/#checkpoint","title":"Checkpoint","text":"<p>Complete snapshot of agent state at a specific execution point.</p> <pre><code>checkpoint = manager.create_checkpoint(agent, trace, span_id)\n\nprint(f\"Agent: {checkpoint.agent_class_path}\")\nprint(f\"Timestamp: {checkpoint.timestamp_ns}\")\nprint(f\"State: {checkpoint.agent_state}\")\n</code></pre>"},{"location":"api/debugging/#clearstone.debugging.checkpoint.Checkpoint","title":"<code>clearstone.debugging.checkpoint.Checkpoint</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A snapshot of an agent's state at a specific point in a trace, designed to be rehydrated by the ReplayEngine.</p>"},{"location":"api/debugging/#checkpointserializer","title":"CheckpointSerializer","text":"<p>Handles serialization and deserialization of checkpoints.</p>"},{"location":"api/debugging/#clearstone.debugging.checkpoint.CheckpointSerializer","title":"<code>clearstone.debugging.checkpoint.CheckpointSerializer</code>","text":"<p>Handles the serialization and deserialization of Checkpoint objects.</p>"},{"location":"api/debugging/#clearstone.debugging.checkpoint.CheckpointSerializer.deserialize","title":"<code>deserialize(data)</code>  <code>staticmethod</code>","text":"<p>Deserializes bytes back into a Checkpoint object.</p>"},{"location":"api/debugging/#clearstone.debugging.checkpoint.CheckpointSerializer.serialize","title":"<code>serialize(checkpoint)</code>  <code>staticmethod</code>","text":"<p>Serializes a checkpoint using a hybrid JSON/pickle approach. Metadata is JSON for readability, while the agent state is pickled for fidelity.</p>"},{"location":"api/debugging/#replay-utilities","title":"Replay Utilities","text":""},{"location":"api/debugging/#deterministicexecutioncontext","title":"DeterministicExecutionContext","text":"<p>Provides deterministic execution context for reproducible debugging.</p>"},{"location":"api/debugging/#clearstone.debugging.replay.DeterministicExecutionContext","title":"<code>clearstone.debugging.replay.DeterministicExecutionContext</code>","text":"<p>A context manager that mocks non-deterministic functions (like time and random) to ensure a replay is deterministic. It can also mock user-specified external calls.</p> <p>This flexible approach allows users to specify exactly which functions to mock and what their recorded return values should be, avoiding hardcoded import paths.</p>"},{"location":"api/debugging/#clearstone.debugging.replay.DeterministicExecutionContext.__enter__","title":"<code>__enter__()</code>","text":"<p>Applies all the patches.</p>"},{"location":"api/debugging/#clearstone.debugging.replay.DeterministicExecutionContext.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Stops all patches.</p>"},{"location":"api/debugging/#clearstone.debugging.replay.DeterministicExecutionContext.__init__","title":"<code>__init__(checkpoint, mock_targets)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Checkpoint</code> <p>The checkpoint to use for context.</p> required <code>mock_targets</code> <code>Dict[str, List[Any]]</code> <p>A dictionary mapping the import path of a function to mock           to a list of its recorded return values.           Example: {\"myapp.llm.invoke\": [response1, response2]}</p> required"},{"location":"api/governance/","title":"Governance API Reference","text":"<p>This page documents the complete API for Clearstone's governance system.</p>"},{"location":"api/governance/#core-module","title":"Core Module","text":""},{"location":"api/governance/#clearstone.core.policy","title":"<code>clearstone.core.policy</code>","text":""},{"location":"api/governance/#clearstone.core.policy.PolicyEngine","title":"<code>PolicyEngine</code>","text":"<p>The central engine that evaluates registered policies against a given context.</p> <p>Parameters:</p> Name Type Description Default <code>policies</code> <code>Optional[List[Callable]]</code> <p>An optional list of decorated policy functions to use.       If provided, this exact list will be used, and auto-discovery       of other policies will be skipped. If None (default), the engine       will auto-discover all imported @Policy-decorated functions.</p> <code>None</code> <code>audit_trail</code> <code>Optional[AuditTrail]</code> <p>Optional AuditTrail instance. If None, creates a new one.</p> <code>None</code> <code>metrics</code> <code>Optional[PolicyMetrics]</code> <p>Optional PolicyMetrics instance. If None, creates a new one.</p> <code>None</code>"},{"location":"api/governance/#clearstone.core.policy.PolicyEngine.evaluate","title":"<code>evaluate(context=None)</code>","text":"<p>Evaluates an action against all registered policies using a composable veto model.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[PolicyContext]</code> <p>The PolicyContext for the evaluation. If None, it's retrieved      from the active context variable scope.</p> <code>None</code>"},{"location":"api/governance/#clearstone.core.policy.PolicyEngine.get_audit_trail","title":"<code>get_audit_trail(limit=100)</code>","text":"<p>Returns the most recent audit trail entries.</p>"},{"location":"api/governance/#clearstone.core.policy.PolicyInfo","title":"<code>PolicyInfo</code>  <code>dataclass</code>","text":"<p>Metadata about a registered policy function.</p>"},{"location":"api/governance/#clearstone.core.policy.Policy","title":"<code>Policy(name, priority=0)</code>","text":"<p>Decorator to register a function as a Clearstone policy.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A unique, human-readable identifier for the policy.</p> required <code>priority</code> <code>int</code> <p>An integer determining execution order. Higher numbers run first.</p> <code>0</code> Example <p>@Policy(name=\"block_admin_tools_for_guests\", priority=100) def my_policy(context: PolicyContext) -&gt; Decision:     if context.metadata.get(\"role\") == \"guest\":         return BLOCK(\"Guests cannot access admin tools.\")     return ALLOW</p>"},{"location":"api/governance/#clearstone.core.policy.get_policies","title":"<code>get_policies()</code>","text":"<p>Returns all registered policies, sorted by priority (descending).</p>"},{"location":"api/governance/#clearstone.core.policy.reset_policies","title":"<code>reset_policies()</code>","text":"<p>Clears the global policy registry. Primarily for testing.</p>"},{"location":"api/governance/#clearstone.core.actions","title":"<code>clearstone.core.actions</code>","text":""},{"location":"api/governance/#clearstone.core.actions.ActionType","title":"<code>ActionType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Defines the set of possible outcomes from a policy evaluation. These are simple, stateless actions represented as singleton instances.</p>"},{"location":"api/governance/#clearstone.core.actions.Decision","title":"<code>Decision</code>  <code>dataclass</code>","text":"<p>Represents a policy decision, including the action and any associated state (e.g., a reason for blocking, metadata). Frozen for immutability.</p>"},{"location":"api/governance/#clearstone.core.actions.Decision.is_block","title":"<code>is_block()</code>","text":"<p>Helper method to check if this is a blocking decision.</p>"},{"location":"api/governance/#clearstone.core.actions.Decision.is_pause","title":"<code>is_pause()</code>","text":"<p>Helper method to check if this is a pause decision.</p>"},{"location":"api/governance/#clearstone.core.actions.BLOCK","title":"<code>BLOCK(reason, **metadata)</code>","text":"<p>Factory function to create a BLOCK decision. A reason is mandatory.</p>"},{"location":"api/governance/#clearstone.core.actions.PAUSE","title":"<code>PAUSE(reason, intervention_id=None, **metadata)</code>","text":"<p>Creates a PAUSE decision, signaling a need for human intervention. An intervention_id is automatically generated if not provided.</p>"},{"location":"api/governance/#clearstone.core.actions.REDACT","title":"<code>REDACT(reason, fields, **metadata)</code>","text":"<p>Factory function to create a REDACT decision. A list of fields is mandatory.</p>"},{"location":"api/governance/#clearstone.core.context","title":"<code>clearstone.core.context</code>","text":""},{"location":"api/governance/#clearstone.core.context.PolicyContext","title":"<code>PolicyContext</code>  <code>dataclass</code>","text":"<p>Immutable execution context for a policy evaluation. It is propagated via contextvars, making it safe for threaded and asynchronous environments.</p>"},{"location":"api/governance/#clearstone.core.context.PolicyContext.current","title":"<code>current()</code>  <code>classmethod</code>","text":"<p>Retrieves the current context from the context variable.</p>"},{"location":"api/governance/#clearstone.core.context.context_scope","title":"<code>context_scope(context)</code>","text":"<p>A context manager for safely setting and automatically resetting the policy context.</p> Usage <p>ctx = create_context(...) with context_scope(ctx):     decision = policy_engine.evaluate()</p>"},{"location":"api/governance/#clearstone.core.context.create_context","title":"<code>create_context(user_id, agent_id, session_id=None, **metadata)</code>","text":"<p>Factory function for conveniently creating a new PolicyContext instance.</p>"},{"location":"api/governance/#clearstone.core.context.get_current_context","title":"<code>get_current_context()</code>","text":"<p>Retrieves the current policy context from the active scope. Returns None if no context has been set.</p>"},{"location":"api/governance/#clearstone.core.context.set_current_context","title":"<code>set_current_context(context)</code>","text":"<p>Manually sets the current policy context for the active scope. It is often safer to use the <code>context_scope</code> context manager.</p>"},{"location":"api/governance/#clearstone.core.exceptions","title":"<code>clearstone.core.exceptions</code>","text":""},{"location":"api/governance/#policy-engine","title":"Policy Engine","text":"<p>The PolicyEngine discovers, evaluates, and enforces policies at runtime.</p>"},{"location":"api/governance/#initialization","title":"Initialization","text":"<pre><code>from clearstone import PolicyEngine\n\n# Auto-discovery mode (default)\nengine = PolicyEngine()\n\n# Explicit configuration mode\nengine = PolicyEngine(policies=[policy1, policy2])\n\n# With audit trail and metrics\nfrom clearstone import AuditTrail, PolicyMetrics\n\naudit = AuditTrail()\nmetrics = PolicyMetrics()\nengine = PolicyEngine(\n    policies=[policy1, policy2],  # Optional\n    audit_trail=audit,\n    metrics=metrics\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>policies</code> (Optional[List[Callable]]): List of policy functions to use. If provided, only these policies will be evaluated (no auto-discovery). If None (default), all imported <code>@Policy</code>-decorated functions are discovered automatically.</li> <li><code>audit_trail</code> (Optional[AuditTrail]): Custom audit trail instance for logging decisions. If None, creates a new instance.</li> <li><code>metrics</code> (Optional[PolicyMetrics]): Custom metrics instance for tracking performance. If None, creates a new instance.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If no valid policies are found (either through auto-discovery or explicit list)</li> </ul>"},{"location":"api/governance/#pre-built-policies","title":"Pre-Built Policies","text":""},{"location":"api/governance/#clearstone.policies.common","title":"<code>clearstone.policies.common</code>","text":"<p>Pre-built policy library for common governance scenarios.</p> <p>This module provides battle-tested policies for: - Token &amp; Cost Control - Role-Based Access Control (RBAC) - PII &amp; Sensitive Data Protection - Dangerous Operation Prevention - Security Alerts - Time-Based Restrictions - Local System &amp; Performance</p>"},{"location":"api/governance/#clearstone.policies.common.admin_only_action_policy","title":"<code>admin_only_action_policy(context)</code>","text":"<p>Block execution unless user is admin.</p> Metadata required <ul> <li>user_role: User's role (str, defaults to \"guest\")</li> <li>tool_name: Name of the tool being called (str)</li> <li>require_admin_for: List of tool names requiring admin role (list)</li> </ul> Example <p>metadata = {     \"user_role\": \"user\",     \"tool_name\": \"delete_all_users\",     \"require_admin_for\": [\"delete_all_users\", \"export_database\"] }</p>"},{"location":"api/governance/#clearstone.policies.common.alert_on_failed_auth_policy","title":"<code>alert_on_failed_auth_policy(context)</code>","text":"<p>Alert on suspicious authentication failures.</p> Metadata required <ul> <li>auth_failed: Boolean indicating auth failure (bool)</li> <li>user_id: User ID (str, from context)</li> <li>attempt_count: Number of failed attempts (int, defaults to 1)</li> </ul> Example <p>metadata = {     \"auth_failed\": True,     \"attempt_count\": 5 }</p>"},{"location":"api/governance/#clearstone.policies.common.alert_on_privileged_access_policy","title":"<code>alert_on_privileged_access_policy(context)</code>","text":"<p>Alert security team when privileged tools are accessed.</p> Metadata required <ul> <li>tool_name: Name of tool (str)</li> <li>privileged_tools: List of privileged tools to alert on (list)</li> <li>user_id: User performing the action (str, from context)</li> </ul> Example <p>metadata = {     \"tool_name\": \"export_all_data\",     \"privileged_tools\": [\"export_all_data\", \"admin_console\", \"grant_permissions\"] }</p>"},{"location":"api/governance/#clearstone.policies.common.block_dangerous_tools_policy","title":"<code>block_dangerous_tools_policy(context)</code>","text":"<p>Block inherently dangerous tools (delete, drop, truncate, etc).</p> Metadata required <ul> <li>tool_name: Name of tool being called (str)</li> </ul> Example <p>metadata = {     \"tool_name\": \"drop_table\" }</p>"},{"location":"api/governance/#clearstone.policies.common.block_external_apis_policy","title":"<code>block_external_apis_policy(context)</code>","text":"<p>Block calls to external APIs that are not whitelisted.</p> Metadata required <ul> <li>tool_name: Name of tool being called (str)</li> <li>external_api_tools: List of tools that call external APIs (list)</li> <li>whitelisted_apis: List of allowed external API tools (list, defaults to [])</li> </ul> Example <p>metadata = {     \"tool_name\": \"call_third_party_api\",     \"external_api_tools\": [\"call_third_party_api\", \"fetch_weather\"],     \"whitelisted_apis\": [\"fetch_weather\"] }</p>"},{"location":"api/governance/#clearstone.policies.common.block_pii_tools_policy","title":"<code>block_pii_tools_policy(context)</code>","text":"<p>Block access to PII-sensitive tools for non-privileged users.</p> Metadata required <ul> <li>user_role: User's role (str, defaults to \"guest\")</li> <li>tool_name: Tool being called (str)</li> <li>pii_tools: List of PII-sensitive tool names (list)</li> </ul> Example <p>metadata = {     \"user_role\": \"guest\",     \"tool_name\": \"fetch_ssn\",     \"pii_tools\": [\"fetch_ssn\", \"get_credit_card\", \"view_medical_records\"] }</p>"},{"location":"api/governance/#clearstone.policies.common.business_hours_only_policy","title":"<code>business_hours_only_policy(context)</code>","text":"<p>Block execution outside of business hours.</p> Metadata required <ul> <li>current_hour: Current hour 0-23 (int, optional - defaults to current time)</li> <li>business_hours: Tuple of (start_hour, end_hour) (tuple, defaults to (9, 17))</li> </ul> Example <p>metadata = {     \"current_hour\": 22,     \"business_hours\": (9, 17) }</p>"},{"location":"api/governance/#clearstone.policies.common.create_audit_mode_policies","title":"<code>create_audit_mode_policies()</code>","text":"<p>Create policies for full audit logging.</p> <p>Combines: - Alert on all privileged access - Alert on failed auth - RBAC enforcement - Rate limiting</p> <p>Returns:</p> Type Description <code>List[Callable]</code> <p>List of policy functions</p>"},{"location":"api/governance/#clearstone.policies.common.create_cost_control_policies","title":"<code>create_cost_control_policies()</code>","text":"<p>Create policies for strict cost control.</p> <p>Combines: - Token limits - Session cost limits - Daily cost limits - High cost approval</p> <p>Returns:</p> Type Description <code>List[Callable]</code> <p>List of policy functions</p>"},{"location":"api/governance/#clearstone.policies.common.create_data_protection_policies","title":"<code>create_data_protection_policies()</code>","text":"<p>Create policies for sensitive data protection.</p> <p>Combines: - PII redaction - Block PII tools for non-privileged users - Admin-only access to sensitive data</p> <p>Returns:</p> Type Description <code>List[Callable]</code> <p>List of policy functions</p>"},{"location":"api/governance/#clearstone.policies.common.create_safe_mode_policies","title":"<code>create_safe_mode_policies()</code>","text":"<p>Create a set of policies for 'safe mode' (conservative execution).</p> <p>Combines: - Block dangerous tools - Pause before writes - Token limits - Alert on privileged access</p> <p>Returns:</p> Type Description <code>List[Callable]</code> <p>List of policy functions ready to be used</p>"},{"location":"api/governance/#clearstone.policies.common.create_security_policies","title":"<code>create_security_policies()</code>","text":"<p>Create comprehensive security policies.</p> <p>Combines: - RBAC - Admin-only actions - Block PII tools - Alert on failed auth - Alert on privileged access - Block dangerous tools</p> <p>Returns:</p> Type Description <code>List[Callable]</code> <p>List of policy functions</p>"},{"location":"api/governance/#clearstone.policies.common.daily_cost_limit_policy","title":"<code>daily_cost_limit_policy(context)</code>","text":"<p>Block execution if daily cost exceeds a threshold.</p> Metadata required <ul> <li>daily_cost_limit: Maximum daily cost in dollars (float)</li> <li>daily_cost: Current daily cost in dollars (float, defaults to 0)</li> </ul> Example <p>metadata = {     \"daily_cost_limit\": 1000.0,     \"daily_cost\": 1250.0 }</p>"},{"location":"api/governance/#clearstone.policies.common.model_health_check_policy","title":"<code>model_health_check_policy(context)</code>","text":"<p>Performs a quick health check on a local model server endpoint before allowing an LLM call to proceed.</p> <p>This prevents wasted time and retry loops when a local model server (Ollama, LM Studio, etc.) is down or unhealthy.</p> Required Metadata <ul> <li>local_model_health_url (optional): Health check endpoint URL.   Default: \"http://localhost:11434/api/tags\" (Ollama default)</li> <li>health_check_timeout (optional): Timeout in seconds. Default: 0.5</li> </ul> Example <p>metadata = {     \"local_model_health_url\": \"http://localhost:11434/api/tags\",     \"health_check_timeout\": 1.0 }</p>"},{"location":"api/governance/#clearstone.policies.common.pause_before_write_policy","title":"<code>pause_before_write_policy(context)</code>","text":"<p>Pause (for manual review) before any write/delete operation.</p> Metadata required <ul> <li>tool_name: Name of tool (str)</li> <li>require_pause_for: List of write operations requiring pause (list)</li> </ul> Example <p>metadata = {     \"tool_name\": \"delete_user\",     \"require_pause_for\": [\"create\", \"update\", \"delete\", \"modify\"] }</p>"},{"location":"api/governance/#clearstone.policies.common.rate_limit_policy","title":"<code>rate_limit_policy(context)</code>","text":"<p>Block execution if rate limit is exceeded.</p> Metadata required <ul> <li>rate_limit: Maximum requests allowed in time window (int)</li> <li>rate_count: Current request count (int)</li> </ul> Example <p>metadata = {     \"rate_limit\": 100,     \"rate_count\": 105 }</p>"},{"location":"api/governance/#clearstone.policies.common.rbac_tool_access_policy","title":"<code>rbac_tool_access_policy(context)</code>","text":"<p>Block tool access based on user role.</p> Metadata required <ul> <li>user_role: User's role (str, defaults to \"guest\")</li> <li>tool_name: Name of the tool being called (str)</li> <li>restricted_tools: Dict mapping roles to forbidden tools (dict)</li> </ul> Example <p>metadata = {     \"user_role\": \"guest\",     \"tool_name\": \"delete_database\",     \"restricted_tools\": {         \"guest\": [\"delete_database\", \"admin_panel\"],         \"user\": [\"admin_panel\"]     } }</p>"},{"location":"api/governance/#clearstone.policies.common.redact_pii_policy","title":"<code>redact_pii_policy(context)</code>","text":"<p>Redact PII fields from outputs automatically.</p> Metadata required <ul> <li>tool_name: Name of tool being called (str)</li> <li>pii_fields: Dict mapping tool names to fields to redact (dict)</li> </ul> Example <p>metadata = {     \"tool_name\": \"fetch_user_data\",     \"pii_fields\": {         \"fetch_user_data\": [\"ssn\", \"credit_card\", \"email\"],         \"get_medical_records\": [\"diagnosis\", \"prescription\"]     } }</p>"},{"location":"api/governance/#clearstone.policies.common.require_approval_for_high_cost_policy","title":"<code>require_approval_for_high_cost_policy(context)</code>","text":"<p>Pause for approval if operation cost exceeds threshold.</p> Metadata required <ul> <li>operation_cost: Estimated cost of operation (float)</li> <li>high_cost_threshold: Cost threshold requiring approval (float, defaults to 10.0)</li> </ul> Example <p>metadata = {     \"operation_cost\": 25.0,     \"high_cost_threshold\": 10.0 }</p>"},{"location":"api/governance/#clearstone.policies.common.session_cost_limit_policy","title":"<code>session_cost_limit_policy(context)</code>","text":"<p>Alert if session cost exceeds a threshold.</p> Metadata required <ul> <li>session_cost_limit: Maximum session cost in dollars (float)</li> <li>session_cost: Current session cost in dollars (float, defaults to 0)</li> </ul> Example <p>metadata = {     \"session_cost_limit\": 50.0,     \"session_cost\": 55.0 }</p>"},{"location":"api/governance/#clearstone.policies.common.system_load_policy","title":"<code>system_load_policy(context)</code>","text":"<p>Blocks new, intensive actions if the local system's CPU or memory is overloaded. This is a critical guardrail for users running local LLMs to prevent system freezes.</p> Required Metadata <ul> <li>cpu_threshold_percent (optional, default 90): CPU usage percent to trigger block.</li> <li>memory_threshold_percent (optional, default 95): Memory usage percent to trigger block.</li> </ul>"},{"location":"api/governance/#clearstone.policies.common.token_limit_policy","title":"<code>token_limit_policy(context)</code>","text":"<p>Block execution if token usage exceeds a threshold.</p> Metadata required <ul> <li>token_limit: Maximum tokens allowed (int)</li> <li>tokens_used: Current tokens consumed (int, defaults to 0)</li> </ul> Example <p>metadata = {     \"token_limit\": 5000,     \"tokens_used\": 6000 }</p>"},{"location":"api/governance/#utility-functions","title":"Utility Functions","text":""},{"location":"api/governance/#compose_and","title":"compose_and","text":"<p>Compose multiple policies with AND logic.</p> <pre><code>from clearstone import compose_and\nfrom clearstone.policies.common import token_limit_policy, cost_limit_policy\n\ncombined = compose_and(token_limit_policy, cost_limit_policy)\n</code></pre>"},{"location":"api/governance/#clearstone.utils.composition.compose_and","title":"<code>clearstone.utils.composition.compose_and(*policies)</code>","text":"<p>Creates a new composite policy where ALL underlying policies must ALLOW an action.</p> <p>This is a fail-safe composition. The moment any policy returns a BLOCK, the entire composition immediately returns that BLOCK decision and stops further evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>*policies</code> <code>Callable[[PolicyContext], Decision]</code> <p>A sequence of policy functions to compose.</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable[[PolicyContext], Decision]</code> <p>A new policy function that can be used by the PolicyEngine.</p> Example <p>combined = compose_and(token_limit_policy, rbac_policy, business_hours_policy)</p> <p>@Policy(name=\"combined_policy\", priority=100) def my_policy(context):     return combined(context)</p>"},{"location":"api/governance/#compose_or","title":"compose_or","text":"<p>Compose multiple policies with OR logic.</p> <pre><code>from clearstone import compose_or\n\ncombined = compose_or(admin_check_policy, superuser_check_policy)\n</code></pre>"},{"location":"api/governance/#clearstone.utils.composition.compose_or","title":"<code>clearstone.utils.composition.compose_or(*policies)</code>","text":"<p>Creates a new composite policy where ANY of the underlying policies can ALLOW an action.</p> <p>This composition returns the decision of the first policy that does not BLOCK. If all policies return BLOCK, it returns the decision of the first policy.</p> <p>Parameters:</p> Name Type Description Default <code>*policies</code> <code>Callable[[PolicyContext], Decision]</code> <p>A sequence of policy functions to compose.</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable[[PolicyContext], Decision]</code> <p>A new policy function.</p> Example <p>either = compose_or(admin_access_policy, emergency_override_policy)</p> <p>@Policy(name=\"flexible_access\", priority=90) def my_policy(context):     return either(context)</p>"},{"location":"api/governance/#developer-tools","title":"Developer Tools","text":""},{"location":"api/governance/#policyvalidator","title":"PolicyValidator","text":"<p>Validate policies before deployment.</p> <pre><code>from clearstone import PolicyValidator\n\nvalidator = PolicyValidator()\nfailures = validator.run_all_checks(my_policy)\n</code></pre>"},{"location":"api/governance/#clearstone.utils.validator.PolicyValidator","title":"<code>clearstone.utils.validator.PolicyValidator</code>","text":"<p>A tool for running pre-deployment checks on policy functions to ensure they are safe, performant, and deterministic.</p> Example <p>validator = PolicyValidator() failures = validator.run_all_checks(my_policy) if failures:     print(\"Policy validation failed:\")     for failure in failures:         print(f\"  - {failure}\")</p>"},{"location":"api/governance/#clearstone.utils.validator.PolicyValidator.__init__","title":"<code>__init__(default_context=None)</code>","text":"<p>Initializes the validator.</p> <p>Parameters:</p> Name Type Description Default <code>default_context</code> <code>PolicyContext</code> <p>A sample PolicyContext to use for tests. If None,              a generic one will be created.</p> <code>None</code>"},{"location":"api/governance/#clearstone.utils.validator.PolicyValidator.run_all_checks","title":"<code>run_all_checks(policy)</code>","text":"<p>Runs all validation checks on a single policy and returns a list of failures.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to validate.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings, where each string is an error message. An empty list means all checks passed.</p> Example <p>validator = PolicyValidator() failures = validator.run_all_checks(my_policy) if not failures:     print(\"All checks passed!\")</p>"},{"location":"api/governance/#clearstone.utils.validator.PolicyValidator.validate_determinism","title":"<code>validate_determinism(policy, num_runs=5)</code>","text":"<p>Checks if a policy returns the same output for the same input. This catches policies that rely on non-deterministic functions (e.g., random, datetime.now()).</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to validate.</p> required <code>num_runs</code> <code>int</code> <p>Number of times to run the policy to check consistency.</p> <code>5</code> <p>Raises:</p> Type Description <code>PolicyValidationError</code> <p>If the policy produces different decisions for the same context.</p>"},{"location":"api/governance/#clearstone.utils.validator.PolicyValidator.validate_exception_safety","title":"<code>validate_exception_safety(policy)</code>","text":"<p>Checks if a policy crashes when given a context with missing metadata. A safe policy should handle missing keys gracefully (e.g., using .get() with defaults).</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to validate.</p> required <p>Raises:</p> Type Description <code>PolicyValidationError</code> <p>If the policy raises an unexpected exception.</p>"},{"location":"api/governance/#clearstone.utils.validator.PolicyValidator.validate_performance","title":"<code>validate_performance(policy, max_latency_ms=1.0, num_runs=1000)</code>","text":"<p>Checks if a policy executes within a given latency budget. This catches slow policies that might perform network requests or heavy computation.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to validate.</p> required <code>max_latency_ms</code> <code>float</code> <p>Maximum acceptable average latency in milliseconds.</p> <code>1.0</code> <code>num_runs</code> <code>int</code> <p>Number of runs to average over.</p> <code>1000</code> <p>Raises:</p> Type Description <code>PolicyValidationError</code> <p>If the policy's average execution time exceeds the threshold.</p>"},{"location":"api/governance/#policydebugger","title":"PolicyDebugger","text":"<p>Debug policy decision-making.</p> <pre><code>from clearstone import PolicyDebugger\n\ndebugger = PolicyDebugger()\ndecision, trace = debugger.trace_evaluation(my_policy, context)\n</code></pre>"},{"location":"api/governance/#clearstone.utils.debugging.PolicyDebugger","title":"<code>clearstone.utils.debugging.PolicyDebugger</code>","text":"<p>Provides tools to trace the execution of a single policy function, offering insight into its decision-making process.</p> Example <p>debugger = PolicyDebugger() decision, trace = debugger.trace_evaluation(my_policy, context) print(debugger.format_trace(my_policy, decision, trace))</p>"},{"location":"api/governance/#clearstone.utils.debugging.PolicyDebugger.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PolicyDebugger.</p>"},{"location":"api/governance/#clearstone.utils.debugging.PolicyDebugger.format_trace","title":"<code>format_trace(policy, decision, trace)</code>","text":"<p>Formats the output of a trace_evaluation into a human-readable string.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable</code> <p>The policy function that was traced.</p> required <code>decision</code> <code>Decision</code> <p>The final decision from the policy.</p> required <code>trace</code> <code>List[Dict[str, Any]]</code> <p>The trace events from trace_evaluation.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A formatted string showing the execution path.</p> Example <p>formatted = debugger.format_trace(my_policy, decision, trace) print(formatted)</p>"},{"location":"api/governance/#clearstone.utils.debugging.PolicyDebugger.trace_evaluation","title":"<code>trace_evaluation(policy, context)</code>","text":"<p>Executes a policy and records each line of code that runs, along with the state of local variables at that line.</p> <p>This uses Python's <code>sys.settrace</code> for a robust, line-by-line trace.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to debug.</p> required <code>context</code> <code>PolicyContext</code> <p>The PolicyContext to run the policy against.</p> required <p>Returns:</p> Type Description <code>Decision</code> <p>A tuple containing:</p> <code>List[Dict[str, Any]]</code> <ul> <li>The final Decision made by the policy.</li> </ul> <code>Tuple[Decision, List[Dict[str, Any]]]</code> <ul> <li>A list of trace events (dictionaries).</li> </ul> Example <p>decision, trace = debugger.trace_evaluation(my_policy, ctx) for event in trace:     print(f\"Line {event['line_no']}: {event['line_text']}\")</p>"},{"location":"api/governance/#policymetrics","title":"PolicyMetrics","text":"<p>Track policy performance metrics.</p> <pre><code>from clearstone import PolicyMetrics\n\nmetrics = PolicyMetrics()\nengine = PolicyEngine(metrics=metrics)\n</code></pre>"},{"location":"api/governance/#clearstone.utils.metrics.PolicyMetrics","title":"<code>clearstone.utils.metrics.PolicyMetrics</code>","text":"<p>A simple, in-memory collector for policy performance and decision metrics. This class is zero-dependency and designed for local-first analysis.</p>"},{"location":"api/governance/#clearstone.utils.metrics.PolicyMetrics.get_slowest_policies","title":"<code>get_slowest_policies(top_n=5)</code>","text":"<p>Returns the top N policies sorted by average latency.</p>"},{"location":"api/governance/#clearstone.utils.metrics.PolicyMetrics.get_top_blocking_policies","title":"<code>get_top_blocking_policies(top_n=5)</code>","text":"<p>Returns the top N policies that blocked most often.</p>"},{"location":"api/governance/#clearstone.utils.metrics.PolicyMetrics.record","title":"<code>record(policy_name, decision, latency_ms)</code>","text":"<p>Records a single policy evaluation event.</p>"},{"location":"api/governance/#clearstone.utils.metrics.PolicyMetrics.summary","title":"<code>summary()</code>","text":"<p>Returns a summary of all collected metrics, calculating averages.</p>"},{"location":"api/governance/#audittrail","title":"AuditTrail","text":"<p>Generate exportable audit logs.</p> <pre><code>from clearstone import AuditTrail\n\naudit = AuditTrail()\nengine = PolicyEngine(audit_trail=audit)\n</code></pre>"},{"location":"api/governance/#clearstone.utils.audit.AuditTrail","title":"<code>clearstone.utils.audit.AuditTrail</code>","text":"<p>Captures and provides utilities for analyzing a sequence of policy decisions.</p> Example <p>audit = AuditTrail() engine = PolicyEngine(audit_trail=audit)</p>"},{"location":"api/governance/#clearstone.utils.audit.AuditTrail--run-policies","title":"... run policies ...","text":"<p>print(audit.summary()) audit.to_json(\"audit_log.json\")</p>"},{"location":"api/governance/#clearstone.utils.audit.AuditTrail.get_entries","title":"<code>get_entries(limit=0)</code>","text":"<p>Returns the recorded audit entries.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>If &gt; 0, returns only the last N entries. If 0, returns all.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of audit entry dictionaries.</p>"},{"location":"api/governance/#clearstone.utils.audit.AuditTrail.record_decision","title":"<code>record_decision(policy_name, context, decision, error=None)</code>","text":"<p>Records a single policy evaluation event.</p> <p>Parameters:</p> Name Type Description Default <code>policy_name</code> <code>str</code> <p>Name of the policy that made the decision.</p> required <code>context</code> <code>PolicyContext</code> <p>The PolicyContext for this evaluation.</p> required <code>decision</code> <code>Decision</code> <p>The Decision returned by the policy.</p> required <code>error</code> <code>str</code> <p>Optional error message if the policy raised an exception.</p> <code>None</code>"},{"location":"api/governance/#clearstone.utils.audit.AuditTrail.summary","title":"<code>summary()</code>","text":"<p>Calculates and returns a summary of the decisions in the trail.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with summary statistics including:</p> <code>Dict[str, Any]</code> <ul> <li>total_decisions: Total number of decisions recorded</li> </ul> <code>Dict[str, Any]</code> <ul> <li>blocks: Number of BLOCK decisions</li> </ul> <code>Dict[str, Any]</code> <ul> <li>alerts: Number of ALERT decisions</li> </ul> <code>Dict[str, Any]</code> <ul> <li>block_rate: Ratio of blocks to total decisions</li> </ul>"},{"location":"api/governance/#clearstone.utils.audit.AuditTrail.to_csv","title":"<code>to_csv(filepath, **kwargs)</code>","text":"<p>Exports the audit trail to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the output CSV file.</p> required <code>**kwargs</code> <p>Additional arguments passed to csv.DictWriter.</p> <code>{}</code> Example <p>audit.to_csv(\"audit_log.csv\")</p>"},{"location":"api/governance/#clearstone.utils.audit.AuditTrail.to_json","title":"<code>to_json(filepath, **kwargs)</code>","text":"<p>Exports the audit trail to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the output JSON file.</p> required <code>**kwargs</code> <p>Additional arguments passed to json.dump().</p> <code>{}</code> Example <p>audit.to_json(\"audit_log.json\", indent=2)</p>"},{"location":"api/governance/#langchain-integration","title":"LangChain Integration","text":""},{"location":"api/governance/#clearstone.integrations.langchain.callbacks","title":"<code>clearstone.integrations.langchain.callbacks</code>","text":""},{"location":"api/governance/#clearstone.integrations.langchain.callbacks.PolicyCallbackHandler","title":"<code>PolicyCallbackHandler</code>","text":"<p>               Bases: <code>BaseCallbackHandler</code></p> <p>Integrates Clearstone policies into the LangChain execution lifecycle.</p>"},{"location":"api/governance/#clearstone.integrations.langchain.callbacks.PolicyCallbackHandler.on_llm_start","title":"<code>on_llm_start(serialized, prompts, **kwargs)</code>","text":"<p>Decision Point 1: Before an LLM call.</p>"},{"location":"api/governance/#clearstone.integrations.langchain.callbacks.PolicyCallbackHandler.on_tool_start","title":"<code>on_tool_start(serialized, input_str, **kwargs)</code>","text":"<p>Decision Point 2: Before a tool is executed.</p>"},{"location":"api/governance/#clearstone.integrations.langchain.callbacks.PolicyPauseError","title":"<code>PolicyPauseError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception raised when a policy returns a PAUSE decision.</p>"},{"location":"api/governance/#clearstone.integrations.langchain.callbacks.PolicyViolationError","title":"<code>PolicyViolationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception raised when a policy returns a BLOCK decision.</p>"},{"location":"api/governance/#clearstone.integrations.langchain.decorators","title":"<code>clearstone.integrations.langchain.decorators</code>","text":""},{"location":"api/governance/#clearstone.integrations.langchain.tools","title":"<code>clearstone.integrations.langchain.tools</code>","text":""},{"location":"api/observability/","title":"Observability API Reference","text":"<p>This page documents the complete API for Clearstone's observability and tracing system.</p>"},{"location":"api/observability/#tracerprovider","title":"TracerProvider","text":"<p>The entry point for the tracing system.</p> <pre><code>from clearstone.observability import TracerProvider\n\nprovider = TracerProvider(db_path=\"traces.db\")\ntracer = provider.get_tracer(\"my_agent\", version=\"1.0\")\n</code></pre>"},{"location":"api/observability/#clearstone.observability.provider.TracerProvider","title":"<code>clearstone.observability.provider.TracerProvider</code>","text":"<p>A central provider that manages the lifecycle of the entire tracing system, including the storage backend, buffer, and individual tracers.</p>"},{"location":"api/observability/#clearstone.observability.provider.TracerProvider.get_tracer","title":"<code>get_tracer(name, version='0.1.0')</code>","text":"<p>Gets or creates a Tracer instance. All tracers created by this provider will share the same underlying storage and buffer.</p>"},{"location":"api/observability/#clearstone.observability.provider.TracerProvider.shutdown","title":"<code>shutdown()</code>","text":"<p>Gracefully shuts down the tracing system, ensuring all buffered spans are written to storage.</p>"},{"location":"api/observability/#tracer","title":"Tracer","text":"<p>Creates and manages spans.</p> <pre><code>tracer = provider.get_tracer(\"my_agent\")\n\nwith tracer.span(\"operation_name\") as span:\n    pass\n</code></pre>"},{"location":"api/observability/#clearstone.observability.tracer.Tracer","title":"<code>clearstone.observability.tracer.Tracer</code>","text":"<p>The primary API for creating and managing spans within a trace. Supports both integrated mode (with external buffer) and legacy mode (internal buffer).</p>"},{"location":"api/observability/#clearstone.observability.tracer.Tracer.clear_buffer","title":"<code>clear_buffer()</code>","text":"<p>Clears the in-memory buffer (legacy mode only).</p>"},{"location":"api/observability/#clearstone.observability.tracer.Tracer.get_buffered_spans","title":"<code>get_buffered_spans()</code>","text":"<p>Returns a copy of the current in-memory span buffer (legacy mode only).</p>"},{"location":"api/observability/#clearstone.observability.tracer.Tracer.span","title":"<code>span(name, kind=SpanKind.INTERNAL, attributes=None)</code>","text":"<p>Creates a new span that is managed by a context manager.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A human-readable name for the operation (e.g., \"agent.think\").</p> required <code>kind</code> <code>SpanKind</code> <p>The OTel-aligned kind of the span (e.g., SpanKind.CLIENT).</p> <code>INTERNAL</code> <code>attributes</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary of initial attributes for the span.</p> <code>None</code> <p>Returns:</p> Type Description <code>SpanContextManager</code> <p>A SpanContextManager to be used in a 'with' statement.</p>"},{"location":"api/observability/#span-models","title":"Span Models","text":""},{"location":"api/observability/#span","title":"Span","text":"<p>Represents a single operation with timing and metadata.</p> <pre><code>with tracer.span(\"operation\", attributes={\"key\": \"value\"}) as span:\n    span.set_status(\"OK\")\n</code></pre>"},{"location":"api/observability/#clearstone.observability.models.Span","title":"<code>clearstone.observability.models.Span</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The core data structure for a single traced operation, designed for high-fidelity capture and replay.</p>"},{"location":"api/observability/#clearstone.observability.models.Span.duration_ns","title":"<code>duration_ns</code>  <code>property</code>","text":"<p>Calculates the span duration in nanoseconds if the span is complete.</p>"},{"location":"api/observability/#trace","title":"Trace","text":"<p>A complete execution flow (collection of spans).</p> <pre><code>trace = provider.trace_store.get_trace(trace_id)\n\nprint(f\"Root: {trace.root_span.name}\")\nfor span in trace.spans:\n    print(f\"  - {span.name}\")\n</code></pre>"},{"location":"api/observability/#clearstone.observability.models.Trace","title":"<code>clearstone.observability.models.Trace</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A collection of spans for a complete agent execution.</p>"},{"location":"api/observability/#spankind","title":"SpanKind","text":"<p>Enum defining the type of operation.</p> <pre><code>from clearstone.observability import SpanKind\n\nwith tracer.span(\"api_call\", kind=SpanKind.CLIENT):\n    pass\n</code></pre>"},{"location":"api/observability/#clearstone.observability.models.SpanKind","title":"<code>clearstone.observability.models.SpanKind</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OTel-aligned span kind categorization.</p>"},{"location":"api/observability/#tracestore","title":"TraceStore","text":"<p>Query and analyze stored traces.</p> <pre><code>trace_store = provider.trace_store\n\ntraces = trace_store.list_traces(limit=100)\ntrace = trace_store.get_trace(trace_id)\n</code></pre>"},{"location":"api/observability/#clearstone.storage.sqlite.TraceStore","title":"<code>clearstone.storage.sqlite.TraceStore</code>","text":"<p>               Bases: <code>BaseTraceStore</code></p> <p>Manages the persistence of traces to a local SQLite database. This class handles database connections, schema creation, and writing data.</p>"},{"location":"api/observability/#clearstone.storage.sqlite.TraceStore.get_trace","title":"<code>get_trace(trace_id)</code>","text":"<p>Retrieves all spans for a given trace_id and reconstructs the Trace.</p>"},{"location":"api/observability/#clearstone.storage.sqlite.TraceStore.write_spans","title":"<code>write_spans(spans)</code>","text":"<p>Writes a batch of spans to the database in a single transaction. This is designed to be called by the SpanBuffer.</p>"},{"location":"api/observability/#storage-components","title":"Storage Components","text":""},{"location":"api/observability/#spanbuffer","title":"SpanBuffer","text":"<p>Asynchronous buffer for batching span writes.</p>"},{"location":"api/observability/#clearstone.storage.sqlite.SpanBuffer","title":"<code>clearstone.storage.sqlite.SpanBuffer</code>","text":"<p>               Bases: <code>BaseSpanBuffer</code></p> <p>An in-memory, thread-safe buffer for spans that flushes them to a writer periodically or when the buffer is full. This decouples span creation from the I/O of writing to disk.</p>"},{"location":"api/observability/#clearstone.storage.sqlite.SpanBuffer.add_span","title":"<code>add_span(span)</code>","text":"<p>Add a span to the buffer. This is a non-blocking operation.</p>"},{"location":"api/observability/#clearstone.storage.sqlite.SpanBuffer.flush","title":"<code>flush()</code>","text":"<p>Manually trigger a flush of all buffered spans.</p>"},{"location":"api/observability/#clearstone.storage.sqlite.SpanBuffer.shutdown","title":"<code>shutdown()</code>","text":"<p>Flush any remaining spans and stop the background thread.</p>"},{"location":"api/testing/","title":"Testing API Reference","text":"<p>This page documents the complete API for Clearstone's AI-native testing framework.</p>"},{"location":"api/testing/#policytestharness","title":"PolicyTestHarness","text":"<p>The core testing tool for backtesting and behavioral assertions.</p> <pre><code>from clearstone.testing import PolicyTestHarness\n\nharness = PolicyTestHarness(\"agent_traces.db\")\ntraces = harness.load_traces(limit=100)\n</code></pre>"},{"location":"api/testing/#clearstone.testing.harness.PolicyTestHarness","title":"<code>clearstone.testing.harness.PolicyTestHarness</code>","text":"<p>A tool for backtesting new governance policies against a database of historical execution traces.</p>"},{"location":"api/testing/#clearstone.testing.harness.PolicyTestHarness.__del__","title":"<code>__del__()</code>","text":"<p>Ensure the database connection is closed when the object is destroyed.</p>"},{"location":"api/testing/#clearstone.testing.harness.PolicyTestHarness.__init__","title":"<code>__init__(trace_db_path)</code>","text":"<p>Initializes the harness with a path to a Clearstone trace database.</p>"},{"location":"api/testing/#clearstone.testing.harness.PolicyTestHarness.load_traces","title":"<code>load_traces(limit=100)</code>","text":"<p>Loads a set of historical traces from the database.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of recent traces to load.</p> <code>100</code>"},{"location":"api/testing/#clearstone.testing.harness.PolicyTestHarness.simulate_policy","title":"<code>simulate_policy(policy, traces)</code>","text":"<p>Simulates the impact of a trace-level policy against a set of historical traces.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[Trace], Decision]</code> <p>A policy function that takes a Trace and returns a Decision.</p> required <code>traces</code> <code>List[Trace]</code> <p>A list of Trace objects to test against.</p> required <p>Returns:</p> Type Description <code>PolicyTestResult</code> <p>A PolicyTestResult object with a full report of the simulation.</p>"},{"location":"api/testing/#clearstone.testing.harness.PolicyTestHarness.simulate_span_policy","title":"<code>simulate_span_policy(policy, traces)</code>","text":"<p>Simulates the impact of a span-level policy against a set of historical traces.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[Span], Decision]</code> <p>A policy function that takes a Span and returns a Decision.</p> required <code>traces</code> <code>List[Trace]</code> <p>A list of Trace objects to test against.</p> required <p>Returns:</p> Type Description <code>PolicyTestResult</code> <p>A PolicyTestResult object with a full report of the simulation.</p>"},{"location":"api/testing/#behavioral-assertions","title":"Behavioral Assertions","text":"<p>Pre-built assertion policies for validating agent behavior.</p>"},{"location":"api/testing/#assert_tool_was_called","title":"assert_tool_was_called","text":"<p>Verify a tool was called the expected number of times.</p> <pre><code>from clearstone.testing import assert_tool_was_called\n\npolicy = assert_tool_was_called(\"web_search\", times=3)\nresult = harness.simulate_policy(policy, traces)\n</code></pre>"},{"location":"api/testing/#clearstone.testing.assertions.assert_tool_was_called","title":"<code>clearstone.testing.assertions.assert_tool_was_called(tool_name, times=None, reason=None)</code>","text":"<p>Creates a policy that asserts a specific tool was called.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool to check for.</p> required <code>times</code> <code>int</code> <p>If provided, asserts the tool was called exactly this many times.</p> <code>None</code> <code>reason</code> <code>str</code> <p>Custom failure message.</p> <code>None</code>"},{"location":"api/testing/#assert_no_errors_in_trace","title":"assert_no_errors_in_trace","text":"<p>Validate that traces executed without errors.</p> <pre><code>from clearstone.testing import assert_no_errors_in_trace\n\npolicy = assert_no_errors_in_trace()\nresult = harness.simulate_policy(policy, traces)\n</code></pre>"},{"location":"api/testing/#clearstone.testing.assertions.assert_no_errors_in_trace","title":"<code>clearstone.testing.assertions.assert_no_errors_in_trace(reason=None)</code>","text":"<p>Creates a policy that asserts no spans in the trace have an ERROR status.</p>"},{"location":"api/testing/#assert_llm_cost_is_less_than","title":"assert_llm_cost_is_less_than","text":"<p>Ensure agent stays within budget.</p> <pre><code>from clearstone.testing import assert_llm_cost_is_less_than\n\npolicy = assert_llm_cost_is_less_than(0.50)\nresult = harness.simulate_policy(policy, traces)\n</code></pre>"},{"location":"api/testing/#clearstone.testing.assertions.assert_llm_cost_is_less_than","title":"<code>clearstone.testing.assertions.assert_llm_cost_is_less_than(max_cost, reason=None)</code>","text":"<p>Creates a policy that asserts the total LLM cost of a trace is below a threshold.</p>"},{"location":"api/testing/#assert_span_order","title":"assert_span_order","text":"<p>Validate workflow sequence is correct.</p> <pre><code>from clearstone.testing import assert_span_order\n\npolicy = assert_span_order([\"plan\", \"search\", \"synthesize\"])\nresult = harness.simulate_policy(policy, traces)\n</code></pre>"},{"location":"api/testing/#clearstone.testing.assertions.assert_span_order","title":"<code>clearstone.testing.assertions.assert_span_order(span_names, reason=None)</code>","text":"<p>Creates a policy that asserts a specific sequence of spans occurred in order. Note: This is a simple subsequence check, not a strict adjacency check.</p>"},{"location":"api/testing/#test-result-models","title":"Test Result Models","text":""},{"location":"api/testing/#testresult","title":"TestResult","text":"<p>Contains the results of policy simulation.</p> <pre><code>result = harness.simulate_policy(policy, traces)\n\nsummary = result.summary()\nprint(f\"Blocked: {summary['runs_blocked']}\")\nprint(f\"Block Rate: {summary['block_rate_percent']}\")\n</code></pre>"},{"location":"api/testing/#clearstone.testing.harness.PolicyTestResult","title":"<code>clearstone.testing.harness.PolicyTestResult</code>  <code>dataclass</code>","text":"<p>Holds the results of a single policy backtest simulation.</p>"},{"location":"api/testing/#clearstone.testing.harness.PolicyTestResult.summary","title":"<code>summary()</code>","text":"<p>Returns a dictionary summarizing the test results.</p>"},{"location":"api/utils/","title":"Utilities API Reference","text":"<p>This page documents utility modules and helper functions.</p>"},{"location":"api/utils/#policy-composition","title":"Policy Composition","text":"<p>Utilities for composing multiple policies into complex logic.</p>"},{"location":"api/utils/#clearstone.utils.composition","title":"<code>clearstone.utils.composition</code>","text":"<p>Policy composition utilities for combining multiple policies with logical operators.</p>"},{"location":"api/utils/#clearstone.utils.composition.compose_and","title":"<code>compose_and(*policies)</code>","text":"<p>Creates a new composite policy where ALL underlying policies must ALLOW an action.</p> <p>This is a fail-safe composition. The moment any policy returns a BLOCK, the entire composition immediately returns that BLOCK decision and stops further evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>*policies</code> <code>Callable[[PolicyContext], Decision]</code> <p>A sequence of policy functions to compose.</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable[[PolicyContext], Decision]</code> <p>A new policy function that can be used by the PolicyEngine.</p> Example <p>combined = compose_and(token_limit_policy, rbac_policy, business_hours_policy)</p> <p>@Policy(name=\"combined_policy\", priority=100) def my_policy(context):     return combined(context)</p>"},{"location":"api/utils/#clearstone.utils.composition.compose_or","title":"<code>compose_or(*policies)</code>","text":"<p>Creates a new composite policy where ANY of the underlying policies can ALLOW an action.</p> <p>This composition returns the decision of the first policy that does not BLOCK. If all policies return BLOCK, it returns the decision of the first policy.</p> <p>Parameters:</p> Name Type Description Default <code>*policies</code> <code>Callable[[PolicyContext], Decision]</code> <p>A sequence of policy functions to compose.</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable[[PolicyContext], Decision]</code> <p>A new policy function.</p> Example <p>either = compose_or(admin_access_policy, emergency_override_policy)</p> <p>@Policy(name=\"flexible_access\", priority=90) def my_policy(context):     return either(context)</p>"},{"location":"api/utils/#policy-validation","title":"Policy Validation","text":"<p>Tools for validating policies before deployment.</p>"},{"location":"api/utils/#clearstone.utils.validator","title":"<code>clearstone.utils.validator</code>","text":"<p>Policy validation tools for pre-deployment checks.</p> <p>This module provides utilities to validate that policies are safe, performant, and deterministic before deploying them to production.</p>"},{"location":"api/utils/#clearstone.utils.validator.PolicyValidationError","title":"<code>PolicyValidationError</code>","text":"<p>               Bases: <code>AssertionError</code></p> <p>Custom exception for policy validation failures.</p>"},{"location":"api/utils/#clearstone.utils.validator.PolicyValidator","title":"<code>PolicyValidator</code>","text":"<p>A tool for running pre-deployment checks on policy functions to ensure they are safe, performant, and deterministic.</p> Example <p>validator = PolicyValidator() failures = validator.run_all_checks(my_policy) if failures:     print(\"Policy validation failed:\")     for failure in failures:         print(f\"  - {failure}\")</p>"},{"location":"api/utils/#clearstone.utils.validator.PolicyValidator.__init__","title":"<code>__init__(default_context=None)</code>","text":"<p>Initializes the validator.</p> <p>Parameters:</p> Name Type Description Default <code>default_context</code> <code>PolicyContext</code> <p>A sample PolicyContext to use for tests. If None,              a generic one will be created.</p> <code>None</code>"},{"location":"api/utils/#clearstone.utils.validator.PolicyValidator.run_all_checks","title":"<code>run_all_checks(policy)</code>","text":"<p>Runs all validation checks on a single policy and returns a list of failures.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to validate.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings, where each string is an error message. An empty list means all checks passed.</p> Example <p>validator = PolicyValidator() failures = validator.run_all_checks(my_policy) if not failures:     print(\"All checks passed!\")</p>"},{"location":"api/utils/#clearstone.utils.validator.PolicyValidator.validate_determinism","title":"<code>validate_determinism(policy, num_runs=5)</code>","text":"<p>Checks if a policy returns the same output for the same input. This catches policies that rely on non-deterministic functions (e.g., random, datetime.now()).</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to validate.</p> required <code>num_runs</code> <code>int</code> <p>Number of times to run the policy to check consistency.</p> <code>5</code> <p>Raises:</p> Type Description <code>PolicyValidationError</code> <p>If the policy produces different decisions for the same context.</p>"},{"location":"api/utils/#clearstone.utils.validator.PolicyValidator.validate_exception_safety","title":"<code>validate_exception_safety(policy)</code>","text":"<p>Checks if a policy crashes when given a context with missing metadata. A safe policy should handle missing keys gracefully (e.g., using .get() with defaults).</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to validate.</p> required <p>Raises:</p> Type Description <code>PolicyValidationError</code> <p>If the policy raises an unexpected exception.</p>"},{"location":"api/utils/#clearstone.utils.validator.PolicyValidator.validate_performance","title":"<code>validate_performance(policy, max_latency_ms=1.0, num_runs=1000)</code>","text":"<p>Checks if a policy executes within a given latency budget. This catches slow policies that might perform network requests or heavy computation.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to validate.</p> required <code>max_latency_ms</code> <code>float</code> <p>Maximum acceptable average latency in milliseconds.</p> <code>1.0</code> <code>num_runs</code> <code>int</code> <p>Number of runs to average over.</p> <code>1000</code> <p>Raises:</p> Type Description <code>PolicyValidationError</code> <p>If the policy's average execution time exceeds the threshold.</p>"},{"location":"api/utils/#policy-debugging","title":"Policy Debugging","text":"<p>Tools for debugging policy decision-making.</p>"},{"location":"api/utils/#clearstone.utils.debugging","title":"<code>clearstone.utils.debugging</code>","text":"<p>Policy debugging tools for tracing execution and understanding policy decisions.</p>"},{"location":"api/utils/#clearstone.utils.debugging.PolicyDebugger","title":"<code>PolicyDebugger</code>","text":"<p>Provides tools to trace the execution of a single policy function, offering insight into its decision-making process.</p> Example <p>debugger = PolicyDebugger() decision, trace = debugger.trace_evaluation(my_policy, context) print(debugger.format_trace(my_policy, decision, trace))</p>"},{"location":"api/utils/#clearstone.utils.debugging.PolicyDebugger.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PolicyDebugger.</p>"},{"location":"api/utils/#clearstone.utils.debugging.PolicyDebugger.format_trace","title":"<code>format_trace(policy, decision, trace)</code>","text":"<p>Formats the output of a trace_evaluation into a human-readable string.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable</code> <p>The policy function that was traced.</p> required <code>decision</code> <code>Decision</code> <p>The final decision from the policy.</p> required <code>trace</code> <code>List[Dict[str, Any]]</code> <p>The trace events from trace_evaluation.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A formatted string showing the execution path.</p> Example <p>formatted = debugger.format_trace(my_policy, decision, trace) print(formatted)</p>"},{"location":"api/utils/#clearstone.utils.debugging.PolicyDebugger.trace_evaluation","title":"<code>trace_evaluation(policy, context)</code>","text":"<p>Executes a policy and records each line of code that runs, along with the state of local variables at that line.</p> <p>This uses Python's <code>sys.settrace</code> for a robust, line-by-line trace.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>Callable[[PolicyContext], Decision]</code> <p>The policy function to debug.</p> required <code>context</code> <code>PolicyContext</code> <p>The PolicyContext to run the policy against.</p> required <p>Returns:</p> Type Description <code>Decision</code> <p>A tuple containing:</p> <code>List[Dict[str, Any]]</code> <ul> <li>The final Decision made by the policy.</li> </ul> <code>Tuple[Decision, List[Dict[str, Any]]]</code> <ul> <li>A list of trace events (dictionaries).</li> </ul> Example <p>decision, trace = debugger.trace_evaluation(my_policy, ctx) for event in trace:     print(f\"Line {event['line_no']}: {event['line_text']}\")</p>"},{"location":"api/utils/#policy-metrics","title":"Policy Metrics","text":"<p>Performance tracking for policies.</p>"},{"location":"api/utils/#clearstone.utils.metrics","title":"<code>clearstone.utils.metrics</code>","text":"<p>Policy performance and decision metrics collector.</p>"},{"location":"api/utils/#clearstone.utils.metrics.PolicyMetrics","title":"<code>PolicyMetrics</code>","text":"<p>A simple, in-memory collector for policy performance and decision metrics. This class is zero-dependency and designed for local-first analysis.</p>"},{"location":"api/utils/#clearstone.utils.metrics.PolicyMetrics.get_slowest_policies","title":"<code>get_slowest_policies(top_n=5)</code>","text":"<p>Returns the top N policies sorted by average latency.</p>"},{"location":"api/utils/#clearstone.utils.metrics.PolicyMetrics.get_top_blocking_policies","title":"<code>get_top_blocking_policies(top_n=5)</code>","text":"<p>Returns the top N policies that blocked most often.</p>"},{"location":"api/utils/#clearstone.utils.metrics.PolicyMetrics.record","title":"<code>record(policy_name, decision, latency_ms)</code>","text":"<p>Records a single policy evaluation event.</p>"},{"location":"api/utils/#clearstone.utils.metrics.PolicyMetrics.summary","title":"<code>summary()</code>","text":"<p>Returns a summary of all collected metrics, calculating averages.</p>"},{"location":"api/utils/#audit-trail","title":"Audit Trail","text":"<p>Audit logging for compliance and analysis.</p>"},{"location":"api/utils/#clearstone.utils.audit","title":"<code>clearstone.utils.audit</code>","text":"<p>Audit trail utilities for capturing and analyzing policy decisions.</p>"},{"location":"api/utils/#clearstone.utils.audit.AuditTrail","title":"<code>AuditTrail</code>","text":"<p>Captures and provides utilities for analyzing a sequence of policy decisions.</p> Example <p>audit = AuditTrail() engine = PolicyEngine(audit_trail=audit)</p>"},{"location":"api/utils/#clearstone.utils.audit.AuditTrail--run-policies","title":"... run policies ...","text":"<p>print(audit.summary()) audit.to_json(\"audit_log.json\")</p>"},{"location":"api/utils/#clearstone.utils.audit.AuditTrail.get_entries","title":"<code>get_entries(limit=0)</code>","text":"<p>Returns the recorded audit entries.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>If &gt; 0, returns only the last N entries. If 0, returns all.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of audit entry dictionaries.</p>"},{"location":"api/utils/#clearstone.utils.audit.AuditTrail.record_decision","title":"<code>record_decision(policy_name, context, decision, error=None)</code>","text":"<p>Records a single policy evaluation event.</p> <p>Parameters:</p> Name Type Description Default <code>policy_name</code> <code>str</code> <p>Name of the policy that made the decision.</p> required <code>context</code> <code>PolicyContext</code> <p>The PolicyContext for this evaluation.</p> required <code>decision</code> <code>Decision</code> <p>The Decision returned by the policy.</p> required <code>error</code> <code>str</code> <p>Optional error message if the policy raised an exception.</p> <code>None</code>"},{"location":"api/utils/#clearstone.utils.audit.AuditTrail.summary","title":"<code>summary()</code>","text":"<p>Calculates and returns a summary of the decisions in the trail.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with summary statistics including:</p> <code>Dict[str, Any]</code> <ul> <li>total_decisions: Total number of decisions recorded</li> </ul> <code>Dict[str, Any]</code> <ul> <li>blocks: Number of BLOCK decisions</li> </ul> <code>Dict[str, Any]</code> <ul> <li>alerts: Number of ALERT decisions</li> </ul> <code>Dict[str, Any]</code> <ul> <li>block_rate: Ratio of blocks to total decisions</li> </ul>"},{"location":"api/utils/#clearstone.utils.audit.AuditTrail.to_csv","title":"<code>to_csv(filepath, **kwargs)</code>","text":"<p>Exports the audit trail to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the output CSV file.</p> required <code>**kwargs</code> <p>Additional arguments passed to csv.DictWriter.</p> <code>{}</code> Example <p>audit.to_csv(\"audit_log.csv\")</p>"},{"location":"api/utils/#clearstone.utils.audit.AuditTrail.to_json","title":"<code>to_json(filepath, **kwargs)</code>","text":"<p>Exports the audit trail to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the output JSON file.</p> required <code>**kwargs</code> <p>Additional arguments passed to json.dump().</p> <code>{}</code> Example <p>audit.to_json(\"audit_log.json\", indent=2)</p>"},{"location":"api/utils/#serialization","title":"Serialization","text":"<p>Hybrid JSON/pickle serialization utilities.</p>"},{"location":"api/utils/#clearstone.serialization.hybrid","title":"<code>clearstone.serialization.hybrid</code>","text":""},{"location":"api/utils/#clearstone.serialization.hybrid.HybridSerializer","title":"<code>HybridSerializer</code>","text":"<p>               Bases: <code>SerializationStrategy</code></p> <p>Hybrid serialization strategy: Attempts JSON, falls back to pickle. This provides a balance of safety, portability, and fidelity.</p>"},{"location":"api/utils/#clearstone.serialization.hybrid.HybridSerializer.deserialize","title":"<code>deserialize(data)</code>","text":"<p>Deserializes data based on the embedded type tag.</p>"},{"location":"api/utils/#clearstone.serialization.hybrid.HybridSerializer.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serializes an object with type tagging for safe deserialization.</p>"},{"location":"api/utils/#clearstone.serialization.hybrid.SelectiveSnapshotCapture","title":"<code>SelectiveSnapshotCapture</code>","text":"<p>A utility for safely capturing snapshots of data for traces, with a configurable size limit to prevent storing excessively large objects.</p>"},{"location":"api/utils/#clearstone.serialization.hybrid.SelectiveSnapshotCapture.capture","title":"<code>capture(obj, max_size_bytes=None)</code>  <code>staticmethod</code>","text":"<p>Captures a snapshot of an object, respecting size limits.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary indicating if the capture was successful, and either</p> <code>Dict[str, Any]</code> <p>the serialized data or the reason for failure.</p>"},{"location":"api/utils/#clearstone.serialization.hybrid.SerializationStrategy","title":"<code>SerializationStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for serialization strategies.</p>"},{"location":"api/utils/#clearstone.serialization.hybrid.SerializationStrategy.deserialize","title":"<code>deserialize(data)</code>  <code>abstractmethod</code>","text":"<p>Deserialize object from a string.</p>"},{"location":"api/utils/#clearstone.serialization.hybrid.SerializationStrategy.serialize","title":"<code>serialize(obj)</code>  <code>abstractmethod</code>","text":"<p>Serialize object to a JSON-compatible string.</p>"},{"location":"api/utils/#human-intervention","title":"Human Intervention","text":"<p>Tools for human-in-the-loop workflows.</p>"},{"location":"api/utils/#clearstone.utils.intervention","title":"<code>clearstone.utils.intervention</code>","text":""},{"location":"api/utils/#clearstone.utils.intervention.InterventionClient","title":"<code>InterventionClient</code>","text":"<p>A simple client for handling Human-in-the-Loop (HITL) interventions. This default implementation uses command-line input/output.</p>"},{"location":"api/utils/#clearstone.utils.intervention.InterventionClient.request_intervention","title":"<code>request_intervention(decision)</code>","text":"<p>Logs a PAUSE decision as a pending intervention request.</p>"},{"location":"api/utils/#clearstone.utils.intervention.InterventionClient.wait_for_approval","title":"<code>wait_for_approval(intervention_id, prompt=None)</code>","text":"<p>Waits for a human to approve or reject a pending intervention via the CLI.</p> <p>Parameters:</p> Name Type Description Default <code>intervention_id</code> <code>str</code> <p>The unique ID of the intervention to wait for.</p> required <code>prompt</code> <code>str</code> <p>The message to display to the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the action was approved, False otherwise.</p>"},{"location":"api/utils/#telemetry","title":"Telemetry","text":"<p>Anonymous usage statistics (opt-out available).</p>"},{"location":"api/utils/#clearstone.utils.telemetry","title":"<code>clearstone.utils.telemetry</code>","text":""},{"location":"api/utils/#clearstone.utils.telemetry.TelemetryManager","title":"<code>TelemetryManager</code>","text":"<p>Manages the collection and sending of anonymous, opt-out usage data. Designed to be transparent, respectful of privacy, and have zero performance impact.</p>"},{"location":"api/utils/#clearstone.utils.telemetry.TelemetryManager.record_event","title":"<code>record_event(event_name, payload)</code>","text":"<p>Records a telemetry event to be sent in the background.</p>"},{"location":"api/utils/#clearstone.utils.telemetry.get_telemetry_manager","title":"<code>get_telemetry_manager()</code>","text":"<p>Gets the global singleton TelemetryManager instance.</p>"},{"location":"api/utils/#path-utilities","title":"Path Utilities","text":"<p>File system path utilities.</p>"},{"location":"api/utils/#clearstone.utils.paths","title":"<code>clearstone.utils.paths</code>","text":""},{"location":"api/utils/#logging","title":"Logging","text":"<p>Logging configuration and utilities.</p>"},{"location":"api/utils/#clearstone.utils.logging","title":"<code>clearstone.utils.logging</code>","text":""},{"location":"architecture/design-philosophy/","title":"Design Philosophy","text":"<p>The Clearstone SDK is designed to be a simple, powerful, and \"Pythonic\" toolkit. To achieve this, we've made several key architectural decisions that might differ from other tools you've used. Understanding these principles will help you use the SDK more effectively.</p>"},{"location":"architecture/design-philosophy/#1-explicit-configuration-over-magic","title":"1. Explicit Configuration over \"Magic\"","text":"<p>The Principle: A developer should always have explicit control over the code that runs in their application.</p> <p>The Implementation:</p> <p>The <code>PolicyEngine</code> supports two modes:</p> <ul> <li>Auto-Discovery (<code>PolicyEngine()</code>): Great for quickstarts and demos. It automatically finds all imported <code>@Policy</code> functions.</li> <li>Explicit Configuration (<code>PolicyEngine(policies=[...])</code>): The recommended pattern for production. You provide an explicit list of the only policies you want to be active.</li> </ul> <p>Why this design?</p> <p>While auto-discovery is convenient, it can lead to \"magic\" behavior where policies you didn't know about are running. In a production environment, being explicit is safer, more predictable, and easier to test. We provide the convenience of auto-discovery for development but give you the explicit control needed for production.</p> <p>Example:</p> <pre><code>from clearstone import PolicyEngine, Policy, ALLOW, BLOCK\n\n@Policy(name=\"auth_policy\", priority=100)\ndef auth_policy(context):\n    if not context.metadata.get(\"authenticated\"):\n        return BLOCK(\"Not authenticated\")\n    return ALLOW\n\n@Policy(name=\"cost_policy\", priority=90)\ndef cost_policy(context):\n    if context.metadata.get(\"cost\", 0) &gt; 100:\n        return BLOCK(\"Cost limit exceeded\")\n    return ALLOW\n\n# Development: Auto-discovery (convenient)\ndev_engine = PolicyEngine()\n\n# Production: Explicit (safe, predictable)\nprod_engine = PolicyEngine(policies=[auth_policy, cost_policy])\n</code></pre>"},{"location":"architecture/design-philosophy/#2-pydantic-models-over-custom-setters","title":"2. Pydantic Models over Custom Setters","text":"<p>The Principle: Data structures should be simple, transparent, and follow standard Python patterns.</p> <p>The Implementation:</p> <p>Our core data models, like the <code>Span</code> object from the observability pillar, are built using Pydantic. You interact with them like you would with any standard Python object or dictionary.</p> <p>Instead of:</p> <pre><code># A common pattern in other libraries (e.g., OpenTelemetry)\nspan.set_attribute(\"my_key\", \"my_value\")\n</code></pre> <p>The Clearstone Way is:</p> <pre><code># The standard, Pythonic way\nspan.attributes[\"my_key\"] = \"my_value\"\n</code></pre> <p>Why this design?</p> <p>By using standard Pydantic models, we avoid creating a custom, proprietary API for interacting with data. This means:</p> <ul> <li>\u2705 Your IDE's autocomplete and type checking work perfectly</li> <li>\u2705 The behavior is instantly familiar to any Python developer</li> <li>\u2705 You can easily serialize our objects to JSON or dictionaries using standard Pydantic methods (<code>.model_dump()</code>)</li> <li>\u2705 We chose to align with the Python ecosystem's best practices rather than inventing our own</li> </ul> <p>Example:</p> <pre><code>from clearstone.observability import TracerProvider\n\nprovider = TracerProvider(db_path=\"traces.db\")\ntracer = provider.get_tracer(\"my_agent\")\n\nwith tracer.span(\"operation\") as span:\n    # Standard Python dict access\n    span.attributes[\"user_id\"] = \"user_123\"\n    span.attributes[\"cost\"] = 0.05\n\n    # Standard Pydantic methods work\n    span_dict = span.model_dump()\n\n    # Standard Python attribute access\n    print(f\"Span ID: {span.span_id}\")\n    print(f\"Duration: {span.duration_ms}ms\")\n</code></pre>"},{"location":"architecture/design-philosophy/#3-targeted-integration-over-universal-abstractions","title":"3. Targeted Integration over Universal Abstractions","text":"<p>The Principle: Integration code should live where it is used.</p> <p>The Implementation:</p> <p>You'll notice that components specific to a particular framework are located in a dedicated integration module. For example, the <code>PolicyViolationError</code> and <code>PolicyPauseError</code> are in <code>clearstone.integrations.langchain</code>.</p> <p>Instead of:</p> <pre><code># A common pattern of putting all exceptions in a central 'core' or 'domain' module\nfrom clearstone.core import PolicyViolationError  # This does NOT exist\n</code></pre> <p>The Clearstone Way is:</p> <pre><code># Exceptions are located with the code that raises them\nfrom clearstone.integrations.langchain import PolicyViolationError\n</code></pre> <p>Why this design?</p> <p>This makes our core SDK (<code>clearstone.core</code>, <code>clearstone.observability</code>, etc.) completely framework-agnostic. The core engine knows nothing about LangChain. This is a deliberate choice that allows Clearstone to be easily adapted to any Python framework (CrewAI, AutoGen, etc.) simply by creating a new, small integration module for it. It keeps the core clean and universally applicable.</p> <p>Example:</p> <pre><code>from clearstone import PolicyEngine, create_context, context_scope\nfrom clearstone.integrations.langchain import (\n    PolicyCallbackHandler,\n    PolicyViolationError,  # LangChain-specific\n    PolicyPauseError        # LangChain-specific\n)\n\nengine = PolicyEngine()\nhandler = PolicyCallbackHandler(engine)\n\n# These exceptions are specific to the LangChain integration\ntry:\n    with context_scope(context):\n        agent.invoke(input, callbacks=[handler])\nexcept PolicyViolationError as e:\n    print(f\"Blocked: {e.decision.reason}\")\nexcept PolicyPauseError as e:\n    print(f\"Paused: {e.decision.reason}\")\n</code></pre> <p>Framework Extensibility:</p> <pre><code># clearstone/core/ - Framework-agnostic\nfrom clearstone.core import PolicyEngine\n\n# clearstone/integrations/langchain/ - LangChain-specific\nfrom clearstone.integrations.langchain import PolicyCallbackHandler\n\n# clearstone/integrations/crewai/ - CrewAI-specific (future)\n# from clearstone.integrations.crewai import CrewAIPolicyMiddleware\n\n# clearstone/integrations/autogen/ - AutoGen-specific (future)\n# from clearstone.integrations.autogen import AutoGenPolicyWrapper\n</code></pre>"},{"location":"architecture/design-philosophy/#core-design-values","title":"Core Design Values","text":"<p>These three principles reflect our broader design values:</p>"},{"location":"architecture/design-philosophy/#clarity-over-convenience","title":"Clarity over Convenience","text":"<p>We prefer code that is explicit and easy to understand over code that is \"clever\" or relies on hidden magic.</p>"},{"location":"architecture/design-philosophy/#python-standards-over-custom-patterns","title":"Python Standards over Custom Patterns","text":"<p>We use standard Python and Pydantic patterns wherever possible, rather than inventing proprietary APIs.</p>"},{"location":"architecture/design-philosophy/#modularity-over-monoliths","title":"Modularity over Monoliths","text":"<p>We keep the core SDK small, focused, and framework-agnostic. Framework-specific code lives in dedicated integration modules.</p>"},{"location":"architecture/design-philosophy/#developer-experience","title":"Developer Experience","text":"<p>We optimize for: - IDE Support: Autocomplete and type checking work perfectly - Discoverability: APIs are intuitive and follow Python conventions - Flexibility: Multiple ways to accomplish tasks (auto-discovery vs explicit) - Safety: Production use cases get explicit, predictable behavior</p>"},{"location":"architecture/design-philosophy/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts: Understand the three pillars</li> <li>Governance Guide: Learn about PolicyEngine configuration modes</li> <li>Contributing: Help us maintain these principles</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>Clearstone SDK is built on a modular, extensible architecture designed for production-grade AI agent governance and observability.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<p>The SDK is organized into several independent but complementary pillars:</p> <pre><code>clearstone/\n\u251c\u2500\u2500 core/               # Framework-agnostic governance engine\n\u251c\u2500\u2500 observability/      # Distributed tracing system\n\u251c\u2500\u2500 debugging/          # Time-travel debugging and checkpointing\n\u251c\u2500\u2500 testing/            # AI-native testing framework\n\u251c\u2500\u2500 integrations/       # Framework-specific adapters\n\u2502   \u251c\u2500\u2500 langchain/\n\u2502   \u2514\u2500\u2500 future/\n\u251c\u2500\u2500 policies/           # Pre-built policy library\n\u251c\u2500\u2500 serialization/      # Hybrid JSON/pickle serialization\n\u251c\u2500\u2500 storage/            # SQLite persistence layer\n\u2514\u2500\u2500 utils/              # Cross-cutting utilities\n</code></pre>"},{"location":"architecture/overview/#core-pillars","title":"Core Pillars","text":""},{"location":"architecture/overview/#1-governance-policy-as-code","title":"1. Governance (Policy-as-Code)","text":"<p>Location: <code>clearstone/core/</code></p> <p>The governance pillar provides declarative policy enforcement for AI agents.</p> <p>Key Components: - PolicyEngine: Discovers, evaluates, and enforces policies - Policy Decorator: Registers functions as policies with metadata - PolicyContext: Immutable execution context passed to policies - Decision Actions: ALLOW, BLOCK, ALERT, PAUSE, REDACT</p> <p>Design Characteristics: - Zero dependencies on AI frameworks - Composable policy logic - Fail-safe defaults (errors don't crash) - Performance metrics and audit trails built-in</p>"},{"location":"architecture/overview/#2-observability-distributed-tracing","title":"2. Observability (Distributed Tracing)","text":"<p>Location: <code>clearstone/observability/</code></p> <p>OpenTelemetry-aligned distributed tracing for complete agent visibility.</p> <p>Key Components: - TracerProvider: Entry point for tracing system - Tracer: Creates and manages spans - Span: Represents a single operation with timing/metadata - TraceStore: SQLite-based persistence with WAL mode</p> <p>Design Characteristics: - Non-blocking capture (&lt; 1\u03bcs overhead) - Automatic parent-child span linking - Batched writes for performance - Thread-safe operations</p>"},{"location":"architecture/overview/#3-testing-backtesting","title":"3. Testing &amp; Backtesting","text":"<p>Location: <code>clearstone/testing/</code></p> <p>AI-native testing framework for validating agent behavior.</p> <p>Key Components: - PolicyTestHarness: Simulates policy enforcement on historical traces - Behavioral Assertions: Declarative tests for agent behavior - TestResult: Impact analysis and metrics</p> <p>Design Characteristics: - Tests \"how\" agents behave, not just \"what\" they return - Historical backtesting against production data - pytest integration - Regression prevention</p>"},{"location":"architecture/overview/#4-time-travel-debugging","title":"4. Time-Travel Debugging","text":"<p>Location: <code>clearstone/debugging/</code></p> <p>Checkpoint and replay agent execution from any point in history.</p> <p>Key Components: - CheckpointManager: Creates, saves, and loads agent snapshots - ReplayEngine: Restores agent state and enables debugging - DeterministicExecutionContext: Mocks non-deterministic functions</p> <p>Design Characteristics: - Complete state preservation - Deterministic replay - Interactive debugging sessions - Upstream span tracking</p>"},{"location":"architecture/overview/#cross-cutting-concerns","title":"Cross-Cutting Concerns","text":""},{"location":"architecture/overview/#serialization","title":"Serialization","text":"<p>Location: <code>clearstone/serialization/</code></p> <p>Hybrid JSON-first approach with pickle fallback for complex objects.</p> <p>Features: - Automatic format selection based on serializability - Size limits for safety - Human-readable JSON when possible</p>"},{"location":"architecture/overview/#storage","title":"Storage","text":"<p>Location: <code>clearstone/storage/</code></p> <p>SQLite-based persistence with production-grade features.</p> <p>Features: - Write-Ahead Logging (WAL) for concurrency - Asynchronous batching with <code>SpanBuffer</code> - Thread-safe operations - Efficient indexing and querying</p>"},{"location":"architecture/overview/#integrations","title":"Integrations","text":"<p>Location: <code>clearstone/integrations/</code></p> <p>Framework-specific adapters that keep the core framework-agnostic.</p> <p>Current: - LangChain: <code>PolicyCallbackHandler</code> for automatic policy enforcement</p> <p>Future: - CrewAI integration - AutoGen integration - Custom framework support</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#policy-enforcement-flow","title":"Policy Enforcement Flow","text":"<pre><code>User Action\n    \u2193\nLangChain Callback\n    \u2193\nPolicyCallbackHandler (integration)\n    \u2193\nPolicyEngine.evaluate() (core)\n    \u2193\nPolicy Functions (sorted by priority)\n    \u2193\nDecision (ALLOW/BLOCK/PAUSE/ALERT/REDACT)\n    \u2193\nAction Taken or Exception Raised\n</code></pre>"},{"location":"architecture/overview/#tracing-flow","title":"Tracing Flow","text":"<pre><code>Agent Operation\n    \u2193\ntracer.span() context manager\n    \u2193\nSpan creation (in-memory)\n    \u2193\nSpanBuffer (batching)\n    \u2193\nTraceStore (SQLite with WAL)\n    \u2193\nQueryable trace data\n</code></pre>"},{"location":"architecture/overview/#testing-flow","title":"Testing Flow","text":"<pre><code>Production Agent Run\n    \u2193\nTraces captured to SQLite\n    \u2193\nPolicyTestHarness.load_traces()\n    \u2193\nPolicyTestHarness.simulate_policy()\n    \u2193\nTestResult with impact analysis\n    \u2193\npytest assertions\n</code></pre>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":"<p>Clearstone's architecture follows these key principles:</p> <ol> <li>Modularity: Each pillar can be used independently</li> <li>Framework Agnostic: Core SDK has zero AI framework dependencies</li> <li>Production Ready: Thread-safe, performant, fail-safe</li> <li>Developer Friendly: Standard Python patterns, strong typing</li> <li>Extensible: Easy to add new integrations and policies</li> </ol> <p>See the Design Philosophy document for deeper insights into our architectural decisions.</p>"},{"location":"architecture/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/overview/#policyengine","title":"PolicyEngine","text":"<ul> <li>Evaluation Time: &lt; 1ms per policy (typical)</li> <li>Memory Overhead: Minimal (policies are pure functions)</li> <li>Concurrency: Thread-safe evaluation</li> </ul>"},{"location":"architecture/overview/#tracing","title":"Tracing","text":"<ul> <li>Capture Overhead: &lt; 1\u03bcs per span</li> <li>Write Throughput: 10,000+ spans/second with batching</li> <li>Storage Growth: ~1KB per span (depends on attributes)</li> <li>Query Performance: Indexed by trace_id, parent_span_id</li> </ul>"},{"location":"architecture/overview/#testing","title":"Testing","text":"<ul> <li>Simulation Speed: 1,000+ traces/second</li> <li>Memory Usage: Loads traces in batches (configurable)</li> </ul>"},{"location":"architecture/overview/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"architecture/overview/#database","title":"Database","text":"<ul> <li>SQLite with WAL mode (concurrent reads, single writer)</li> <li>Recommended: Regular backups of trace database</li> <li>Optional: External storage for long-term archive</li> </ul>"},{"location":"architecture/overview/#scaling","title":"Scaling","text":"<ul> <li>Vertical: Single process handles 10,000+ policy evaluations/second</li> <li>Horizontal: Each process has its own SQLite database</li> <li>Aggregation: Use external tools to aggregate traces from multiple processes</li> </ul>"},{"location":"architecture/overview/#monitoring","title":"Monitoring","text":"<ul> <li>Built-in <code>PolicyMetrics</code> for policy performance</li> <li>Built-in <code>AuditTrail</code> for compliance logging</li> <li>Export traces to external systems via TraceStore API</li> </ul>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Design Philosophy: Understand our architectural decisions</li> <li>Core Concepts: Learn the three pillars</li> <li>API Reference: Complete API documentation</li> <li>Contributing: Help improve the architecture</li> </ul>"},{"location":"guide/core-concepts/","title":"Core Concepts","text":"<p>Clearstone is built around three foundational pillars that work together to make AI agents safe, observable, and debuggable. Understanding these core concepts will help you use Clearstone effectively.</p>"},{"location":"guide/core-concepts/#the-three-pillars","title":"The Three Pillars","text":""},{"location":"guide/core-concepts/#1-governance-policy-as-code","title":"1. Governance (Policy-as-Code)","text":"<p>Policies are declarative rules that control agent behavior at runtime. They intercept agent actions before they execute and decide whether to allow, block, modify, or pause them.</p> <p>Key Components: - @Policy Decorator: Turns a Python function into a policy - PolicyContext: Provides metadata about the current execution - Decision Actions: ALLOW, BLOCK, ALERT, PAUSE, REDACT - PolicyEngine: Evaluates all policies and enforces decisions</p> <p>Example: <pre><code>from clearstone import Policy, ALLOW, BLOCK\n\n@Policy(name=\"cost_control\", priority=100)\ndef cost_control_policy(context):\n    session_cost = context.metadata.get(\"session_cost\", 0.0)\n\n    if session_cost &gt; 50.0:\n        return BLOCK(f\"Cost limit exceeded: ${session_cost:.2f}\")\n\n    return ALLOW\n</code></pre></p>"},{"location":"guide/core-concepts/#2-observability-distributed-tracing","title":"2. Observability (Distributed Tracing)","text":"<p>Tracing captures a complete record of what your agent does at runtime. Every operation is recorded as a span with precise timing, inputs, outputs, and relationships to other spans.</p> <p>Key Components: - TracerProvider: Initializes the tracing system - Tracer: Creates and manages spans - Span: Represents a single operation with timing and metadata - Trace: A complete execution flow (collection of spans) - TraceStore: Persists traces to SQLite for later analysis</p> <p>Example: <pre><code>from clearstone.observability import TracerProvider, SpanKind\n\nprovider = TracerProvider(db_path=\"traces.db\")\ntracer = provider.get_tracer(\"my_agent\", version=\"1.0\")\n\nwith tracer.span(\"agent_workflow\", kind=SpanKind.INTERNAL) as root:\n    with tracer.span(\"llm_call\", attributes={\"model\": \"gpt-4\"}):\n        result = call_llm()\n\n    with tracer.span(\"tool_execution\", attributes={\"tool\": \"calculator\"}):\n        output = run_tool()\n\nprovider.shutdown()\n</code></pre></p>"},{"location":"guide/core-concepts/#3-debugging-time-travel-testing","title":"3. Debugging (Time-Travel &amp; Testing)","text":"<p>Checkpointing and testing allow you to debug agents retrospectively and validate behavior against historical data.</p> <p>Key Components: - CheckpointManager: Creates snapshots of agent state - ReplayEngine: Restores agent state and re-executes from any point - PolicyTestHarness: Tests policies against historical traces - Behavioral Assertions: Declarative tests for agent behavior</p> <p>Example: <pre><code>from clearstone.debugging import CheckpointManager, ReplayEngine\n\nmanager = CheckpointManager()\ncheckpoint = manager.create_checkpoint(agent, trace, span_id=\"span_abc\")\n\nengine = ReplayEngine(checkpoint)\nengine.start_debugging_session(\"process_next_step\", input_data)\n</code></pre></p>"},{"location":"guide/core-concepts/#core-abstractions","title":"Core Abstractions","text":""},{"location":"guide/core-concepts/#policycontext","title":"PolicyContext","text":"<p>The <code>PolicyContext</code> is the data structure passed to every policy function. It provides information about the current execution:</p> <pre><code>@dataclass\nclass PolicyContext:\n    user_id: str\n    agent_id: str\n    timestamp: float\n    metadata: Dict[str, Any]\n</code></pre> <p>Metadata is where you pass operation-specific data: <pre><code>context = create_context(\n    user_id=\"user_123\",\n    agent_id=\"research_agent\",\n    metadata={\n        \"tool_name\": \"web_search\",\n        \"session_cost\": 12.50,\n        \"user_role\": \"admin\"\n    }\n)\n</code></pre></p>"},{"location":"guide/core-concepts/#decision-actions","title":"Decision Actions","text":"<p>Policies return a Decision that tells the engine what to do:</p> Action Behavior Use Case ALLOW Continue execution normally Default - no issues detected BLOCK Stop execution immediately, raise error Prevent dangerous or unauthorized actions ALERT Continue but log a warning Monitor suspicious behavior PAUSE Stop and wait for human approval Require manual review for high-stakes operations REDACT Continue but remove sensitive fields Protect PII in outputs <p>Example: <pre><code>from clearstone import ALLOW, BLOCK, ALERT, PAUSE, REDACT\n\nreturn ALLOW\n\nreturn BLOCK(\"User not authorized\")\n\nreturn ALERT\n\nreturn PAUSE(\"Manual approval required for $10k transaction\")\n\nreturn REDACT(reason=\"PII protection\", fields=[\"ssn\", \"credit_card\"])\n</code></pre></p>"},{"location":"guide/core-concepts/#traces-and-spans","title":"Traces and Spans","text":"<p>A trace represents a complete agent execution. A span represents a single operation within that trace.</p> <p>Span Hierarchy: <pre><code>Trace: research_workflow\n\u251c\u2500\u2500 Span: agent_execution\n\u2502   \u251c\u2500\u2500 Span: plan_generation\n\u2502   \u251c\u2500\u2500 Span: web_search (tool)\n\u2502   \u2514\u2500\u2500 Span: synthesis\n</code></pre></p> <p>Span Attributes: <pre><code>with tracer.span(\"llm_call\", attributes={\n    \"model\": \"gpt-4\",\n    \"temperature\": 0.7,\n    \"tokens\": 1500,\n    \"cost\": 0.045\n}) as span:\n    result = call_llm()\n</code></pre></p>"},{"location":"guide/core-concepts/#checkpoints","title":"Checkpoints","text":"<p>A checkpoint is a snapshot of agent state at a specific moment in time. It includes: - Agent's complete internal state - The trace context (all parent spans) - Metadata about the execution point - Timestamp and version information</p> <p>Creating a Checkpoint: <pre><code>from clearstone.debugging import CheckpointManager\n\nmanager = CheckpointManager(checkpoint_dir=\".checkpoints\")\n\ncheckpoint = manager.create_checkpoint(\n    agent=my_agent,\n    trace=execution_trace,\n    span_id=\"span_xyz\"\n)\n\ncheckpoint_path = manager.save_checkpoint(checkpoint)\n</code></pre></p> <p>Loading a Checkpoint: <pre><code>checkpoint = manager.load_checkpoint(\"t1_ckpt_abc123.ckpt\")\n\nrestored_agent = checkpoint.agent\nexecution_context = checkpoint.trace\n</code></pre></p>"},{"location":"guide/core-concepts/#how-they-work-together","title":"How They Work Together","text":"<p>The three pillars integrate seamlessly:</p> <ol> <li>Tracing captures everything your agent does</li> <li>Policies enforce rules at runtime</li> <li>Testing validates behavior against historical traces</li> <li>Checkpoints enable time-travel debugging</li> </ol> <p>Complete Example: <pre><code>from clearstone import Policy, BLOCK, ALLOW, PolicyEngine, create_context, context_scope\nfrom clearstone.observability import TracerProvider\nfrom clearstone.testing import PolicyTestHarness, assert_tool_was_called\n\n@Policy(name=\"block_expensive_tools\", priority=100)\ndef block_expensive_tools(context):\n    tool_name = context.metadata.get(\"tool_name\")\n    if tool_name == \"gpt4_turbo\":\n        return BLOCK(\"Expensive tool blocked\")\n    return ALLOW\n\nprovider = TracerProvider(db_path=\"traces.db\")\ntracer = provider.get_tracer(\"cost_conscious_agent\")\nengine = PolicyEngine()\n\nwith tracer.span(\"agent_run\"):\n    context = create_context(\n        user_id=\"user_1\",\n        agent_id=\"agent_1\",\n        metadata={\"tool_name\": \"gpt4_turbo\"}\n    )\n\n    with context_scope(context):\n        try:\n            engine.evaluate(context)\n        except Exception as e:\n            print(f\"Blocked: {e}\")\n\nprovider.shutdown()\n\nharness = PolicyTestHarness(\"traces.db\")\ntraces = harness.load_traces()\nresult = harness.simulate_policy(\n    assert_tool_was_called(\"gpt4_turbo\", times=0),\n    traces\n)\n</code></pre></p>"},{"location":"guide/core-concepts/#key-design-principles","title":"Key Design Principles","text":""},{"location":"guide/core-concepts/#1-declarative-over-imperative","title":"1. Declarative Over Imperative","text":"<p>Policies are written as simple functions, not complex state machines. You declare what should happen, not how to enforce it.</p>"},{"location":"guide/core-concepts/#2-zero-performance-impact","title":"2. Zero Performance Impact","text":"<p>Tracing uses asynchronous batching and thread-safe operations to ensure zero impact on agent execution speed.</p>"},{"location":"guide/core-concepts/#3-composability","title":"3. Composability","text":"<p>Policies can be combined using <code>compose_and</code> and <code>compose_or</code> to build complex rules from simple parts.</p>"},{"location":"guide/core-concepts/#4-fail-safe-defaults","title":"4. Fail-Safe Defaults","text":"<p>If a policy throws an error, the engine defaults to ALLOW and logs the error. The system never crashes due to a policy bug.</p>"},{"location":"guide/core-concepts/#5-testability-first","title":"5. Testability First","text":"<p>Every feature is designed to be testable. Policies can be validated before deployment, and agent behavior can be tested against historical data.</p>"},{"location":"guide/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Governance Guide: Deep dive into writing and composing policies</li> <li>Observability Guide: Master distributed tracing</li> <li>Testing Guide: Learn behavioral testing and backtesting</li> <li>Time-Travel Debugging: Debug agents by traveling back in time</li> </ul>"},{"location":"guide/governance/","title":"Governance","text":"<p>The Governance pillar provides declarative Policy-as-Code to control agent behavior at runtime. This guide covers everything you need to write, compose, debug, and deploy production-ready policies.</p>"},{"location":"guide/governance/#the-policyengine","title":"The PolicyEngine","text":"<p>The <code>PolicyEngine</code> is the heart of the governance system. It discovers, evaluates, and enforces policies.</p>"},{"location":"guide/governance/#basic-setup","title":"Basic Setup","text":"<pre><code>from clearstone import PolicyEngine\n\nengine = PolicyEngine()\n</code></pre> <p>The engine automatically discovers all policies decorated with <code>@Policy</code> that have been imported into your application.</p>"},{"location":"guide/governance/#configuration-modes","title":"Configuration Modes","text":"<p>The <code>PolicyEngine</code> supports two configuration modes:</p>"},{"location":"guide/governance/#auto-discovery-default","title":"Auto-Discovery (Default)","text":"<p>The engine automatically discovers all imported <code>@Policy</code>-decorated functions:</p> <pre><code>from clearstone import PolicyEngine\nimport my_policies  # Policies are auto-discovered\n\nengine = PolicyEngine()\n</code></pre> <p>This is the most convenient mode for most applications. Simply import your policy modules, and the engine will find and use them.</p>"},{"location":"guide/governance/#explicit-configuration","title":"Explicit Configuration","text":"<p>Pass a specific list of policies to use only those policies:</p> <pre><code>from clearstone import PolicyEngine, Policy, ALLOW, BLOCK\n\n@Policy(name=\"policy1\", priority=10)\ndef policy1(context):\n    return ALLOW\n\n@Policy(name=\"policy2\", priority=20)\ndef policy2(context):\n    return ALLOW\n\n# Use ONLY these two policies, ignore all others\nengine = PolicyEngine(policies=[policy1, policy2])\n</code></pre> <p>When to use explicit configuration:</p> <ul> <li>Multi-Environment Deployments: Use different policy sets for development, staging, and production</li> <li>Testing Scenarios: Isolate specific policies for unit testing without interference from others</li> <li>Multi-Tenant Applications: Different tenants/customers need different policy sets</li> <li>Fine-Grained Control: You have many policies defined but only want specific ones active</li> <li>Dynamic Policy Loading: Load policies based on runtime configuration or feature flags</li> </ul> <p>Example: Environment-Specific Policies</p> <pre><code>from clearstone import PolicyEngine\nfrom my_policies import strict_auth_policy, lenient_auth_policy, cost_policy\n\nif os.getenv(\"ENV\") == \"production\":\n    engine = PolicyEngine(policies=[strict_auth_policy, cost_policy])\nelse:\n    engine = PolicyEngine(policies=[lenient_auth_policy])\n</code></pre>"},{"location":"guide/governance/#evaluating-policies","title":"Evaluating Policies","text":"<pre><code>from clearstone import create_context\n\ncontext = create_context(\n    user_id=\"user_123\",\n    agent_id=\"research_agent\",\n    metadata={\"tool_name\": \"web_search\", \"cost\": 0.05}\n)\n\ndecision = engine.evaluate(context)\n\nif decision.action == \"BLOCK\":\n    raise Exception(f\"Action blocked: {decision.reason}\")\n</code></pre>"},{"location":"guide/governance/#writing-policies","title":"Writing Policies","text":""},{"location":"guide/governance/#the-policy-decorator","title":"The @Policy Decorator","text":"<p>Every policy is a simple Python function decorated with <code>@Policy</code>:</p> <pre><code>from clearstone import Policy, ALLOW, BLOCK\n\n@Policy(name=\"my_policy\", priority=100)\ndef my_policy(context):\n    \"\"\"Description of what this policy does.\"\"\"\n\n    if context.metadata.get(\"some_condition\"):\n        return BLOCK(\"Reason for blocking\")\n\n    return ALLOW\n</code></pre> <p>Parameters: - <code>name</code>: Unique identifier for the policy (required) - <code>priority</code>: Higher priority policies are evaluated first (default: 50) - <code>enabled</code>: Whether the policy is active (default: True)</p>"},{"location":"guide/governance/#policy-priority","title":"Policy Priority","text":"<p>Policies are evaluated in descending priority order (highest first):</p> <pre><code>@Policy(name=\"security_check\", priority=100)\ndef security_policy(context):\n    pass\n\n@Policy(name=\"cost_check\", priority=80)\ndef cost_policy(context):\n    pass\n\n@Policy(name=\"logging\", priority=10)\ndef logging_policy(context):\n    pass\n</code></pre> <p>Evaluation order: <code>security_policy</code> \u2192 <code>cost_policy</code> \u2192 <code>logging_policy</code></p>"},{"location":"guide/governance/#short-circuit-evaluation","title":"Short-Circuit Evaluation","text":"<p>The engine stops evaluating as soon as a policy returns BLOCK or PAUSE:</p> <pre><code>@Policy(name=\"auth\", priority=100)\ndef auth_policy(context):\n    if not context.metadata.get(\"authenticated\"):\n        return BLOCK(\"Not authenticated\")\n    return ALLOW\n\n@Policy(name=\"expensive_check\", priority=50)\ndef expensive_check(context):\n    pass\n</code></pre> <p>If <code>auth_policy</code> blocks, <code>expensive_check</code> is never evaluated.</p>"},{"location":"guide/governance/#decision-types","title":"Decision Types","text":""},{"location":"guide/governance/#allow","title":"ALLOW","text":"<p>Continue execution normally.</p> <pre><code>from clearstone import ALLOW\n\nreturn ALLOW\n</code></pre>"},{"location":"guide/governance/#block","title":"BLOCK","text":"<p>Stop execution immediately and raise a <code>PolicyViolationError</code>.</p> <pre><code>from clearstone import BLOCK\n\nreturn BLOCK(\"User does not have permission\")\n</code></pre> <p>In LangChain: <pre><code>from clearstone.integrations.langchain import PolicyCallbackHandler\n\nhandler = PolicyCallbackHandler(engine)\n\ntry:\n    with context_scope(context):\n        handler.on_tool_start(serialized={\"name\": \"admin_tool\"}, input_str=\"\")\nexcept PolicyViolationError as e:\n    print(f\"Blocked: {e.decision.reason}\")\n</code></pre></p>"},{"location":"guide/governance/#alert","title":"ALERT","text":"<p>Continue execution but log a warning for monitoring.</p> <pre><code>from clearstone import ALERT\n\nreturn ALERT\n</code></pre> <p>Best Practice: Use <code>ALERT</code> for suspicious but not dangerous behavior: <pre><code>@Policy(name=\"suspicious_activity_monitor\", priority=70)\ndef monitor_suspicious(context):\n    failed_attempts = context.metadata.get(\"failed_auth_attempts\", 0)\n\n    if failed_attempts &gt; 3:\n        return Decision(\n            ActionType.ALERT,\n            reason=f\"User has {failed_attempts} failed login attempts\"\n        )\n\n    return ALLOW\n</code></pre></p>"},{"location":"guide/governance/#pause","title":"PAUSE","text":"<p>Stop execution and wait for human approval.</p> <pre><code>from clearstone import PAUSE\n\nreturn PAUSE(\"High-risk transaction requires manual approval\")\n</code></pre> <p>Human-in-the-Loop Example: <pre><code>import dataclasses\nfrom clearstone import InterventionClient\nfrom clearstone.integrations.langchain import PolicyPauseError\n\n@Policy(name=\"require_approval\", priority=100)\ndef approval_policy(context):\n    amount = context.metadata.get(\"transaction_amount\", 0)\n    is_approved = context.metadata.get(\"is_approved\", False)\n\n    if amount &gt; 1000 and not is_approved:\n        return PAUSE(f\"Transaction of ${amount} requires approval\")\n\n    return ALLOW\n\ndef run_with_approval(engine, context):\n    handler = PolicyCallbackHandler(engine)\n\n    try:\n        with context_scope(context):\n            handler.on_tool_start(serialized={\"name\": \"execute_payment\"}, input_str=\"\")\n        return True\n\n    except PolicyPauseError as e:\n        print(f\"\u23f8\ufe0f Paused: {e.decision.reason}\")\n\n        client = InterventionClient()\n        client.request_intervention(e.decision)\n        intervention_id = e.decision.metadata.get(\"intervention_id\")\n\n        if client.wait_for_approval(intervention_id):\n            approved_context = dataclasses.replace(\n                context,\n                metadata={**context.metadata, \"is_approved\": True}\n            )\n            return run_with_approval(engine, approved_context)\n        else:\n            print(\"\u274c Transaction rejected\")\n            return False\n</code></pre></p>"},{"location":"guide/governance/#redact","title":"REDACT","text":"<p>Continue execution but remove sensitive fields from outputs.</p> <pre><code>from clearstone import REDACT\n\nreturn REDACT(\n    reason=\"PII protection\",\n    fields=[\"ssn\", \"credit_card\", \"email\"]\n)\n</code></pre> <p>Usage Example: <pre><code>@Policy(name=\"redact_pii\", priority=85)\ndef redact_pii_policy(context):\n    tool_name = context.metadata.get(\"tool_name\")\n\n    pii_tools = {\n        \"fetch_user\": [\"ssn\", \"credit_card\"],\n        \"get_medical\": [\"diagnosis\", \"prescription\"]\n    }\n\n    if tool_name in pii_tools:\n        return REDACT(\n            reason=f\"PII redaction for {tool_name}\",\n            fields=pii_tools[tool_name]\n        )\n\n    return ALLOW\n</code></pre></p>"},{"location":"guide/governance/#composing-policies","title":"Composing Policies","text":"<p>Build complex policies from simple, reusable parts.</p>"},{"location":"guide/governance/#compose_and","title":"compose_and","text":"<p>Create a policy that only passes if all sub-policies pass.</p> <pre><code>from clearstone import compose_and\nfrom clearstone.policies.common import token_limit_policy, cost_limit_policy\n\ncombined_policy = compose_and(token_limit_policy, cost_limit_policy)\n</code></pre>"},{"location":"guide/governance/#compose_or","title":"compose_or","text":"<p>Create a policy that passes if any sub-policy passes.</p> <pre><code>from clearstone import compose_or\n\nadmin_or_superuser = compose_or(admin_check_policy, superuser_check_policy)\n</code></pre>"},{"location":"guide/governance/#custom-composition","title":"Custom Composition","text":"<p>For complex logic, write a new policy that delegates to others:</p> <pre><code>@Policy(name=\"multi_stage_check\", priority=100)\ndef multi_stage_policy(context):\n    auth_result = auth_policy(context)\n    if auth_result.action != \"ALLOW\":\n        return auth_result\n\n    cost_result = cost_policy(context)\n    if cost_result.action != \"ALLOW\":\n        return cost_result\n\n    return ALLOW\n</code></pre>"},{"location":"guide/governance/#langchain-integration","title":"LangChain Integration","text":""},{"location":"guide/governance/#policycallbackhandler","title":"PolicyCallbackHandler","text":"<p>The <code>PolicyCallbackHandler</code> is a LangChain callback that enforces policies automatically:</p> <pre><code>from clearstone import PolicyEngine, create_context, context_scope\nfrom clearstone.integrations.langchain import PolicyCallbackHandler\nfrom langchain.agents import AgentExecutor\n\nengine = PolicyEngine()\nhandler = PolicyCallbackHandler(engine)\n\ncontext = create_context(\n    user_id=\"user_123\",\n    agent_id=\"my_agent\",\n    metadata={\"role\": \"user\", \"session_cost\": 0.0}\n)\n\nwith context_scope(context):\n    result = agent.invoke(\n        {\"input\": \"Search for AI safety papers\"},\n        callbacks=[handler]\n    )\n</code></pre>"},{"location":"guide/governance/#intercepted-events","title":"Intercepted Events","text":"<p>The handler intercepts these LangChain events: - <code>on_tool_start</code>: Before a tool is called - <code>on_llm_start</code>: Before an LLM call - <code>on_chain_start</code>: Before a chain executes</p>"},{"location":"guide/governance/#error-handling","title":"Error Handling","text":"<pre><code>from clearstone.integrations.langchain import PolicyViolationError, PolicyPauseError\n\ntry:\n    with context_scope(context):\n        result = agent.invoke(input, callbacks=[handler])\n\nexcept PolicyViolationError as e:\n    print(f\"\u274c Blocked: {e.decision.reason}\")\n\nexcept PolicyPauseError as e:\n    print(f\"\u23f8\ufe0f Paused: {e.decision.reason}\")\n</code></pre>"},{"location":"guide/governance/#developer-tools","title":"Developer Tools","text":""},{"location":"guide/governance/#policyvalidator","title":"PolicyValidator","text":"<p>Validate policies before deployment to catch bugs, slowness, and non-determinism.</p> <pre><code>from clearstone import PolicyValidator\n\nvalidator = PolicyValidator()\n\nfailures = validator.run_all_checks(my_policy)\n\nif failures:\n    print(\"\u274c Policy failed validation:\")\n    for failure in failures:\n        print(f\"  - {failure}\")\nelse:\n    print(\"\u2705 Policy is production-ready\")\n</code></pre> <p>Validation Checks: - Performance: Ensures policy executes in &lt; 100ms - Determinism: Ensures policy returns same result for same input - Error Handling: Ensures policy doesn't crash on edge cases</p>"},{"location":"guide/governance/#policydebugger","title":"PolicyDebugger","text":"<p>Understand exactly why a policy made a specific decision.</p> <pre><code>from clearstone import PolicyDebugger\n\ndebugger = PolicyDebugger()\n\ndecision, trace = debugger.trace_evaluation(my_complex_policy, context)\n\nprint(debugger.format_trace(my_complex_policy, decision, trace))\n</code></pre> <p>Output: <pre><code>Policy: my_complex_policy\nDecision: BLOCK (Line 15)\nReason: Cost limit exceeded\n\nExecution Trace:\n  Line 10: cost = context.metadata.get(\"cost\")  # cost = 25.5\n  Line 12: limit = context.metadata.get(\"limit\")  # limit = 10.0\n  Line 15: if cost &gt; limit:  # True\n  Line 16:     return BLOCK(\"Cost limit exceeded\")  # RETURNED\n</code></pre></p>"},{"location":"guide/governance/#policymetrics","title":"PolicyMetrics","text":"<p>Track policy performance and identify bottlenecks.</p> <pre><code>from clearstone import PolicyMetrics\n\nmetrics = PolicyMetrics()\nengine = PolicyEngine(metrics=metrics)\n\nsummary = metrics.summary()\n\nfor policy_name, stats in summary.items():\n    print(f\"{policy_name}:\")\n    print(f\"  Avg Latency: {stats['avg_latency_ms']:.4f}ms\")\n    print(f\"  Calls: {stats['call_count']}\")\n    print(f\"  Block Rate: {stats['block_rate']:.2%}\")\n\nslowest = metrics.get_slowest_policies(top_n=5)\ntop_blockers = metrics.get_top_blocking_policies(top_n=5)\n</code></pre>"},{"location":"guide/governance/#audittrail","title":"AuditTrail","text":"<p>Generate exportable audit logs for compliance.</p> <pre><code>from clearstone import AuditTrail\n\naudit = AuditTrail()\nengine = PolicyEngine(audit_trail=audit)\n\nsummary = audit.summary()\nprint(f\"Total Decisions: {summary['total_decisions']}\")\nprint(f\"Blocks: {summary['blocks']}\")\nprint(f\"Block Rate: {summary['block_rate']:.2%}\")\n\naudit.to_json(\"audit_log.json\")\naudit.to_csv(\"audit_log.csv\")\n</code></pre>"},{"location":"guide/governance/#cli-tools","title":"CLI Tools","text":""},{"location":"guide/governance/#scaffolding-new-policies","title":"Scaffolding New Policies","text":"<pre><code>clearstone new-policy enforce_data_locality --priority=80 --dir=my_app/policies\n</code></pre> <p>Generated File: <pre><code>from clearstone import Policy, ALLOW, BLOCK, Decision\nfrom clearstone.core.context import PolicyContext\n\n@Policy(name=\"enforce_data_locality\", priority=80)\ndef enforce_data_locality_policy(context: PolicyContext) -&gt; Decision:\n    \"\"\"\n    [TODO: Describe what this policy does.]\n    \"\"\"\n    return ALLOW\n</code></pre></p>"},{"location":"guide/governance/#best-practices","title":"Best Practices","text":""},{"location":"guide/governance/#1-keep-policies-simple","title":"1. Keep Policies Simple","text":"<p>Each policy should check one thing. Use composition for complex logic.</p> <p>Bad: <pre><code>@Policy(name=\"mega_policy\", priority=100)\ndef mega_policy(context):\n    if not check_auth(context):\n        return BLOCK(\"Auth failed\")\n    if not check_cost(context):\n        return BLOCK(\"Cost exceeded\")\n    if not check_rate_limit(context):\n        return BLOCK(\"Rate limit exceeded\")\n    return ALLOW\n</code></pre></p> <p>Good: <pre><code>@Policy(name=\"auth_check\", priority=100)\ndef auth_policy(context):\n    if not check_auth(context):\n        return BLOCK(\"Auth failed\")\n    return ALLOW\n\n@Policy(name=\"cost_check\", priority=90)\ndef cost_policy(context):\n    if not check_cost(context):\n        return BLOCK(\"Cost exceeded\")\n    return ALLOW\n\ncombined = compose_and(auth_policy, cost_policy)\n</code></pre></p>"},{"location":"guide/governance/#2-use-priorities-strategically","title":"2. Use Priorities Strategically","text":"<p>Security policies should have highest priority, monitoring policies lowest: - Security: 100-150 - Access Control: 80-100 - Cost/Rate Limits: 60-80 - Monitoring/Alerts: 10-50</p>"},{"location":"guide/governance/#3-provide-clear-reasons","title":"3. Provide Clear Reasons","text":"<p>Always include a descriptive reason when blocking:</p> <pre><code>return BLOCK(f\"User '{user_id}' does not have role '{required_role}'\")\n</code></pre>"},{"location":"guide/governance/#4-test-before-deploying","title":"4. Test Before Deploying","text":"<p>Always run <code>PolicyValidator</code> before deploying a new policy:</p> <pre><code>python -m clearstone.utils.validator my_policy\n</code></pre>"},{"location":"guide/governance/#5-monitor-in-production","title":"5. Monitor in Production","text":"<p>Use <code>PolicyMetrics</code> and <code>AuditTrail</code> to monitor policy behavior in production.</p>"},{"location":"guide/governance/#next-steps","title":"Next Steps","text":"<ul> <li>Pre-Built Policies: Explore 17+ production-ready policies</li> <li>Observability Guide: Add distributed tracing</li> <li>Testing Guide: Validate policies with backtesting</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"guide/observability/","title":"Observability","text":"<p>The Observability pillar provides production-grade distributed tracing for AI agents. This guide covers everything you need to instrument, trace, and analyze agent execution.</p>"},{"location":"guide/observability/#quick-start","title":"Quick Start","text":"<p>Initialize tracing in three lines:</p> <pre><code>from clearstone.observability import TracerProvider\n\nprovider = TracerProvider(db_path=\"traces.db\")\ntracer = provider.get_tracer(\"my_agent\", version=\"1.0\")\n\nwith tracer.span(\"agent_workflow\"):\n    pass\n\nprovider.shutdown()\n</code></pre>"},{"location":"guide/observability/#tracerprovider","title":"TracerProvider","text":"<p>The <code>TracerProvider</code> is the entry point for the tracing system.</p>"},{"location":"guide/observability/#initialization","title":"Initialization","text":"<pre><code>from clearstone.observability import TracerProvider\n\nprovider = TracerProvider(\n    db_path=\"agent_traces.db\",\n    service_name=\"production_agent\",\n    batch_size=100,\n    flush_interval_seconds=5.0\n)\n</code></pre> <p>Parameters: - <code>db_path</code>: Path to SQLite database for trace storage - <code>service_name</code>: Name of your service (default: \"clearstone\") - <code>batch_size</code>: Number of spans to batch before writing (default: 100) - <code>flush_interval_seconds</code>: How often to flush spans (default: 5.0)</p>"},{"location":"guide/observability/#shutdown","title":"Shutdown","text":"<p>Always call <code>shutdown()</code> to flush remaining spans:</p> <pre><code>provider.shutdown()\n</code></pre>"},{"location":"guide/observability/#creating-tracers","title":"Creating Tracers","text":"<p>Get a tracer instance for your agent:</p> <pre><code>tracer = provider.get_tracer(\n    name=\"research_agent\",\n    version=\"2.1.0\"\n)\n</code></pre> <p>Tracers are lightweight and thread-safe. Create one per logical component.</p>"},{"location":"guide/observability/#creating-spans","title":"Creating Spans","text":""},{"location":"guide/observability/#basic-span","title":"Basic Span","text":"<pre><code>with tracer.span(\"operation_name\") as span:\n    result = do_work()\n</code></pre>"},{"location":"guide/observability/#span-with-attributes","title":"Span with Attributes","text":"<pre><code>with tracer.span(\"llm_call\", attributes={\n    \"model\": \"gpt-4\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 1000,\n    \"cost\": 0.045\n}) as span:\n    result = call_llm()\n</code></pre>"},{"location":"guide/observability/#span-kinds","title":"Span Kinds","text":"<p>Specify the type of operation:</p> <pre><code>from clearstone.observability import SpanKind\n\nwith tracer.span(\"agent_workflow\", kind=SpanKind.INTERNAL):\n    pass\n\nwith tracer.span(\"api_call\", kind=SpanKind.CLIENT):\n    pass\n\nwith tracer.span(\"database_query\", kind=SpanKind.CLIENT):\n    pass\n</code></pre> <p>SpanKind values: - <code>INTERNAL</code>: Default - internal operation - <code>CLIENT</code>: Outbound call (API, database, LLM) - <code>SERVER</code>: Inbound request - <code>PRODUCER</code>: Message queue producer - <code>CONSUMER</code>: Message queue consumer</p>"},{"location":"guide/observability/#automatic-hierarchy","title":"Automatic Hierarchy","text":"<p>Nested spans automatically link to their parents:</p> <pre><code>with tracer.span(\"parent_operation\") as parent:\n    print(f\"Parent Trace ID: {parent.trace_id}\")\n\n    with tracer.span(\"child_operation\") as child:\n        print(f\"Child Trace ID: {child.trace_id}\")\n        print(f\"Child Parent ID: {child.parent_span_id}\")\n\n        with tracer.span(\"grandchild_operation\") as grandchild:\n            print(f\"Grandchild Parent ID: {grandchild.parent_span_id}\")\n</code></pre> <p>All spans share the same <code>trace_id</code>. Each span's <code>parent_span_id</code> points to its parent.</p>"},{"location":"guide/observability/#exception-tracking","title":"Exception Tracking","text":"<p>Exceptions are automatically captured:</p> <pre><code>with tracer.span(\"risky_operation\") as span:\n    raise ValueError(\"Something went wrong\")\n</code></pre> <p>The span automatically records: - <code>status</code>: ERROR - <code>error_message</code>: \"Something went wrong\" - <code>error_stacktrace</code>: Full traceback</p>"},{"location":"guide/observability/#manual-status-setting","title":"Manual Status Setting","text":"<pre><code>with tracer.span(\"operation\") as span:\n    if error_occurred:\n        span.set_status(\"ERROR\")\n    else:\n        span.set_status(\"OK\")\n</code></pre>"},{"location":"guide/observability/#tracestore","title":"TraceStore","text":"<p>Query and analyze stored traces.</p>"},{"location":"guide/observability/#getting-the-tracestore","title":"Getting the TraceStore","text":"<pre><code>trace_store = provider.trace_store\n</code></pre>"},{"location":"guide/observability/#listing-traces","title":"Listing Traces","text":"<pre><code>traces = trace_store.list_traces(limit=100)\n\nfor trace in traces:\n    print(f\"Trace ID: {trace.trace_id}\")\n    print(f\"Root Span: {trace.root_span.name}\")\n    print(f\"Duration: {trace.root_span.duration_ms:.2f}ms\")\n    print(f\"Status: {trace.root_span.status}\")\n</code></pre>"},{"location":"guide/observability/#getting-a-specific-trace","title":"Getting a Specific Trace","text":"<pre><code>trace = trace_store.get_trace(trace_id=\"abc123\")\n\nprint(f\"Root: {trace.root_span.name}\")\nfor span in trace.spans:\n    print(f\"  - {span.name} ({span.duration_ms:.2f}ms)\")\n</code></pre>"},{"location":"guide/observability/#querying-spans","title":"Querying Spans","text":"<pre><code>spans = trace_store.query_spans(\n    trace_id=\"abc123\",\n    name=\"llm_call\"\n)\n\nfor span in spans:\n    print(f\"Model: {span.attributes.get('model')}\")\n    print(f\"Cost: ${span.attributes.get('cost'):.4f}\")\n</code></pre>"},{"location":"guide/observability/#filtering-traces","title":"Filtering Traces","text":"<pre><code>recent_errors = trace_store.list_traces(\n    limit=50,\n    status=\"ERROR\"\n)\n\nexpensive_traces = [\n    t for t in trace_store.list_traces()\n    if t.root_span.attributes.get(\"cost\", 0) &gt; 1.0\n]\n</code></pre>"},{"location":"guide/observability/#span-attributes","title":"Span Attributes","text":""},{"location":"guide/observability/#standard-attributes","title":"Standard Attributes","text":"<pre><code>with tracer.span(\"operation\", attributes={\n    \"operation.type\": \"search\",\n    \"operation.retries\": 3,\n    \"resource.name\": \"web_search_api\",\n    \"cost.usd\": 0.05\n}) as span:\n    pass\n</code></pre>"},{"location":"guide/observability/#llm-specific-attributes","title":"LLM-Specific Attributes","text":"<pre><code>with tracer.span(\"llm_call\", attributes={\n    \"llm.model\": \"gpt-4-turbo\",\n    \"llm.temperature\": 0.7,\n    \"llm.max_tokens\": 2000,\n    \"llm.prompt_tokens\": 150,\n    \"llm.completion_tokens\": 450,\n    \"llm.total_tokens\": 600,\n    \"llm.cost_usd\": 0.045\n}) as span:\n    pass\n</code></pre>"},{"location":"guide/observability/#tool-specific-attributes","title":"Tool-Specific Attributes","text":"<pre><code>with tracer.span(\"tool_execution\", attributes={\n    \"tool.name\": \"calculator\",\n    \"tool.input\": \"2 + 2\",\n    \"tool.output\": \"4\",\n    \"tool.duration_ms\": 12.5\n}) as span:\n    pass\n</code></pre>"},{"location":"guide/observability/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"guide/observability/#non-blocking-capture","title":"Non-Blocking Capture","text":"<p>Span creation takes &lt; 1\u03bcs and doesn't block execution:</p> <pre><code>import time\n\nstart = time.perf_counter()\n\nwith tracer.span(\"fast_operation\"):\n    pass\n\nelapsed = time.perf_counter() - start\nprint(f\"Overhead: {elapsed * 1000:.3f}ms\")\n</code></pre>"},{"location":"guide/observability/#batched-writes","title":"Batched Writes","text":"<p>Spans are written in batches of 100 (configurable) to minimize I/O:</p> <pre><code>provider = TracerProvider(\n    db_path=\"traces.db\",\n    batch_size=200,\n    flush_interval_seconds=10.0\n)\n</code></pre>"},{"location":"guide/observability/#thread-safe","title":"Thread-Safe","text":"<p>Multiple threads can trace concurrently:</p> <pre><code>import threading\n\ndef worker(tracer, worker_id):\n    with tracer.span(f\"worker_{worker_id}\"):\n        do_work()\n\nthreads = [\n    threading.Thread(target=worker, args=(tracer, i))\n    for i in range(10)\n]\n\nfor t in threads:\n    t.start()\n\nfor t in threads:\n    t.join()\n</code></pre>"},{"location":"guide/observability/#integration-with-policies","title":"Integration with Policies","text":"<p>Combine tracing with policy enforcement:</p> <pre><code>from clearstone import PolicyEngine, create_context, context_scope\nfrom clearstone.integrations.langchain import PolicyCallbackHandler\n\nprovider = TracerProvider(db_path=\"traces.db\")\ntracer = provider.get_tracer(\"governed_agent\")\nengine = PolicyEngine()\nhandler = PolicyCallbackHandler(engine)\n\nwith tracer.span(\"agent_run\") as root_span:\n    context = create_context(\n        user_id=\"user_123\",\n        agent_id=\"agent_1\",\n        metadata={\"tool_name\": \"web_search\"}\n    )\n\n    with context_scope(context):\n        with tracer.span(\"policy_check\"):\n            try:\n                handler.on_tool_start(\n                    serialized={\"name\": \"web_search\"},\n                    input_str=\"search query\"\n                )\n            except PolicyViolationError as e:\n                root_span.set_status(\"ERROR\")\n                raise\n\n        with tracer.span(\"tool_execution\", attributes={\"tool\": \"web_search\"}):\n            result = execute_tool()\n\nprovider.shutdown()\n</code></pre>"},{"location":"guide/observability/#analyzing-traces","title":"Analyzing Traces","text":""},{"location":"guide/observability/#cost-analysis","title":"Cost Analysis","text":"<pre><code>trace_store = provider.trace_store\n\ntotal_cost = 0.0\nfor trace in trace_store.list_traces():\n    for span in trace.spans:\n        total_cost += span.attributes.get(\"cost\", 0.0)\n\nprint(f\"Total Cost: ${total_cost:.2f}\")\n</code></pre>"},{"location":"guide/observability/#performance-analysis","title":"Performance Analysis","text":"<pre><code>slow_operations = []\n\nfor trace in trace_store.list_traces():\n    for span in trace.spans:\n        if span.duration_ms &gt; 1000:\n            slow_operations.append({\n                \"name\": span.name,\n                \"duration_ms\": span.duration_ms,\n                \"trace_id\": span.trace_id\n            })\n\nslow_operations.sort(key=lambda x: x[\"duration_ms\"], reverse=True)\n\nfor op in slow_operations[:10]:\n    print(f\"{op['name']}: {op['duration_ms']:.2f}ms\")\n</code></pre>"},{"location":"guide/observability/#error-rate-analysis","title":"Error Rate Analysis","text":"<pre><code>total_traces = 0\nerror_traces = 0\n\nfor trace in trace_store.list_traces():\n    total_traces += 1\n    if trace.root_span.status == \"ERROR\":\n        error_traces += 1\n\nerror_rate = error_traces / total_traces if total_traces &gt; 0 else 0\nprint(f\"Error Rate: {error_rate:.2%}\")\n</code></pre>"},{"location":"guide/observability/#export-and-visualization","title":"Export and Visualization","text":""},{"location":"guide/observability/#export-to-json","title":"Export to JSON","text":"<pre><code>import json\n\ntraces = trace_store.list_traces(limit=100)\n\ntrace_data = [\n    {\n        \"trace_id\": trace.trace_id,\n        \"root_span\": trace.root_span.name,\n        \"duration_ms\": trace.root_span.duration_ms,\n        \"status\": trace.root_span.status,\n        \"spans\": [\n            {\n                \"name\": span.name,\n                \"duration_ms\": span.duration_ms,\n                \"attributes\": span.attributes\n            }\n            for span in trace.spans\n        ]\n    }\n    for trace in traces\n]\n\nwith open(\"traces.json\", \"w\") as f:\n    json.dump(trace_data, f, indent=2)\n</code></pre>"},{"location":"guide/observability/#export-to-csv","title":"Export to CSV","text":"<pre><code>import csv\n\nwith open(\"spans.csv\", \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\n        \"trace_id\", \"span_id\", \"name\", \"duration_ms\",\n        \"status\", \"parent_span_id\"\n    ])\n\n    for trace in trace_store.list_traces():\n        for span in trace.spans:\n            writer.writerow([\n                span.trace_id,\n                span.span_id,\n                span.name,\n                span.duration_ms,\n                span.status,\n                span.parent_span_id or \"\"\n            ])\n</code></pre>"},{"location":"guide/observability/#best-practices","title":"Best Practices","text":""},{"location":"guide/observability/#1-always-shutdown","title":"1. Always Shutdown","text":"<pre><code>try:\n    with tracer.span(\"operation\"):\n        do_work()\nfinally:\n    provider.shutdown()\n</code></pre>"},{"location":"guide/observability/#2-use-meaningful-names","title":"2. Use Meaningful Names","text":"<pre><code>with tracer.span(\"user_registration_workflow\"):\n    pass\n</code></pre>"},{"location":"guide/observability/#3-include-key-attributes","title":"3. Include Key Attributes","text":"<pre><code>with tracer.span(\"api_call\", attributes={\n    \"http.method\": \"POST\",\n    \"http.url\": \"https://api.example.com/users\",\n    \"http.status_code\": 201\n}):\n    pass\n</code></pre>"},{"location":"guide/observability/#4-set-span-status","title":"4. Set Span Status","text":"<pre><code>with tracer.span(\"operation\") as span:\n    try:\n        result = do_work()\n        span.set_status(\"OK\")\n    except Exception as e:\n        span.set_status(\"ERROR\")\n        raise\n</code></pre>"},{"location":"guide/observability/#5-trace-at-the-right-granularity","title":"5. Trace at the Right Granularity","text":"<p>Trace operations that are: - Meaningful (e.g., \"llm_call\", not \"add_two_numbers\") - Measurable (have meaningful duration) - Observable (need monitoring)</p>"},{"location":"guide/observability/#next-steps","title":"Next Steps","text":"<ul> <li>Testing Guide: Use traces for behavioral testing</li> <li>Time-Travel Debugging: Debug with checkpoints</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"guide/testing/","title":"Testing","text":"<p>The Testing pillar provides AI-native testing capabilities for validating agent behavior. Unlike traditional unit tests that check outputs, Clearstone tests validate how agents behave at runtime.</p>"},{"location":"guide/testing/#the-policytestharness","title":"The PolicyTestHarness","text":"<p>The <code>PolicyTestHarness</code> is the core testing tool. It loads historical traces and simulates policy enforcement to predict impact before deployment.</p>"},{"location":"guide/testing/#basic-setup","title":"Basic Setup","text":"<pre><code>from clearstone.testing import PolicyTestHarness\n\nharness = PolicyTestHarness(\"agent_traces.db\")\n\ntraces = harness.load_traces(limit=100)\n</code></pre>"},{"location":"guide/testing/#behavioral-assertions","title":"Behavioral Assertions","text":"<p>Behavioral assertions are policies designed for testing. They validate specific agent behaviors.</p>"},{"location":"guide/testing/#assert_tool_was_called","title":"assert_tool_was_called","text":"<p>Verify a tool was called the expected number of times.</p> <pre><code>from clearstone.testing import assert_tool_was_called\n\npolicy = assert_tool_was_called(\"web_search\")\n\nresult = harness.simulate_policy(policy, traces)\n\nsummary = result.summary()\nprint(f\"Passed: {summary['traces_analyzed'] - summary['runs_blocked']}\")\nprint(f\"Failed: {summary['runs_blocked']}\")\n</code></pre> <p>With Exact Count: <pre><code>policy = assert_tool_was_called(\"web_search\", times=3)\n</code></pre></p>"},{"location":"guide/testing/#assert_no_errors_in_trace","title":"assert_no_errors_in_trace","text":"<p>Validate that traces executed without errors.</p> <pre><code>from clearstone.testing import assert_no_errors_in_trace\n\npolicy = assert_no_errors_in_trace()\n\nresult = harness.simulate_policy(policy, traces)\n\nif result.summary()[\"runs_blocked\"] &gt; 0:\n    print(\"\u274c Found traces with errors:\")\n    print(f\"   Blocked traces: {result.blocked_trace_ids}\")\n</code></pre>"},{"location":"guide/testing/#assert_llm_cost_is_less_than","title":"assert_llm_cost_is_less_than","text":"<p>Ensure agent stays within budget.</p> <pre><code>from clearstone.testing import assert_llm_cost_is_less_than\n\npolicy = assert_llm_cost_is_less_than(0.50)\n\nresult = harness.simulate_policy(policy, traces)\n\nsummary = result.summary()\nprint(f\"Over Budget: {summary['runs_blocked']} / {summary['traces_analyzed']}\")\n</code></pre>"},{"location":"guide/testing/#assert_span_order","title":"assert_span_order","text":"<p>Validate workflow sequence is correct.</p> <pre><code>from clearstone.testing import assert_span_order\n\npolicy = assert_span_order([\"plan\", \"search\", \"synthesize\"])\n\nresult = harness.simulate_policy(policy, traces)\n\nif result.summary()[\"runs_blocked\"] &gt; 0:\n    print(\"\u274c Workflow order violated\")\n</code></pre>"},{"location":"guide/testing/#historical-backtesting","title":"Historical Backtesting","text":"<p>Test new policies against production data before deployment.</p>"},{"location":"guide/testing/#basic-backtesting","title":"Basic Backtesting","text":"<pre><code>from clearstone import Policy, BLOCK, ALLOW\n\n@Policy(name=\"new_cost_policy\", priority=100)\ndef new_cost_policy(context):\n    cost = sum(\n        span.attributes.get(\"cost\", 0)\n        for span in context.metadata.get(\"spans\", [])\n    )\n\n    if cost &gt; 2.0:\n        return BLOCK(f\"Cost ${cost:.2f} exceeds new limit\")\n\n    return ALLOW\n\nharness = PolicyTestHarness(\"production_traces.db\")\ntraces = harness.load_traces(limit=1000)\n\nresult = harness.simulate_policy(new_cost_policy, traces)\n\nsummary = result.summary()\nprint(f\"Impact Analysis:\")\nprint(f\"  Traces Analyzed: {summary['traces_analyzed']}\")\nprint(f\"  Would Block: {summary['runs_blocked']}\")\nprint(f\"  Block Rate: {summary['block_rate_percent']}\")\nprint(f\"  Blocked Trace IDs: {result.blocked_trace_ids[:10]}\")\n</code></pre>"},{"location":"guide/testing/#comparing-policies","title":"Comparing Policies","text":"<pre><code>from clearstone.policies.common import token_limit_policy, cost_limit_policy\n\nharness = PolicyTestHarness(\"traces.db\")\ntraces = harness.load_traces()\n\ntoken_result = harness.simulate_policy(token_limit_policy, traces)\ncost_result = harness.simulate_policy(cost_limit_policy, traces)\n\nprint(\"Token Policy:\")\nprint(f\"  Block Rate: {token_result.summary()['block_rate_percent']}\")\n\nprint(\"Cost Policy:\")\nprint(f\"  Block Rate: {cost_result.summary()['block_rate_percent']}\")\n</code></pre>"},{"location":"guide/testing/#analyzing-impact","title":"Analyzing Impact","text":"<pre><code>result = harness.simulate_policy(new_policy, traces)\n\nprint(f\"Total Traces: {len(traces)}\")\nprint(f\"Blocked: {len(result.blocked_trace_ids)}\")\nprint(f\"Allowed: {len(result.allowed_trace_ids)}\")\n\nprint(\"\\nBlocked Trace Details:\")\nfor trace_id in result.blocked_trace_ids[:5]:\n    trace = next(t for t in traces if t.trace_id == trace_id)\n    print(f\"  {trace_id}: {trace.root_span.name}\")\n    print(f\"    Duration: {trace.root_span.duration_ms:.2f}ms\")\n    print(f\"    Status: {trace.root_span.status}\")\n</code></pre>"},{"location":"guide/testing/#pytest-integration","title":"pytest Integration","text":"<p>Integrate behavioral tests into your test suite.</p>"},{"location":"guide/testing/#basic-test","title":"Basic Test","text":"<pre><code>import pytest\nfrom clearstone.observability import TracerProvider\nfrom clearstone.testing import PolicyTestHarness, assert_tool_was_called\n\ndef test_research_agent_uses_search(tmp_path):\n    db_path = tmp_path / \"test_traces.db\"\n\n    provider = TracerProvider(db_path=str(db_path))\n    tracer = provider.get_tracer(\"test_agent\")\n\n    with tracer.span(\"research_workflow\"):\n        with tracer.span(\"search\", attributes={\"tool.name\": \"web_search\"}):\n            pass\n\n    provider.shutdown()\n\n    harness = PolicyTestHarness(str(db_path))\n    traces = harness.load_traces()\n\n    policy = assert_tool_was_called(\"web_search\", times=1)\n    result = harness.simulate_policy(policy, traces)\n\n    assert result.summary()[\"runs_blocked\"] == 0, \\\n        \"Agent should use web_search exactly once\"\n</code></pre>"},{"location":"guide/testing/#testing-agent-behavior","title":"Testing Agent Behavior","text":"<pre><code>def test_agent_stays_within_budget(tmp_path):\n    db_path = tmp_path / \"test_traces.db\"\n\n    provider = TracerProvider(db_path=str(db_path))\n    tracer = provider.get_tracer(\"test_agent\")\n\n    with tracer.span(\"task\", attributes={\"cost\": 0.45}):\n        pass\n\n    provider.shutdown()\n\n    harness = PolicyTestHarness(str(db_path))\n    traces = harness.load_traces()\n\n    policy = assert_llm_cost_is_less_than(0.50)\n    result = harness.simulate_policy(policy, traces)\n\n    assert result.summary()[\"runs_blocked\"] == 0\n</code></pre>"},{"location":"guide/testing/#testing-error-handling","title":"Testing Error Handling","text":"<pre><code>def test_agent_handles_errors_gracefully(tmp_path):\n    db_path = tmp_path / \"test_traces.db\"\n\n    provider = TracerProvider(db_path=str(db_path))\n    tracer = provider.get_tracer(\"test_agent\")\n\n    with tracer.span(\"workflow\") as span:\n        with tracer.span(\"risky_operation\") as child:\n            child.set_status(\"ERROR\")\n\n    provider.shutdown()\n\n    harness = PolicyTestHarness(str(db_path))\n    traces = harness.load_traces()\n\n    policy = assert_no_errors_in_trace()\n    result = harness.simulate_policy(policy, traces)\n\n    assert result.summary()[\"runs_blocked\"] &gt; 0, \\\n        \"Should detect error in trace\"\n</code></pre>"},{"location":"guide/testing/#parametrized-tests","title":"Parametrized Tests","text":"<pre><code>@pytest.mark.parametrize(\"tool_name,expected_count\", [\n    (\"web_search\", 2),\n    (\"calculator\", 1),\n    (\"summarizer\", 1),\n])\ndef test_tool_usage_counts(tmp_path, tool_name, expected_count):\n    db_path = tmp_path / \"test_traces.db\"\n\n    provider = TracerProvider(db_path=str(db_path))\n    tracer = provider.get_tracer(\"test_agent\")\n\n    with tracer.span(\"workflow\"):\n        for _ in range(expected_count):\n            with tracer.span(\"tool\", attributes={\"tool.name\": tool_name}):\n                pass\n\n    provider.shutdown()\n\n    harness = PolicyTestHarness(str(db_path))\n    traces = harness.load_traces()\n\n    policy = assert_tool_was_called(tool_name, times=expected_count)\n    result = harness.simulate_policy(policy, traces)\n\n    assert result.summary()[\"runs_blocked\"] == 0\n</code></pre>"},{"location":"guide/testing/#custom-behavioral-assertions","title":"Custom Behavioral Assertions","text":"<p>Create your own assertions for specific needs.</p>"},{"location":"guide/testing/#isolated-policy-testing","title":"Isolated Policy Testing","text":"<p>Use explicit configuration to test policies in isolation without interference from other policies:</p> <pre><code>import pytest\nfrom clearstone import PolicyEngine, Policy, BLOCK, ALLOW, create_context\nfrom clearstone.core.actions import ActionType\n\n@Policy(name=\"value_check_policy\", priority=100)\ndef value_check_policy(context):\n    \"\"\"Ensure value doesn't exceed threshold.\"\"\"\n    value = context.metadata.get(\"value\", 0)\n    threshold = context.metadata.get(\"threshold\", 10)\n\n    if value &gt; threshold:\n        return BLOCK(f\"Value {value} exceeds threshold {threshold}\")\n\n    return ALLOW\n\ndef test_value_check_policy_blocks_high_values():\n    \"\"\"Test that the policy blocks values above threshold.\"\"\"\n    # Test this policy in isolation, ignoring all other registered policies\n    engine = PolicyEngine(policies=[value_check_policy])\n\n    context = create_context(\n        \"user\", \"agent\",\n        metadata={\"value\": 15, \"threshold\": 10}\n    )\n\n    decision = engine.evaluate(context)\n\n    assert decision.action == ActionType.BLOCK\n    assert \"exceeds threshold\" in decision.reason\n\ndef test_value_check_policy_allows_low_values():\n    \"\"\"Test that the policy allows values below threshold.\"\"\"\n    engine = PolicyEngine(policies=[value_check_policy])\n\n    context = create_context(\n        \"user\", \"agent\",\n        metadata={\"value\": 5, \"threshold\": 10}\n    )\n\n    decision = engine.evaluate(context)\n\n    assert decision.action == ActionType.ALLOW\n\ndef test_multiple_policies_together():\n    \"\"\"Test how multiple policies interact.\"\"\"\n    @Policy(name=\"auth_check\", priority=100)\n    def auth_check(context):\n        if not context.metadata.get(\"authenticated\"):\n            return BLOCK(\"Not authenticated\")\n        return ALLOW\n\n    # Test specific combination of policies\n    engine = PolicyEngine(policies=[auth_check, value_check_policy])\n\n    context = create_context(\n        \"user\", \"agent\",\n        metadata={\"authenticated\": False, \"value\": 5}\n    )\n\n    decision = engine.evaluate(context)\n\n    # Auth should block first (higher priority when both are 100)\n    assert decision.action == ActionType.BLOCK\n    assert \"Not authenticated\" in decision.reason\n</code></pre> <p>Benefits of Isolated Testing:</p> <ul> <li>No Side Effects: Other policies don't interfere with your test</li> <li>Deterministic: Test outcome depends only on the policy being tested</li> <li>Fast: Only evaluates the policies you need</li> <li>Clear Failures: Easy to identify which policy caused a test failure</li> <li>Flexible: Mix and match policies to test specific combinations</li> </ul>"},{"location":"guide/testing/#basic-custom-assertion","title":"Basic Custom Assertion","text":"<pre><code>from clearstone import Policy, BLOCK, ALLOW\n\n@Policy(name=\"assert_no_external_apis\", priority=100)\ndef assert_no_external_apis(context):\n    \"\"\"Ensure agent doesn't call external APIs.\"\"\"\n    spans = context.metadata.get(\"spans\", [])\n\n    for span in spans:\n        if span.attributes.get(\"external_api\", False):\n            return BLOCK(\n                f\"Unexpected external API call: {span.name}\"\n            )\n\n    return ALLOW\n</code></pre>"},{"location":"guide/testing/#advanced-custom-assertion","title":"Advanced Custom Assertion","text":"<pre><code>@Policy(name=\"assert_optimal_workflow\", priority=100)\ndef assert_optimal_workflow(context):\n    \"\"\"Ensure agent uses optimal workflow pattern.\"\"\"\n    spans = context.metadata.get(\"spans\", [])\n\n    span_names = [s.name for s in spans]\n\n    if \"plan\" not in span_names:\n        return BLOCK(\"Workflow missing 'plan' step\")\n\n    if \"validate\" not in span_names:\n        return BLOCK(\"Workflow missing 'validate' step\")\n\n    plan_idx = span_names.index(\"plan\")\n    execute_idx = next(\n        (i for i, name in enumerate(span_names) if \"execute\" in name),\n        None\n    )\n\n    if execute_idx and execute_idx &lt; plan_idx:\n        return BLOCK(\"Workflow executed before planning\")\n\n    return ALLOW\n</code></pre>"},{"location":"guide/testing/#regression-testing","title":"Regression Testing","text":"<p>Prevent behavioral regressions by testing against baseline traces.</p>"},{"location":"guide/testing/#creating-a-baseline","title":"Creating a Baseline","text":"<pre><code>provider = TracerProvider(db_path=\"baseline_traces.db\")\ntracer = provider.get_tracer(\"production_agent\")\n\nwith tracer.span(\"baseline_workflow\"):\n    run_agent()\n\nprovider.shutdown()\n</code></pre>"},{"location":"guide/testing/#testing-against-baseline","title":"Testing Against Baseline","text":"<pre><code>def test_no_regression():\n    baseline_harness = PolicyTestHarness(\"baseline_traces.db\")\n    current_harness = PolicyTestHarness(\"current_traces.db\")\n\n    baseline_traces = baseline_harness.load_traces()\n    current_traces = current_harness.load_traces()\n\n    policy = assert_tool_was_called(\"expensive_api\", times=0)\n\n    baseline_result = baseline_harness.simulate_policy(policy, baseline_traces)\n    current_result = current_harness.simulate_policy(policy, current_traces)\n\n    assert baseline_result.summary()[\"runs_blocked\"] == \\\n           current_result.summary()[\"runs_blocked\"], \\\n           \"Behavioral regression detected\"\n</code></pre>"},{"location":"guide/testing/#test-result-analysis","title":"Test Result Analysis","text":""},{"location":"guide/testing/#result-object","title":"Result Object","text":"<pre><code>result = harness.simulate_policy(policy, traces)\n\nprint(f\"Policy Name: {result.policy_name}\")\nprint(f\"Traces Analyzed: {len(result.allowed_trace_ids) + len(result.blocked_trace_ids)}\")\nprint(f\"Allowed: {len(result.allowed_trace_ids)}\")\nprint(f\"Blocked: {len(result.blocked_trace_ids)}\")\n</code></pre>"},{"location":"guide/testing/#summary-statistics","title":"Summary Statistics","text":"<pre><code>summary = result.summary()\n\nprint(f\"Traces Analyzed: {summary['traces_analyzed']}\")\nprint(f\"Runs Blocked: {summary['runs_blocked']}\")\nprint(f\"Block Rate: {summary['block_rate_percent']}\")\n</code></pre>"},{"location":"guide/testing/#detailed-analysis","title":"Detailed Analysis","text":"<pre><code>for trace_id in result.blocked_trace_ids[:10]:\n    trace = next(t for t in traces if t.trace_id == trace_id)\n\n    print(f\"Blocked Trace: {trace_id}\")\n    print(f\"  Root: {trace.root_span.name}\")\n    print(f\"  Duration: {trace.root_span.duration_ms:.2f}ms\")\n\n    for span in trace.spans:\n        print(f\"    - {span.name}\")\n        for key, value in span.attributes.items():\n            print(f\"        {key}: {value}\")\n</code></pre>"},{"location":"guide/testing/#best-practices","title":"Best Practices","text":""},{"location":"guide/testing/#1-test-early-and-often","title":"1. Test Early and Often","text":"<p>Run behavioral tests in CI/CD:</p> <pre><code># .github/workflows/test.yml\n- name: Run Behavioral Tests\n  run: pytest tests/behavioral/\n</code></pre>"},{"location":"guide/testing/#2-maintain-representative-test-data","title":"2. Maintain Representative Test Data","text":"<p>Keep a dataset of representative traces:</p> <pre><code>BASELINE_TRACES = \"tests/fixtures/baseline_traces.db\"\n\nharness = PolicyTestHarness(BASELINE_TRACES)\n</code></pre>"},{"location":"guide/testing/#3-test-multiple-scenarios","title":"3. Test Multiple Scenarios","text":"<pre><code>@pytest.mark.parametrize(\"scenario\", [\n    \"happy_path\",\n    \"error_recovery\",\n    \"high_load\",\n    \"edge_case\"\n])\ndef test_agent_behavior(scenario):\n    harness = PolicyTestHarness(f\"traces/{scenario}.db\")\n    pass\n</code></pre>"},{"location":"guide/testing/#4-document-test-intent","title":"4. Document Test Intent","text":"<pre><code>def test_agent_respects_cost_limits():\n    \"\"\"\n    Verify that the agent never exceeds a $1.00 cost per session.\n\n    This is a critical business requirement to prevent unexpected bills.\n    If this test fails, DO NOT deploy to production.\n    \"\"\"\n    policy = assert_llm_cost_is_less_than(1.00)\n    pass\n</code></pre>"},{"location":"guide/testing/#5-use-fixtures-for-common-setup","title":"5. Use Fixtures for Common Setup","text":"<pre><code>@pytest.fixture\ndef test_harness(tmp_path):\n    db_path = tmp_path / \"test_traces.db\"\n    return PolicyTestHarness(str(db_path))\n\ndef test_something(test_harness):\n    traces = test_harness.load_traces()\n    pass\n</code></pre>"},{"location":"guide/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Time-Travel Debugging: Debug agents with checkpoints</li> <li>Pre-Built Policies: Explore behavioral assertions</li> <li>API Reference: Complete testing API documentation</li> </ul>"},{"location":"guide/time-travel-debugging/","title":"Time-Travel Debugging","text":"<p>The Time-Travel Debugging pillar allows you to capture complete agent state snapshots and replay execution from any point in history. This enables debugging agents in ways that weren't possible before.</p>"},{"location":"guide/time-travel-debugging/#the-checkpoint-system","title":"The Checkpoint System","text":"<p>A checkpoint is a complete snapshot of your agent's state at a specific moment in execution. It includes: - Agent's internal state (memory, configuration, variables) - Full trace context (all parent spans up to that point) - Execution metadata (timestamp, version, span ID) - Serialized state (JSON metadata + pickle state)</p>"},{"location":"guide/time-travel-debugging/#checkpointmanager","title":"CheckpointManager","text":"<p>The <code>CheckpointManager</code> creates, saves, and loads checkpoints.</p>"},{"location":"guide/time-travel-debugging/#initialization","title":"Initialization","text":"<pre><code>from clearstone.debugging import CheckpointManager\n\nmanager = CheckpointManager(checkpoint_dir=\".checkpoints\")\n</code></pre>"},{"location":"guide/time-travel-debugging/#creating-a-checkpoint","title":"Creating a Checkpoint","text":"<pre><code>from clearstone.observability import TracerProvider\n\nprovider = TracerProvider(db_path=\"traces.db\")\ntracer = provider.get_tracer(\"my_agent\", version=\"1.0\")\n\nwith tracer.span(\"agent_workflow\") as root_span:\n    trace_id = root_span.trace_id\n\n    with tracer.span(\"step_1\") as span1:\n        agent.process_step_1()\n\n    with tracer.span(\"step_2\") as span2:\n        span_id = span2.span_id\n        agent.process_step_2()\n\nprovider.shutdown()\n\ntrace = provider.trace_store.get_trace(trace_id)\n\ncheckpoint = manager.create_checkpoint(\n    agent=agent,\n    trace=trace,\n    span_id=span_id\n)\n\ncheckpoint_path = manager.save_checkpoint(checkpoint)\nprint(f\"Checkpoint saved: {checkpoint_path}\")\n</code></pre>"},{"location":"guide/time-travel-debugging/#loading-a-checkpoint","title":"Loading a Checkpoint","text":"<pre><code>checkpoint = manager.load_checkpoint(\"t1_ckpt_abc123.ckpt\")\n\nprint(f\"Agent Type: {checkpoint.agent_class_name}\")\nprint(f\"Trace ID: {checkpoint.trace.trace_id}\")\nprint(f\"Span ID: {checkpoint.span_id}\")\nprint(f\"Timestamp: {checkpoint.timestamp}\")\n</code></pre>"},{"location":"guide/time-travel-debugging/#replayengine","title":"ReplayEngine","text":"<p>The <code>ReplayEngine</code> restores agent state from a checkpoint and enables interactive debugging.</p>"},{"location":"guide/time-travel-debugging/#basic-replay","title":"Basic Replay","text":"<pre><code>from clearstone.debugging import ReplayEngine\n\ncheckpoint = manager.load_checkpoint(\"t1_ckpt_abc123.ckpt\")\n\n# Pass the trace_store to access all spans in the trace\nengine = ReplayEngine(checkpoint, trace_store=provider.trace_store)\n\nresult = engine.replay_from_checkpoint(\n    function_name=\"process_next_step\",\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"guide/time-travel-debugging/#interactive-debugging-session","title":"Interactive Debugging Session","text":"<p>The <code>start_debugging_session</code> method drops you into an interactive <code>pdb</code> session with the restored agent state:</p> <pre><code># Pass trace_store to access all spans including child spans\nengine = ReplayEngine(checkpoint, trace_store=provider.trace_store)\n\n# Define how your trace data maps to your code\nmock_config = {\n    \"llm\": \"my_app.tools.llm.invoke\",\n    \"tool\": \"my_app.tools.api.run_tool\"\n}\n\nengine.start_debugging_session(\n    function_to_replay=\"process_next_step\",\n    mock_config=mock_config,\n    input_data={\"query\": \"test\"}\n)\n</code></pre> <p>When you run this: <pre><code>--- \ud83d\udd70\ufe0f Welcome to the Clearstone Time-Travel Debugger ---\n  Trace ID: abc123\n  Checkpoint: ckpt_xyz (at span: 'process_step_2')\n  Agent State: Rehydrated for 'my_app.agent.MyAgent'\n\nDropping into interactive debugger (pdb). Type 'c' to continue execution from the checkpoint.\n------------------------------------------------------------\n\n--- Pre-flight Mock Analysis ---\n  - Mocking 'my_app.tools.llm.invoke' (for span_type='llm')\n    - Found 3 recorded response(s) in the trace.\n  - Mocking 'my_app.tools.api.run_tool' (for span_type='tool')\n    - Found 2 recorded response(s) in the trace.\n------------------------------------------------------------\n\n&gt; /path/to/agent.py(42)process_next_step()\n-&gt; result = self.process(input_data)\n(Pdb) print(self.memory)\n[{'role': 'user', 'content': 'previous message'}]\n(Pdb) next\n&gt; /path/to/agent.py(43)process_next_step()\n-&gt; return result\n(Pdb) continue\n\n------------------------------------------------------------\n--- \u2705 Replay finished. Final result: {'status': 'success'} ---\n</code></pre></p> <p>Pre-flight Mock Analysis</p> <p>Before entering the debugger, the replay engine performs a pre-flight analysis: - Shows which functions will be mocked - Displays how many recorded responses were found for each function - Warns if no responses are found (preventing <code>StopIteration</code> errors)</p> <p>This makes debugging much easier by giving you visibility into what will happen during replay.</p>"},{"location":"guide/time-travel-debugging/#agent-requirements","title":"Agent Requirements","text":"<p>For agents to be checkpointable, they must implement two methods:</p>"},{"location":"guide/time-travel-debugging/#get_state","title":"get_state()","text":"<p>Returns a dictionary of all state to preserve:</p> <pre><code>class MyAgent:\n    def get_state(self):\n        \"\"\"Return complete agent state.\"\"\"\n        return {\n            \"memory\": self.memory,\n            \"config\": self.config,\n            \"tool_history\": self.tool_history,\n            \"session_cost\": self.session_cost\n        }\n</code></pre>"},{"location":"guide/time-travel-debugging/#load_state","title":"load_state()","text":"<p>Restores agent from a state dictionary:</p> <pre><code>class MyAgent:\n    def load_state(self, state):\n        \"\"\"Restore agent from state dictionary.\"\"\"\n        self.memory = state[\"memory\"]\n        self.config = state[\"config\"]\n        self.tool_history = state[\"tool_history\"]\n        self.session_cost = state[\"session_cost\"]\n</code></pre>"},{"location":"guide/time-travel-debugging/#automatic-state-capture","title":"Automatic State Capture","text":"<p>If your agent doesn't implement these methods, Clearstone automatically captures <code>__dict__</code>:</p> <pre><code>class SimpleAgent:\n    def __init__(self):\n        self.memory = []\n        self.count = 0\n</code></pre>"},{"location":"guide/time-travel-debugging/#deterministic-replay","title":"Deterministic Replay","text":"<p>The <code>DeterministicExecutionContext</code> mocks non-deterministic functions to ensure reproducible debugging.</p>"},{"location":"guide/time-travel-debugging/#automatically-mocked-functions","title":"Automatically Mocked Functions","text":"<p>When replaying from a checkpoint, these are automatically mocked:</p> <p>Time Functions: <pre><code>import time\n\ntime.time()  # Always returns checkpoint timestamp\ntime.time_ns()\ndatetime.datetime.now()\n</code></pre></p> <p>Random Functions: <pre><code>import random\n\nrandom.random()  # Always returns 0.5\n</code></pre></p>"},{"location":"guide/time-travel-debugging/#user-specified-mock-targets","title":"User-Specified Mock Targets","text":"<p>The replay engine allows you to specify exactly which functions to mock and what values they should return. This is done through the <code>mock_config</code> parameter:</p> <pre><code>from clearstone.debugging import ReplayEngine\n\n# Load checkpoint\ncheckpoint = manager.load_checkpoint(\"checkpoint.ckpt\")\nengine = ReplayEngine(checkpoint)\n\n# Map your trace's span types to your actual code paths\nmock_config = {\n    \"llm\": \"my_app.tools.llm.invoke\",           # LLM calls\n    \"tool\": \"my_app.tools.api.run_tool\",        # Tool executions\n    \"database\": \"my_app.db.query\"               # Database queries\n}\n\n# The replay engine automatically extracts recorded responses from the trace\n# and mocks these functions to return those values in order\nengine.start_debugging_session(\n    function_to_replay=\"process_step\",\n    mock_config=mock_config,\n    input_data={\"query\": \"test\"}\n)\n</code></pre> <p>How It Works:</p> <ol> <li>The replay engine scans the checkpoint's trace for spans matching the specified types (e.g., \"llm\", \"tool\")</li> <li>It extracts the recorded outputs from those spans in chronological order</li> <li>It patches the specified import paths to return those outputs sequentially</li> <li>Your agent's code calls the functions naturally, but gets deterministic responses</li> </ol> <p>Error Handling:</p> <p>If a mocked function is called more times than there are recorded responses, you'll see:</p> <pre><code>------------------------------------------------------------\n--- \u274c Replay Failed: StopIteration ---\nThis usually means a mocked function was called more times than there were recorded responses in the trace.\nPlease check the 'Pre-flight Mock Analysis' above to see how many responses were found.\n</code></pre> <p>This clear error message helps you quickly identify: - Which function ran out of responses - How many responses were expected vs. actual calls - Whether your mock configuration is correct</p>"},{"location":"guide/time-travel-debugging/#direct-context-usage-advanced","title":"Direct Context Usage (Advanced)","text":"<p>For more control, you can use <code>DeterministicExecutionContext</code> directly:</p> <pre><code>from clearstone.debugging import DeterministicExecutionContext\n\n# Manually specify mock targets and their return values\nmock_targets = {\n    \"my_app.llm.invoke\": [\"response1\", \"response2\", \"response3\"],\n    \"my_app.api.fetch\": [{\"data\": \"result1\"}, {\"data\": \"result2\"}]\n}\n\nwith DeterministicExecutionContext(checkpoint, mock_targets):\n    result = agent.run()\n</code></pre>"},{"location":"guide/time-travel-debugging/#complete-example","title":"Complete Example","text":""},{"location":"guide/time-travel-debugging/#1-create-agent-with-checkpointing","title":"1. Create Agent with Checkpointing","text":"<pre><code>from clearstone.observability import TracerProvider\nfrom clearstone.debugging import CheckpointManager\n\nclass ResearchAgent:\n    def __init__(self):\n        self.memory = []\n        self.total_cost = 0.0\n\n    def get_state(self):\n        return {\n            \"memory\": self.memory,\n            \"total_cost\": self.total_cost\n        }\n\n    def load_state(self, state):\n        self.memory = state[\"memory\"]\n        self.total_cost = state[\"total_cost\"]\n\n    def research(self, query):\n        self.memory.append({\"query\": query})\n        result = self.call_llm(query)\n        self.memory.append({\"result\": result})\n        return result\n\n    def call_llm(self, prompt):\n        return f\"Answer to: {prompt}\"\n\nagent = ResearchAgent()\n\nprovider = TracerProvider(db_path=\"research_traces.db\")\ntracer = provider.get_tracer(\"research_agent\")\n\nwith tracer.span(\"research_workflow\") as root_span:\n    trace_id = root_span.trace_id\n\n    with tracer.span(\"query_1\"):\n        agent.research(\"What is AI safety?\")\n\n    with tracer.span(\"query_2\") as span:\n        checkpoint_span_id = span.span_id\n        agent.research(\"What are the risks?\")\n\nprovider.shutdown()\n\nmanager = CheckpointManager()\ntrace = provider.trace_store.get_trace(trace_id)\ncheckpoint = manager.create_checkpoint(agent, trace, checkpoint_span_id)\ncheckpoint_path = manager.save_checkpoint(checkpoint)\n\nprint(f\"Checkpoint saved: {checkpoint_path}\")\n</code></pre>"},{"location":"guide/time-travel-debugging/#2-debug-from-checkpoint","title":"2. Debug from Checkpoint","text":"<pre><code>from clearstone.debugging import CheckpointManager, ReplayEngine\n\nmanager = CheckpointManager()\ncheckpoint = manager.load_checkpoint(checkpoint_path)\n\nprint(f\"Agent Memory at Checkpoint:\")\nfor item in checkpoint.agent_state[\"memory\"]:\n    print(f\"  {item}\")\n\nengine = ReplayEngine(checkpoint)\n\n# Map trace span types to your agent's code\nmock_config = {\n    \"llm\": \"research_agent.call_llm\"  # Mock LLM calls\n}\n\nengine.start_debugging_session(\n    function_to_replay=\"research\",\n    mock_config=mock_config,\n    query=\"What are AI alignment solutions?\"\n)\n</code></pre>"},{"location":"guide/time-travel-debugging/#advanced-features","title":"Advanced Features","text":""},{"location":"guide/time-travel-debugging/#checkpoint-metadata","title":"Checkpoint Metadata","text":"<p>Checkpoints include rich metadata:</p> <pre><code>checkpoint = manager.load_checkpoint(\"checkpoint.ckpt\")\n\nprint(f\"Created: {checkpoint.timestamp}\")\nprint(f\"Agent: {checkpoint.agent_class_name}\")\nprint(f\"Module: {checkpoint.agent_module}\")\nprint(f\"Trace ID: {checkpoint.trace.trace_id}\")\nprint(f\"Span: {checkpoint.span_id}\")\n</code></pre>"},{"location":"guide/time-travel-debugging/#upstream-span-tracking","title":"Upstream Span Tracking","text":"<p>Checkpoints capture the complete parent span hierarchy:</p> <pre><code>checkpoint = manager.load_checkpoint(\"checkpoint.ckpt\")\n\nprint(\"Execution History:\")\nfor span in checkpoint.trace.spans:\n    print(f\"  {span.name} ({span.duration_ms:.2f}ms)\")\n    print(f\"    Attributes: {span.attributes}\")\n</code></pre>"},{"location":"guide/time-travel-debugging/#checkpoint-comparison","title":"Checkpoint Comparison","text":"<p>Compare agent state at different points:</p> <pre><code>checkpoint_1 = manager.load_checkpoint(\"ckpt_step1.ckpt\")\ncheckpoint_2 = manager.load_checkpoint(\"ckpt_step2.ckpt\")\n\nstate_1 = checkpoint_1.agent_state\nstate_2 = checkpoint_2.agent_state\n\nprint(\"Memory Growth:\")\nprint(f\"  Step 1: {len(state_1['memory'])} items\")\nprint(f\"  Step 2: {len(state_2['memory'])} items\")\n\nprint(\"Cost Delta:\")\nprint(f\"  Step 1: ${state_1['total_cost']:.2f}\")\nprint(f\"  Step 2: ${state_2['total_cost']:.2f}\")\nprint(f\"  Delta: ${state_2['total_cost'] - state_1['total_cost']:.2f}\")\n</code></pre>"},{"location":"guide/time-travel-debugging/#conditional-checkpointing","title":"Conditional Checkpointing","text":"<p>Create checkpoints only when specific conditions are met:</p> <pre><code>with tracer.span(\"risky_operation\") as span:\n    try:\n        result = agent.risky_operation()\n    except Exception as e:\n        span.set_status(\"ERROR\")\n\n        provider.shutdown()\n        trace = provider.trace_store.get_trace(span.trace_id)\n\n        checkpoint = manager.create_checkpoint(agent, trace, span.span_id)\n        checkpoint_path = manager.save_checkpoint(checkpoint)\n\n        print(f\"Error checkpoint saved: {checkpoint_path}\")\n        raise\n</code></pre>"},{"location":"guide/time-travel-debugging/#debugging-workflows","title":"Debugging Workflows","text":""},{"location":"guide/time-travel-debugging/#debugging-a-bug","title":"Debugging a Bug","text":"<ol> <li> <p>Reproduce the bug with tracing enabled <pre><code>provider = TracerProvider(db_path=\"bug_trace.db\")\ntracer = provider.get_tracer(\"agent\")\n\nwith tracer.span(\"workflow\") as root:\n    try:\n        agent.run_workflow()\n    except BuggyBehaviorError as e:\n        checkpoint_span_id = root.span_id\n</code></pre></p> </li> <li> <p>Create checkpoint at failure point <pre><code>provider.shutdown()\ntrace = provider.trace_store.get_trace(root.trace_id)\ncheckpoint = manager.create_checkpoint(agent, trace, checkpoint_span_id)\nmanager.save_checkpoint(checkpoint)\n</code></pre></p> </li> <li> <p>Load checkpoint and debug <pre><code>checkpoint = manager.load_checkpoint(\"checkpoint.ckpt\")\nengine = ReplayEngine(checkpoint)\nengine.start_debugging_session(\"run_workflow\")\n</code></pre></p> </li> </ol>"},{"location":"guide/time-travel-debugging/#performance-debugging","title":"Performance Debugging","text":"<p>Find slow operations:</p> <pre><code>trace = trace_store.get_trace(trace_id)\n\nslow_spans = [\n    span for span in trace.spans\n    if span.duration_ms &gt; 1000\n]\n\nfor span in slow_spans:\n    checkpoint = manager.create_checkpoint(agent, trace, span.span_id)\n\n    print(f\"Created checkpoint before slow operation: {span.name}\")\n    print(f\"  Duration: {span.duration_ms:.2f}ms\")\n</code></pre>"},{"location":"guide/time-travel-debugging/#best-practices","title":"Best Practices","text":""},{"location":"guide/time-travel-debugging/#1-checkpoint-before-critical-operations","title":"1. Checkpoint Before Critical Operations","text":"<pre><code>with tracer.span(\"critical_operation\") as span:\n    checkpoint = manager.create_checkpoint(agent, trace, span.span_id)\n    agent.critical_operation()\n</code></pre>"},{"location":"guide/time-travel-debugging/#2-name-checkpoints-descriptively","title":"2. Name Checkpoints Descriptively","text":"<pre><code>checkpoint_path = manager.save_checkpoint(\n    checkpoint,\n    filename=f\"before_payment_{transaction_id}.ckpt\"\n)\n</code></pre>"},{"location":"guide/time-travel-debugging/#3-clean-up-old-checkpoints","title":"3. Clean Up Old Checkpoints","text":"<pre><code>import os\nimport time\n\ncheckpoint_dir = \".checkpoints\"\nmax_age_days = 30\n\nfor filename in os.listdir(checkpoint_dir):\n    path = os.path.join(checkpoint_dir, filename)\n    age_days = (time.time() - os.path.getmtime(path)) / 86400\n\n    if age_days &gt; max_age_days:\n        os.remove(path)\n</code></pre>"},{"location":"guide/time-travel-debugging/#4-use-version-control-for-checkpoints","title":"4. Use Version Control for Checkpoints","text":"<pre><code># .gitignore\n.checkpoints/*.ckpt\n\n# Keep only representative checkpoints in version control\ngit add .checkpoints/baseline_v1.ckpt\n</code></pre>"},{"location":"guide/time-travel-debugging/#5-document-checkpoint-purpose","title":"5. Document Checkpoint Purpose","text":"<pre><code>checkpoint.metadata[\"purpose\"] = \"Before prod deployment 2024-01-15\"\ncheckpoint.metadata[\"ticket\"] = \"JIRA-123\"\ncheckpoint.metadata[\"notes\"] = \"Agent was experiencing high costs\"\n</code></pre>"},{"location":"guide/time-travel-debugging/#next-steps","title":"Next Steps","text":"<ul> <li>Observability Guide: Learn about distributed tracing</li> <li>Testing Guide: Test policies with backtesting</li> <li>API Reference: Complete debugging API documentation</li> </ul>"}]}